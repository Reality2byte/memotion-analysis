{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ktrain_text_training.ipynb","provenance":[{"file_id":"1AwJeipvlg-Yrw4T6zQ1gvub6b4fUIT6t","timestamp":1583942518757}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"kC6U3c_eVFsb","colab_type":"code","outputId":"2d26c391-c84c-476b-8ae2-b484453c8e44","executionInfo":{"status":"ok","timestamp":1584078801334,"user_tz":-330,"elapsed":27673,"user":{"displayName":"Harsh Kataria","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimH0VsMEaiEzaEeo5nJzMe1WuJRQ2b1DK9fb3S=s64","userId":"07300304294680146354"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7QSFQGEsrJV4","colab_type":"code","outputId":"7b12ea7e-68de-480b-ddd4-10cbe26516a7","executionInfo":{"status":"ok","timestamp":1584078938532,"user_tz":-330,"elapsed":88267,"user":{"displayName":"Harsh Kataria","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimH0VsMEaiEzaEeo5nJzMe1WuJRQ2b1DK9fb3S=s64","userId":"07300304294680146354"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["! pip3 install tensorflow==2.0\n","! pip3 install ktrain"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting tensorflow==2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/0f/7bd55361168bb32796b360ad15a25de6966c9c1beb58a8e30c01c8279862/tensorflow-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (86.3MB)\n","\u001b[K     |████████████████████████████████| 86.3MB 49kB/s \n","\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.27.1)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (3.10.0)\n","Collecting tensorflow-estimator<2.1.0,>=2.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n","\u001b[K     |████████████████████████████████| 450kB 51.5MB/s \n","\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (0.8.1)\n","Collecting tensorboard<2.1.0,>=2.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/54/99b9d5d52d5cb732f099baaaf7740403e83fe6b0cedde940fabd2b13d75a/tensorboard-2.0.2-py3-none-any.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 52.2MB/s \n","\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (3.1.0)\n","Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (0.2.2)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.1.0)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (0.9.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.12.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.1.0)\n","Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.0.8)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (0.34.2)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.17.5)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (0.1.8)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0) (1.11.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==2.0) (45.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.2.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (1.0.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (0.4.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (2.21.0)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (1.7.2)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0) (2.8.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (1.3.0)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (2.8)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (2019.11.28)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.0.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (0.2.8)\n","Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (4.0)\n","Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.1.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.1.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (0.4.8)\n","Installing collected packages: tensorflow-estimator, tensorboard, tensorflow\n","  Found existing installation: tensorflow-estimator 1.15.1\n","    Uninstalling tensorflow-estimator-1.15.1:\n","      Successfully uninstalled tensorflow-estimator-1.15.1\n","  Found existing installation: tensorboard 1.15.0\n","    Uninstalling tensorboard-1.15.0:\n","      Successfully uninstalled tensorboard-1.15.0\n","  Found existing installation: tensorflow 1.15.0\n","    Uninstalling tensorflow-1.15.0:\n","      Successfully uninstalled tensorflow-1.15.0\n","Successfully installed tensorboard-2.0.2 tensorflow-2.0.0 tensorflow-estimator-2.0.1\n","Collecting ktrain\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b2/ba/ad440774c3377d24c33a87df26cfa5e0572e49a626062c5243306a5ecaaf/ktrain-0.10.1.tar.gz (25.2MB)\n","\u001b[K     |████████████████████████████████| 25.2MB 189kB/s \n","\u001b[?25hCollecting scikit-learn==0.21.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/c5/d2238762d780dde84a20b8c761f563fe882b88c5a5fb03c056547c442a19/scikit_learn-0.21.3-cp36-cp36m-manylinux1_x86_64.whl (6.7MB)\n","\u001b[K     |████████████████████████████████| 6.7MB 43.0MB/s \n","\u001b[?25hRequirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from ktrain) (3.1.3)\n","Collecting pandas>=1.0.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/ec/b5dd8cfb078380fb5ae9325771146bccd4e8cad2d3e4c72c7433010684eb/pandas-1.0.1-cp36-cp36m-manylinux1_x86_64.whl (10.1MB)\n","\u001b[K     |████████████████████████████████| 10.1MB 38.9MB/s \n","\u001b[?25hRequirement already satisfied: fastprogress>=0.1.21 in /usr/local/lib/python3.6/dist-packages (from ktrain) (0.2.2)\n","Collecting keras_bert\n","  Downloading https://files.pythonhosted.org/packages/2c/0f/cdc886c1018943ea62d3209bc964413d5aa9d0eb7e493abd8545be679294/keras-bert-0.81.0.tar.gz\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from ktrain) (2.21.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from ktrain) (0.14.1)\n","Collecting langdetect\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/a3/8407c1e62d5980188b4acc45ef3d94b933d14a2ebc9ef3505f22cf772570/langdetect-1.0.8.tar.gz (981kB)\n","\u001b[K     |████████████████████████████████| 983kB 41.7MB/s \n","\u001b[?25hRequirement already satisfied: jieba in /usr/local/lib/python3.6/dist-packages (from ktrain) (0.42.1)\n","Collecting cchardet\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/4e/847feebfc3e71c773b23ee06c74687b8c50a5a6d6aaff452a0a4f4eb9a32/cchardet-2.1.5-cp36-cp36m-manylinux1_x86_64.whl (241kB)\n","\u001b[K     |████████████████████████████████| 245kB 54.5MB/s \n","\u001b[?25hCollecting networkx==2.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/08/f20aef11d4c343b557e5de6b9548761811eb16e438cee3d32b1c66c8566b/networkx-2.3.zip (1.7MB)\n","\u001b[K     |████████████████████████████████| 1.8MB 41.0MB/s \n","\u001b[?25hRequirement already satisfied: bokeh in /usr/local/lib/python3.6/dist-packages (from ktrain) (1.4.0)\n","Collecting seqeval\n","  Downloading https://files.pythonhosted.org/packages/34/91/068aca8d60ce56dd9ba4506850e876aba5e66a6f2f29aa223224b50df0de/seqeval-0.0.12.tar.gz\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from ktrain) (20.1)\n","Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.6/dist-packages (from ktrain) (2.0.0)\n","Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/33/ffb67897a6985a7b7d8e5e7878c3628678f553634bd3836404fef06ef19b/transformers-2.5.1-py3-none-any.whl (499kB)\n","\u001b[K     |████████████████████████████████| 501kB 52.5MB/s \n","\u001b[?25hRequirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from ktrain) (5.5.0)\n","Collecting syntok\n","  Downloading https://files.pythonhosted.org/packages/ff/36/5b423791cd877a21c2771a2b070194270f163f2969066923f89aa3099e2d/syntok-1.2.2.tar.gz\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.21.3->ktrain) (1.4.1)\n","Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.21.3->ktrain) (1.17.5)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->ktrain) (2.4.6)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->ktrain) (1.1.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->ktrain) (2.6.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->ktrain) (0.10.0)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=1.0.1->ktrain) (2018.9)\n","Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras_bert->ktrain) (2.2.5)\n","Collecting keras-transformer>=0.30.0\n","  Downloading https://files.pythonhosted.org/packages/54/0c/fede535ac576c03863c44bf2e0bf051fe21f5e10103631b6b6236ae446f3/keras-transformer-0.32.0.tar.gz\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->ktrain) (2019.11.28)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->ktrain) (1.24.3)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->ktrain) (2.8)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->ktrain) (3.0.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from langdetect->ktrain) (1.12.0)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx==2.3->ktrain) (4.4.1)\n","Requirement already satisfied: tornado>=4.3 in /usr/local/lib/python3.6/dist-packages (from bokeh->ktrain) (4.5.3)\n","Requirement already satisfied: Jinja2>=2.7 in /usr/local/lib/python3.6/dist-packages (from bokeh->ktrain) (2.11.1)\n","Requirement already satisfied: pillow>=4.0 in /usr/local/lib/python3.6/dist-packages (from bokeh->ktrain) (6.2.2)\n","Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from bokeh->ktrain) (3.13)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->ktrain) (0.9.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->ktrain) (0.16.0)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->ktrain) (1.1.0)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->ktrain) (3.10.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->ktrain) (4.28.1)\n","Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->ktrain) (0.3.1.1)\n","Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->ktrain) (19.3.0)\n","Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->ktrain) (0.21.1)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->ktrain) (1.11.2)\n","Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->ktrain) (2.3)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n","\u001b[K     |████████████████████████████████| 870kB 43.1MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers->ktrain) (3.0.12)\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n","\u001b[K     |████████████████████████████████| 1.0MB 45.6MB/s \n","\u001b[?25hCollecting tokenizers==0.5.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n","\u001b[K     |████████████████████████████████| 3.7MB 51.3MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers->ktrain) (2019.12.20)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers->ktrain) (1.11.15)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (0.7.5)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (2.1.3)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (4.3.3)\n","Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (4.8.0)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (0.8.1)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (45.2.0)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (1.0.18)\n","Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras_bert->ktrain) (1.1.0)\n","Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Keras->keras_bert->ktrain) (1.0.8)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras_bert->ktrain) (2.8.0)\n","Collecting keras-pos-embd>=0.10.0\n","  Downloading https://files.pythonhosted.org/packages/09/70/b63ed8fc660da2bb6ae29b9895401c628da5740c048c190b5d7107cadd02/keras-pos-embd-0.11.0.tar.gz\n","Collecting keras-multi-head>=0.22.0\n","  Downloading https://files.pythonhosted.org/packages/40/3e/d0a64bb2ac5217928effe4507c26bbd19b86145d16a1948bc2d4f4c6338a/keras-multi-head-0.22.0.tar.gz\n","Collecting keras-layer-normalization>=0.12.0\n","  Downloading https://files.pythonhosted.org/packages/a4/0e/d1078df0494bac9ce1a67954e5380b6e7569668f0f3b50a9531c62c1fc4a/keras-layer-normalization-0.14.0.tar.gz\n","Collecting keras-position-wise-feed-forward>=0.5.0\n","  Downloading https://files.pythonhosted.org/packages/e3/59/f0faa1037c033059e7e9e7758e6c23b4d1c0772cd48de14c4b6fd4033ad5/keras-position-wise-feed-forward-0.6.0.tar.gz\n","Collecting keras-embed-sim>=0.7.0\n","  Downloading https://files.pythonhosted.org/packages/bc/20/735fd53f6896e2af63af47e212601c1b8a7a80d00b6126c388c9d1233892/keras-embed-sim-0.7.0.tar.gz\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.7->bokeh->ktrain) (1.1.1)\n","Requirement already satisfied: googleapis-common-protos in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow_datasets->ktrain) (1.51.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers->ktrain) (7.0)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers->ktrain) (0.3.3)\n","Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers->ktrain) (1.14.15)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers->ktrain) (0.9.4)\n","Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython->ktrain) (0.2.0)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->ktrain) (0.6.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ktrain) (0.1.8)\n","Collecting keras-self-attention==0.41.0\n","  Downloading https://files.pythonhosted.org/packages/1b/1c/01599219bef7266fa43b3316e4f55bcb487734d3bafdc60ffd564f3cfe29/keras-self-attention-0.41.0.tar.gz\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers->ktrain) (0.15.2)\n","Building wheels for collected packages: ktrain, keras-bert, langdetect, networkx, seqeval, syntok, keras-transformer, sacremoses, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-self-attention\n","  Building wheel for ktrain (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ktrain: filename=ktrain-0.10.1-cp36-none-any.whl size=25209015 sha256=9b554b4d190eb9037976b9fd2a66c3512d2c4a32c9b244a61777beb7a153bd85\n","  Stored in directory: /root/.cache/pip/wheels/d6/01/7f/2cbd7f9451abd37a89d70eaee8b7d2674201d0ada06e6d3569\n","  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-bert: filename=keras_bert-0.81.0-cp36-none-any.whl size=37913 sha256=85451d5b39617edcb1de93b1a304c72dc04255e48642933b8493b16c9e712e57\n","  Stored in directory: /root/.cache/pip/wheels/bd/27/da/ffc2d573aa48b87440ec4f98bc7c992e3a2d899edb2d22ef9e\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.8-cp36-none-any.whl size=993193 sha256=fe8833ab3f454829661198bc8b884a69fe6858ec70df619f872b457feb39d6ac\n","  Stored in directory: /root/.cache/pip/wheels/8d/b3/aa/6d99de9f3841d7d3d40a60ea06e6d669e8e5012e6c8b947a57\n","  Building wheel for networkx (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for networkx: filename=networkx-2.3-py2.py3-none-any.whl size=1556408 sha256=d76f689b5cd997f91bd274e7fb66fad7f97c0f86dd86093565b019d0b89d6281\n","  Stored in directory: /root/.cache/pip/wheels/de/63/64/3699be2a9d0ccdb37c7f16329acf3863fd76eda58c39c737af\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-0.0.12-cp36-none-any.whl size=7424 sha256=6e8db462fef9633ce99b30e02020bca1beaa4bcd7562ac80ad687f419f516f75\n","  Stored in directory: /root/.cache/pip/wheels/4f/32/0a/df3b340a82583566975377d65e724895b3fad101a3fb729f68\n","  Building wheel for syntok (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for syntok: filename=syntok-1.2.2-cp36-none-any.whl size=20724 sha256=80153b11f4b98973bccc00674fb5284018e8185875aa67075085e50e3fc540be\n","  Stored in directory: /root/.cache/pip/wheels/b4/b0/d2/ffdbbc1a16cb37e580fb7b3a6fbaaf09c7f7c163981db385b3\n","  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-transformer: filename=keras_transformer-0.32.0-cp36-none-any.whl size=13266 sha256=d0196f0e78b883330c84aa187cb3f90ed2062b7453c64ac65238c2af78aa3f2c\n","  Stored in directory: /root/.cache/pip/wheels/62/f0/ce/82fa5d024d5ef8e263f26a50dcee23820efe245680ce9c922a\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=d8ac2263924b51f1d80a5c2911ea338f3457d852b2b0a319a8fda1bd837a9db1\n","  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n","  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.11.0-cp36-none-any.whl size=7554 sha256=b91f3ee6bfb0436b5459374868965e93674ef6bee1ef91b6269beb9ccabe19a8\n","  Stored in directory: /root/.cache/pip/wheels/5b/a1/a0/ce6b1d49ba1a9a76f592e70cf297b05c96bc9f418146761032\n","  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-multi-head: filename=keras_multi_head-0.22.0-cp36-none-any.whl size=15371 sha256=b08bbece7ca9937799b36ac19e41961c406218dc395b75ac5ba3c27abebdfa60\n","  Stored in directory: /root/.cache/pip/wheels/bb/df/3f/81b36f41b66e6a9cd69224c70a737de2bb6b2f7feb3272c25e\n","  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.14.0-cp36-none-any.whl size=5268 sha256=c04fb4e5187586be31783a48d05557de1b88b9df1a924066f041dea2026eb150\n","  Stored in directory: /root/.cache/pip/wheels/54/80/22/a638a7d406fd155e507aa33d703e3fa2612b9eb7bb4f4fe667\n","  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.6.0-cp36-none-any.whl size=5623 sha256=d6385d8959e0343a6714b566e70b397dc156498e7a666e77c395750c5cb26598\n","  Stored in directory: /root/.cache/pip/wheels/39/e2/e2/3514fef126a00574b13bc0b9e23891800158df3a3c19c96e3b\n","  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.7.0-cp36-none-any.whl size=4676 sha256=1ca94ab3b98e60ecbc8a12d9e5ee188ac3850418e257e6ff72b933b8714a4671\n","  Stored in directory: /root/.cache/pip/wheels/d1/bc/b1/b0c45cee4ca2e6c86586b0218ffafe7f0703c6d07fdf049866\n","  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-self-attention: filename=keras_self_attention-0.41.0-cp36-none-any.whl size=17288 sha256=a96d4f6c79beb676c19b48354ec03afd311bb22ba37f740dbee6b8ccf3eabf95\n","  Stored in directory: /root/.cache/pip/wheels/cc/dc/17/84258b27a04cd38ac91998abe148203720ca696186635db694\n","Successfully built ktrain keras-bert langdetect networkx seqeval syntok keras-transformer sacremoses keras-pos-embd keras-multi-head keras-layer-normalization keras-position-wise-feed-forward keras-embed-sim keras-self-attention\n","\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=0.25.0; python_version >= \"3.0\", but you'll have pandas 1.0.1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Installing collected packages: scikit-learn, pandas, keras-pos-embd, keras-self-attention, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-transformer, keras-bert, langdetect, cchardet, networkx, seqeval, sacremoses, sentencepiece, tokenizers, transformers, syntok, ktrain\n","  Found existing installation: scikit-learn 0.22.1\n","    Uninstalling scikit-learn-0.22.1:\n","      Successfully uninstalled scikit-learn-0.22.1\n","  Found existing installation: pandas 0.25.3\n","    Uninstalling pandas-0.25.3:\n","      Successfully uninstalled pandas-0.25.3\n","  Found existing installation: networkx 2.4\n","    Uninstalling networkx-2.4:\n","      Successfully uninstalled networkx-2.4\n","Successfully installed cchardet-2.1.5 keras-bert-0.81.0 keras-embed-sim-0.7.0 keras-layer-normalization-0.14.0 keras-multi-head-0.22.0 keras-pos-embd-0.11.0 keras-position-wise-feed-forward-0.6.0 keras-self-attention-0.41.0 keras-transformer-0.32.0 ktrain-0.10.1 langdetect-1.0.8 networkx-2.3 pandas-1.0.1 sacremoses-0.0.38 scikit-learn-0.21.3 sentencepiece-0.1.85 seqeval-0.0.12 syntok-1.2.2 tokenizers-0.5.2 transformers-2.5.1\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["pandas"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"-XMxo00FygIQ","colab_type":"code","outputId":"af61874f-b73c-460b-9a43-30ddb386c2a4","executionInfo":{"status":"ok","timestamp":1584010299978,"user_tz":-330,"elapsed":90310,"user":{"displayName":"Harsh Kataria","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimH0VsMEaiEzaEeo5nJzMe1WuJRQ2b1DK9fb3S=s64","userId":"07300304294680146354"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!pip3 install ktrain"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: ktrain in /usr/local/lib/python3.6/dist-packages (0.10.1)\n","Requirement already satisfied: scikit-learn==0.21.3 in /usr/local/lib/python3.6/dist-packages (from ktrain) (0.21.3)\n","Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from ktrain) (3.1.3)\n","Requirement already satisfied: bokeh in /usr/local/lib/python3.6/dist-packages (from ktrain) (1.4.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from ktrain) (0.14.1)\n","Requirement already satisfied: langdetect in /usr/local/lib/python3.6/dist-packages (from ktrain) (1.0.8)\n","Requirement already satisfied: jieba in /usr/local/lib/python3.6/dist-packages (from ktrain) (0.42.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from ktrain) (20.1)\n","Requirement already satisfied: networkx==2.3 in /usr/local/lib/python3.6/dist-packages (from ktrain) (2.3)\n","Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.6/dist-packages (from ktrain) (2.0.0)\n","Requirement already satisfied: fastprogress>=0.1.21 in /usr/local/lib/python3.6/dist-packages (from ktrain) (0.2.2)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from ktrain) (1.0.1)\n","Requirement already satisfied: seqeval in /usr/local/lib/python3.6/dist-packages (from ktrain) (0.0.12)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (from ktrain) (2.5.1)\n","Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from ktrain) (5.5.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from ktrain) (2.21.0)\n","Requirement already satisfied: keras-bert in /usr/local/lib/python3.6/dist-packages (from ktrain) (0.81.0)\n","Requirement already satisfied: cchardet in /usr/local/lib/python3.6/dist-packages (from ktrain) (2.1.5)\n","Requirement already satisfied: syntok in /usr/local/lib/python3.6/dist-packages (from ktrain) (1.2.2)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.21.3->ktrain) (1.4.1)\n","Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.21.3->ktrain) (1.17.5)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->ktrain) (2.4.6)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->ktrain) (2.6.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->ktrain) (1.1.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->ktrain) (0.10.0)\n","Requirement already satisfied: tornado>=4.3 in /usr/local/lib/python3.6/dist-packages (from bokeh->ktrain) (4.5.3)\n","Requirement already satisfied: Jinja2>=2.7 in /usr/local/lib/python3.6/dist-packages (from bokeh->ktrain) (2.11.1)\n","Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from bokeh->ktrain) (3.13)\n","Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.6/dist-packages (from bokeh->ktrain) (1.12.0)\n","Requirement already satisfied: pillow>=4.0 in /usr/local/lib/python3.6/dist-packages (from bokeh->ktrain) (6.2.2)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx==2.3->ktrain) (4.4.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->ktrain) (4.28.1)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->ktrain) (3.10.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->ktrain) (1.11.2)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->ktrain) (0.16.0)\n","Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->ktrain) (2.3)\n","Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->ktrain) (19.3.0)\n","Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->ktrain) (0.3.1.1)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->ktrain) (0.9.0)\n","Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->ktrain) (0.21.1)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->ktrain) (1.1.0)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=1.0.1->ktrain) (2018.9)\n","Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval->ktrain) (2.2.5)\n","Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers->ktrain) (0.5.2)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers->ktrain) (0.0.38)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers->ktrain) (3.0.12)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers->ktrain) (2019.12.20)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers->ktrain) (1.11.15)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers->ktrain) (0.1.85)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (1.0.18)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (45.2.0)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (4.3.3)\n","Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (4.8.0)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (2.1.3)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (0.7.5)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->ktrain) (0.8.1)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->ktrain) (2.8)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->ktrain) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->ktrain) (2019.11.28)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->ktrain) (1.24.3)\n","Requirement already satisfied: keras-transformer>=0.30.0 in /usr/local/lib/python3.6/dist-packages (from keras-bert->ktrain) (0.32.0)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.7->bokeh->ktrain) (1.1.1)\n","Requirement already satisfied: googleapis-common-protos in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow-datasets->ktrain) (1.51.0)\n","Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->ktrain) (1.1.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->ktrain) (2.8.0)\n","Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->ktrain) (1.0.8)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers->ktrain) (7.0)\n","Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers->ktrain) (1.14.15)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers->ktrain) (0.3.3)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers->ktrain) (0.9.4)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ktrain) (0.1.8)\n","Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython->ktrain) (0.2.0)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->ktrain) (0.6.0)\n","Requirement already satisfied: keras-position-wise-feed-forward>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.30.0->keras-bert->ktrain) (0.6.0)\n","Requirement already satisfied: keras-layer-normalization>=0.12.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.30.0->keras-bert->ktrain) (0.14.0)\n","Requirement already satisfied: keras-pos-embd>=0.10.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.30.0->keras-bert->ktrain) (0.11.0)\n","Requirement already satisfied: keras-embed-sim>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.30.0->keras-bert->ktrain) (0.7.0)\n","Requirement already satisfied: keras-multi-head>=0.22.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.30.0->keras-bert->ktrain) (0.22.0)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers->ktrain) (0.15.2)\n","Requirement already satisfied: keras-self-attention==0.41.0 in /usr/local/lib/python3.6/dist-packages (from keras-multi-head>=0.22.0->keras-transformer>=0.30.0->keras-bert->ktrain) (0.41.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TRehkw93rXg6","colab_type":"code","outputId":"46fe6519-4d9a-497e-d80c-3ef71df84f30","executionInfo":{"status":"ok","timestamp":1584078942842,"user_tz":-330,"elapsed":88008,"user":{"displayName":"Harsh Kataria","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimH0VsMEaiEzaEeo5nJzMe1WuJRQ2b1DK9fb3S=s64","userId":"07300304294680146354"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["import ktrain\n","ktrain.__file__"],"execution_count":2,"outputs":[{"output_type":"stream","text":["using Keras version: 2.2.4-tf\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["'/usr/local/lib/python3.6/dist-packages/ktrain/__init__.py'"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"JrhKd6LvtH0m","colab_type":"code","colab":{}},"source":["#! cp '/content/core.py' '/usr/local/lib/python3.6/dist-packages/ktrain/'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OFnrJ6SLPGTl","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9gr9u5RcaINi","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XlOyhFpiaIes","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"c30893a0-0665-42bd-9534-500f7f9467ea","executionInfo":{"status":"ok","timestamp":1584080157947,"user_tz":-330,"elapsed":185647,"user":{"displayName":"Harsh Kataria","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimH0VsMEaiEzaEeo5nJzMe1WuJRQ2b1DK9fb3S=s64","userId":"07300304294680146354"}}},"source":["import ktrain\n","from ktrain import text\n","\n","\n","\n","modelslist = [['slight','very_offensive','hateful_offensive']]\n","data_files = ['memotion_eq_onlyoffensive22.csv']\n","for columnclass, data_file in zip(modelslist, data_files):\n","\n","    \n","    columns=columnclass      \n","    DATA_PATH = '/content/drive/My Drive/memotion/' + data_file\n","    print(DATA_PATH)\n","\n","    NUM_WORDS = 90000\n","    MAXLEN = 250\n","\n","    h5name = '_'.join(columnclass)\n","    print('model', h5name)\n","\n","    (x_train, y_train), (x_test, y_test), preproc = text.texts_from_csv(DATA_PATH,\n","                          'cleaned_ocr',\n","                          label_columns = columns,\n","                          val_filepath=None, # if None, 10% of data will be used for validation\n","                          max_features=NUM_WORDS, maxlen=MAXLEN,\n","                        ngram_range=5,)\n","\n","\n","    model = text.text_classifier('logreg', (x_train, y_train), preproc=preproc,)\n","\n","    learner = ktrain.get_learner(model, train_data=(x_train, y_train), val_data=(x_test, y_test),batch_size=20)\n","    a='/content/drive/My Drive/Weight_file/'+h5name\n","    learner.autofit(0.001, 100,early_stopping=80, reduce_on_plateau=5,)\n","\n","    a=learner.validate(val_data=(x_test, y_test), class_names=columns)\n","  \n","    predictor = ktrain.get_predictor(learner.model, preproc)\n","\n","   # score = 'xyz'########################\n","    h5name = \"/content/drive/My Drive/memotion/wgts/off22\"+h5name +  '.h5'\n","    print('model', h5name)\n","\n","    predictor.save(h5name)\n","    print(\"Model Saved\")\n"],"execution_count":9,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/memotion/memotion_eq_onlyoffensive22.csv\n","model slight_very_offensive_hateful_offensive\n","detected encoding: utf-8 (if wrong, set manually)\n","language: en\n","Word Counts: 8421\n","Nrows: 6740\n","6740 train sequences\n","train sequence lengths:\n","\tmean : 8\n","\t95percentile : 18\n","\t99percentile : 27\n","Adding 5-gram features\n","max_features changed to 173113 with addition of ngrams\n","Average train sequence length with ngrams: 30\n","train (w/ngrams) sequence lengths:\n","\tmean : 31\n","\t95percentile : 80\n","\t99percentile : 125\n","x_train shape: (6740,250)\n","y_train shape: (6740, 3)\n","749 test sequences\n","test sequence lengths:\n","\tmean : 7\n","\t95percentile : 16\n","\t99percentile : 26\n","Average test sequence length with ngrams: 21\n","test (w/ngrams) sequence lengths:\n","\tmean : 21\n","\t95percentile : 65\n","\t99percentile : 118\n","x_test shape: (749,250)\n","y_test shape: (749, 3)\n","Is Multi-Label? False\n","compiling word ID features...\n","maxlen is 250\n","done.\n","\n","\n","begin training using triangular learning rate policy with max lr of 0.001...\n","Train on 6740 samples, validate on 749 samples\n","Epoch 1/100\n","6740/6740 [==============================] - 2s 331us/sample - loss: 0.9684 - accuracy: 0.6337 - val_loss: 0.8531 - val_accuracy: 0.8011\n","Epoch 2/100\n","6740/6740 [==============================] - 2s 270us/sample - loss: 0.6996 - accuracy: 0.9577 - val_loss: 0.7289 - val_accuracy: 0.8344\n","Epoch 3/100\n","6740/6740 [==============================] - 2s 277us/sample - loss: 0.5360 - accuracy: 0.9774 - val_loss: 0.6494 - val_accuracy: 0.8438\n","Epoch 4/100\n","6740/6740 [==============================] - 2s 270us/sample - loss: 0.4307 - accuracy: 0.9844 - val_loss: 0.5968 - val_accuracy: 0.8438\n","Epoch 5/100\n","6740/6740 [==============================] - 2s 280us/sample - loss: 0.3583 - accuracy: 0.9866 - val_loss: 0.5597 - val_accuracy: 0.8425\n","Epoch 6/100\n","6740/6740 [==============================] - 2s 279us/sample - loss: 0.3057 - accuracy: 0.9877 - val_loss: 0.5285 - val_accuracy: 0.8465\n","Epoch 7/100\n","6740/6740 [==============================] - 2s 260us/sample - loss: 0.2650 - accuracy: 0.9899 - val_loss: 0.5040 - val_accuracy: 0.8465\n","Epoch 8/100\n","6740/6740 [==============================] - 2s 265us/sample - loss: 0.2330 - accuracy: 0.9908 - val_loss: 0.4831 - val_accuracy: 0.8531\n","Epoch 9/100\n","6740/6740 [==============================] - 2s 278us/sample - loss: 0.2070 - accuracy: 0.9904 - val_loss: 0.4664 - val_accuracy: 0.8558\n","Epoch 10/100\n","6740/6740 [==============================] - 2s 276us/sample - loss: 0.1854 - accuracy: 0.9911 - val_loss: 0.4515 - val_accuracy: 0.8585\n","Epoch 11/100\n","6740/6740 [==============================] - 2s 271us/sample - loss: 0.1673 - accuracy: 0.9920 - val_loss: 0.4384 - val_accuracy: 0.8571\n","Epoch 12/100\n","6740/6740 [==============================] - 2s 274us/sample - loss: 0.1521 - accuracy: 0.9921 - val_loss: 0.4283 - val_accuracy: 0.8558\n","Epoch 13/100\n","6740/6740 [==============================] - 2s 271us/sample - loss: 0.1389 - accuracy: 0.9932 - val_loss: 0.4181 - val_accuracy: 0.8531\n","Epoch 14/100\n","6740/6740 [==============================] - 2s 275us/sample - loss: 0.1276 - accuracy: 0.9935 - val_loss: 0.4098 - val_accuracy: 0.8545\n","Epoch 15/100\n","6740/6740 [==============================] - 2s 264us/sample - loss: 0.1176 - accuracy: 0.9939 - val_loss: 0.4025 - val_accuracy: 0.8545\n","Epoch 16/100\n","6740/6740 [==============================] - 2s 264us/sample - loss: 0.1087 - accuracy: 0.9936 - val_loss: 0.3963 - val_accuracy: 0.8531\n","Epoch 17/100\n","6740/6740 [==============================] - 2s 261us/sample - loss: 0.1010 - accuracy: 0.9939 - val_loss: 0.3910 - val_accuracy: 0.8531\n","Epoch 18/100\n","6740/6740 [==============================] - 2s 270us/sample - loss: 0.0940 - accuracy: 0.9938 - val_loss: 0.3852 - val_accuracy: 0.8491\n","Epoch 19/100\n","6740/6740 [==============================] - 2s 275us/sample - loss: 0.0878 - accuracy: 0.9941 - val_loss: 0.3810 - val_accuracy: 0.8545\n","Epoch 20/100\n","6740/6740 [==============================] - 2s 284us/sample - loss: 0.0821 - accuracy: 0.9941 - val_loss: 0.3766 - val_accuracy: 0.8505\n","Epoch 21/100\n","6740/6740 [==============================] - 2s 278us/sample - loss: 0.0773 - accuracy: 0.9938 - val_loss: 0.3722 - val_accuracy: 0.8598\n","Epoch 22/100\n","6740/6740 [==============================] - 2s 274us/sample - loss: 0.0728 - accuracy: 0.9939 - val_loss: 0.3690 - val_accuracy: 0.8558\n","Epoch 23/100\n","6740/6740 [==============================] - 2s 273us/sample - loss: 0.0685 - accuracy: 0.9942 - val_loss: 0.3659 - val_accuracy: 0.8611\n","Epoch 24/100\n","6740/6740 [==============================] - 2s 263us/sample - loss: 0.0648 - accuracy: 0.9941 - val_loss: 0.3642 - val_accuracy: 0.8611\n","Epoch 25/100\n","6740/6740 [==============================] - 2s 257us/sample - loss: 0.0613 - accuracy: 0.9942 - val_loss: 0.3616 - val_accuracy: 0.8598\n","Epoch 26/100\n","6740/6740 [==============================] - 2s 263us/sample - loss: 0.0582 - accuracy: 0.9942 - val_loss: 0.3591 - val_accuracy: 0.8611\n","Epoch 27/100\n","6740/6740 [==============================] - 2s 267us/sample - loss: 0.0553 - accuracy: 0.9941 - val_loss: 0.3570 - val_accuracy: 0.8545\n","Epoch 28/100\n","6740/6740 [==============================] - 2s 266us/sample - loss: 0.0528 - accuracy: 0.9942 - val_loss: 0.3554 - val_accuracy: 0.8571\n","Epoch 29/100\n","6740/6740 [==============================] - 2s 267us/sample - loss: 0.0504 - accuracy: 0.9938 - val_loss: 0.3545 - val_accuracy: 0.8611\n","Epoch 30/100\n","6740/6740 [==============================] - 2s 261us/sample - loss: 0.0481 - accuracy: 0.9941 - val_loss: 0.3535 - val_accuracy: 0.8598\n","Epoch 31/100\n","6740/6740 [==============================] - 2s 269us/sample - loss: 0.0461 - accuracy: 0.9941 - val_loss: 0.3523 - val_accuracy: 0.8585\n","Epoch 32/100\n","6740/6740 [==============================] - 2s 284us/sample - loss: 0.0441 - accuracy: 0.9947 - val_loss: 0.3519 - val_accuracy: 0.8558\n","Epoch 33/100\n","6740/6740 [==============================] - 2s 268us/sample - loss: 0.0423 - accuracy: 0.9947 - val_loss: 0.3512 - val_accuracy: 0.8571\n","Epoch 34/100\n","6740/6740 [==============================] - 2s 268us/sample - loss: 0.0407 - accuracy: 0.9947 - val_loss: 0.3505 - val_accuracy: 0.8571\n","Epoch 35/100\n","6740/6740 [==============================] - 2s 270us/sample - loss: 0.0392 - accuracy: 0.9947 - val_loss: 0.3502 - val_accuracy: 0.8571\n","Epoch 36/100\n","6740/6740 [==============================] - 2s 271us/sample - loss: 0.0376 - accuracy: 0.9950 - val_loss: 0.3504 - val_accuracy: 0.8571\n","Epoch 37/100\n","6740/6740 [==============================] - 2s 272us/sample - loss: 0.0363 - accuracy: 0.9947 - val_loss: 0.3502 - val_accuracy: 0.8545\n","Epoch 38/100\n","6740/6740 [==============================] - 2s 291us/sample - loss: 0.0351 - accuracy: 0.9947 - val_loss: 0.3509 - val_accuracy: 0.8531\n","Epoch 39/100\n","6740/6740 [==============================] - 2s 294us/sample - loss: 0.0339 - accuracy: 0.9948 - val_loss: 0.3507 - val_accuracy: 0.8518\n","Epoch 40/100\n","6680/6740 [============================>.] - ETA: 0s - loss: 0.0327 - accuracy: 0.9952\n","Epoch 00040: Reducing Max LR on Plateau: new max lr will be 0.0005 (if not early_stopping).\n","6740/6740 [==============================] - 2s 280us/sample - loss: 0.0329 - accuracy: 0.9953 - val_loss: 0.3512 - val_accuracy: 0.8545\n","Epoch 41/100\n","6740/6740 [==============================] - 2s 275us/sample - loss: 0.0317 - accuracy: 0.9950 - val_loss: 0.3509 - val_accuracy: 0.8545\n","Epoch 42/100\n","6740/6740 [==============================] - 2s 279us/sample - loss: 0.0311 - accuracy: 0.9951 - val_loss: 0.3516 - val_accuracy: 0.8545\n","Epoch 43/100\n","6740/6740 [==============================] - 2s 278us/sample - loss: 0.0306 - accuracy: 0.9951 - val_loss: 0.3517 - val_accuracy: 0.8558\n","Epoch 44/100\n","6740/6740 [==============================] - 2s 267us/sample - loss: 0.0301 - accuracy: 0.9953 - val_loss: 0.3522 - val_accuracy: 0.8558\n","Epoch 45/100\n","6680/6740 [============================>.] - ETA: 0s - loss: 0.0295 - accuracy: 0.9954\n","Epoch 00045: Reducing Max LR on Plateau: new max lr will be 0.00025 (if not early_stopping).\n","6740/6740 [==============================] - 2s 262us/sample - loss: 0.0296 - accuracy: 0.9953 - val_loss: 0.3528 - val_accuracy: 0.8545\n","Epoch 46/100\n","6740/6740 [==============================] - 2s 273us/sample - loss: 0.0290 - accuracy: 0.9953 - val_loss: 0.3532 - val_accuracy: 0.8545\n","Epoch 47/100\n","6740/6740 [==============================] - 2s 281us/sample - loss: 0.0288 - accuracy: 0.9953 - val_loss: 0.3536 - val_accuracy: 0.8545\n","Epoch 48/100\n","6740/6740 [==============================] - 2s 273us/sample - loss: 0.0285 - accuracy: 0.9953 - val_loss: 0.3540 - val_accuracy: 0.8545\n","Epoch 49/100\n","6740/6740 [==============================] - 2s 269us/sample - loss: 0.0283 - accuracy: 0.9953 - val_loss: 0.3544 - val_accuracy: 0.8545\n","Epoch 50/100\n","6640/6740 [============================>.] - ETA: 0s - loss: 0.0282 - accuracy: 0.9955\n","Epoch 00050: Reducing Max LR on Plateau: new max lr will be 0.000125 (if not early_stopping).\n","6740/6740 [==============================] - 2s 272us/sample - loss: 0.0280 - accuracy: 0.9955 - val_loss: 0.3549 - val_accuracy: 0.8545\n","Epoch 51/100\n","6740/6740 [==============================] - 2s 267us/sample - loss: 0.0277 - accuracy: 0.9954 - val_loss: 0.3553 - val_accuracy: 0.8558\n","Epoch 52/100\n","6740/6740 [==============================] - 2s 266us/sample - loss: 0.0276 - accuracy: 0.9954 - val_loss: 0.3555 - val_accuracy: 0.8558\n","Epoch 53/100\n","6740/6740 [==============================] - 2s 264us/sample - loss: 0.0274 - accuracy: 0.9954 - val_loss: 0.3558 - val_accuracy: 0.8558\n","Epoch 54/100\n","6740/6740 [==============================] - 2s 264us/sample - loss: 0.0273 - accuracy: 0.9955 - val_loss: 0.3563 - val_accuracy: 0.8558\n","Epoch 55/100\n","6560/6740 [============================>.] - ETA: 0s - loss: 0.0272 - accuracy: 0.9956\n","Epoch 00055: Reducing Max LR on Plateau: new max lr will be 6.25e-05 (if not early_stopping).\n","6740/6740 [==============================] - 2s 267us/sample - loss: 0.0271 - accuracy: 0.9954 - val_loss: 0.3567 - val_accuracy: 0.8558\n","Epoch 56/100\n","6740/6740 [==============================] - 2s 270us/sample - loss: 0.0270 - accuracy: 0.9954 - val_loss: 0.3569 - val_accuracy: 0.8558\n","Epoch 57/100\n","6740/6740 [==============================] - 2s 278us/sample - loss: 0.0269 - accuracy: 0.9955 - val_loss: 0.3571 - val_accuracy: 0.8558\n","Epoch 58/100\n","6740/6740 [==============================] - 2s 294us/sample - loss: 0.0268 - accuracy: 0.9955 - val_loss: 0.3573 - val_accuracy: 0.8558\n","Epoch 59/100\n","6740/6740 [==============================] - 2s 295us/sample - loss: 0.0267 - accuracy: 0.9955 - val_loss: 0.3576 - val_accuracy: 0.8558\n","Epoch 60/100\n","6580/6740 [============================>.] - ETA: 0s - loss: 0.0262 - accuracy: 0.9957\n","Epoch 00060: Reducing Max LR on Plateau: new max lr will be 3.125e-05 (if not early_stopping).\n","6740/6740 [==============================] - 2s 266us/sample - loss: 0.0266 - accuracy: 0.9955 - val_loss: 0.3578 - val_accuracy: 0.8558\n","Epoch 61/100\n","6740/6740 [==============================] - 2s 265us/sample - loss: 0.0265 - accuracy: 0.9954 - val_loss: 0.3581 - val_accuracy: 0.8558\n","Epoch 62/100\n","6740/6740 [==============================] - 2s 271us/sample - loss: 0.0264 - accuracy: 0.9954 - val_loss: 0.3583 - val_accuracy: 0.8558\n","Epoch 63/100\n","6740/6740 [==============================] - 2s 276us/sample - loss: 0.0264 - accuracy: 0.9954 - val_loss: 0.3584 - val_accuracy: 0.8558\n","Epoch 64/100\n","6740/6740 [==============================] - 2s 277us/sample - loss: 0.0263 - accuracy: 0.9955 - val_loss: 0.3586 - val_accuracy: 0.8558\n","Epoch 65/100\n","6720/6740 [============================>.] - ETA: 0s - loss: 0.0263 - accuracy: 0.9954\n","Epoch 00065: Reducing Max LR on Plateau: new max lr will be 1.5625e-05 (if not early_stopping).\n","6740/6740 [==============================] - 2s 277us/sample - loss: 0.0262 - accuracy: 0.9954 - val_loss: 0.3589 - val_accuracy: 0.8558\n","Epoch 66/100\n","6740/6740 [==============================] - 2s 278us/sample - loss: 0.0262 - accuracy: 0.9955 - val_loss: 0.3590 - val_accuracy: 0.8558\n","Epoch 67/100\n","6740/6740 [==============================] - 2s 279us/sample - loss: 0.0261 - accuracy: 0.9955 - val_loss: 0.3592 - val_accuracy: 0.8558\n","Epoch 68/100\n","6740/6740 [==============================] - 2s 262us/sample - loss: 0.0261 - accuracy: 0.9955 - val_loss: 0.3594 - val_accuracy: 0.8558\n","Epoch 69/100\n","6740/6740 [==============================] - 2s 258us/sample - loss: 0.0260 - accuracy: 0.9955 - val_loss: 0.3596 - val_accuracy: 0.8558\n","Epoch 70/100\n","6620/6740 [============================>.] - ETA: 0s - loss: 0.0259 - accuracy: 0.9955\n","Epoch 00070: Reducing Max LR on Plateau: new max lr will be 7.8125e-06 (if not early_stopping).\n","6740/6740 [==============================] - 2s 265us/sample - loss: 0.0260 - accuracy: 0.9955 - val_loss: 0.3597 - val_accuracy: 0.8558\n","Epoch 71/100\n","6740/6740 [==============================] - 2s 283us/sample - loss: 0.0259 - accuracy: 0.9955 - val_loss: 0.3598 - val_accuracy: 0.8558\n","Epoch 72/100\n","6740/6740 [==============================] - 2s 267us/sample - loss: 0.0259 - accuracy: 0.9955 - val_loss: 0.3599 - val_accuracy: 0.8558\n","Epoch 73/100\n","6740/6740 [==============================] - 2s 270us/sample - loss: 0.0258 - accuracy: 0.9955 - val_loss: 0.3600 - val_accuracy: 0.8558\n","Epoch 74/100\n","6740/6740 [==============================] - 2s 273us/sample - loss: 0.0258 - accuracy: 0.9955 - val_loss: 0.3602 - val_accuracy: 0.8558\n","Epoch 75/100\n","6680/6740 [============================>.] - ETA: 0s - loss: 0.0257 - accuracy: 0.9957\n","Epoch 00075: Reducing Max LR on Plateau: new max lr will be 3.90625e-06 (if not early_stopping).\n","6740/6740 [==============================] - 2s 273us/sample - loss: 0.0257 - accuracy: 0.9955 - val_loss: 0.3603 - val_accuracy: 0.8558\n","Epoch 76/100\n","6740/6740 [==============================] - 2s 273us/sample - loss: 0.0257 - accuracy: 0.9955 - val_loss: 0.3605 - val_accuracy: 0.8558\n","Epoch 77/100\n","6740/6740 [==============================] - 2s 267us/sample - loss: 0.0257 - accuracy: 0.9955 - val_loss: 0.3607 - val_accuracy: 0.8558\n","Epoch 78/100\n","6740/6740 [==============================] - 2s 269us/sample - loss: 0.0256 - accuracy: 0.9955 - val_loss: 0.3609 - val_accuracy: 0.8558\n","Epoch 79/100\n","6740/6740 [==============================] - 2s 266us/sample - loss: 0.0256 - accuracy: 0.9955 - val_loss: 0.3609 - val_accuracy: 0.8558\n","Epoch 80/100\n","6540/6740 [============================>.] - ETA: 0s - loss: 0.0252 - accuracy: 0.9956\n","Epoch 00080: Reducing Max LR on Plateau: new max lr will be 1.953125e-06 (if not early_stopping).\n","6740/6740 [==============================] - 2s 262us/sample - loss: 0.0255 - accuracy: 0.9955 - val_loss: 0.3611 - val_accuracy: 0.8558\n","Epoch 81/100\n","6740/6740 [==============================] - 2s 264us/sample - loss: 0.0255 - accuracy: 0.9955 - val_loss: 0.3613 - val_accuracy: 0.8558\n","Epoch 82/100\n","6740/6740 [==============================] - 2s 259us/sample - loss: 0.0255 - accuracy: 0.9955 - val_loss: 0.3614 - val_accuracy: 0.8558\n","Epoch 83/100\n","6740/6740 [==============================] - 2s 273us/sample - loss: 0.0254 - accuracy: 0.9955 - val_loss: 0.3616 - val_accuracy: 0.8558\n","Epoch 84/100\n","6740/6740 [==============================] - 2s 277us/sample - loss: 0.0254 - accuracy: 0.9955 - val_loss: 0.3617 - val_accuracy: 0.8558\n","Epoch 85/100\n","6660/6740 [============================>.] - ETA: 0s - loss: 0.0251 - accuracy: 0.9956\n","Epoch 00085: Reducing Max LR on Plateau: new max lr will be 9.765625e-07 (if not early_stopping).\n","6740/6740 [==============================] - 2s 284us/sample - loss: 0.0253 - accuracy: 0.9955 - val_loss: 0.3618 - val_accuracy: 0.8558\n","Epoch 86/100\n","6740/6740 [==============================] - 2s 280us/sample - loss: 0.0253 - accuracy: 0.9955 - val_loss: 0.3619 - val_accuracy: 0.8558\n","Epoch 87/100\n","6740/6740 [==============================] - 2s 278us/sample - loss: 0.0253 - accuracy: 0.9955 - val_loss: 0.3619 - val_accuracy: 0.8558\n","Epoch 88/100\n","6740/6740 [==============================] - 2s 271us/sample - loss: 0.0252 - accuracy: 0.9954 - val_loss: 0.3620 - val_accuracy: 0.8558\n","Epoch 89/100\n","6740/6740 [==============================] - 2s 262us/sample - loss: 0.0252 - accuracy: 0.9955 - val_loss: 0.3622 - val_accuracy: 0.8558\n","Epoch 90/100\n","6720/6740 [============================>.] - ETA: 0s - loss: 0.0250 - accuracy: 0.9955\n","Epoch 00090: Reducing Max LR on Plateau: new max lr will be 4.8828125e-07 (if not early_stopping).\n","6740/6740 [==============================] - 2s 254us/sample - loss: 0.0252 - accuracy: 0.9955 - val_loss: 0.3624 - val_accuracy: 0.8558\n","Epoch 91/100\n","6740/6740 [==============================] - 2s 259us/sample - loss: 0.0251 - accuracy: 0.9955 - val_loss: 0.3625 - val_accuracy: 0.8558\n","Epoch 92/100\n","6740/6740 [==============================] - 2s 264us/sample - loss: 0.0251 - accuracy: 0.9954 - val_loss: 0.3624 - val_accuracy: 0.8558\n","Epoch 93/100\n","6740/6740 [==============================] - 2s 267us/sample - loss: 0.0250 - accuracy: 0.9955 - val_loss: 0.3626 - val_accuracy: 0.8558\n","Epoch 94/100\n","6740/6740 [==============================] - 2s 267us/sample - loss: 0.0250 - accuracy: 0.9955 - val_loss: 0.3628 - val_accuracy: 0.8558\n","Epoch 95/100\n","6620/6740 [============================>.] - ETA: 0s - loss: 0.0248 - accuracy: 0.9956\n","Epoch 00095: Reducing Max LR on Plateau: new max lr will be 2.44140625e-07 (if not early_stopping).\n","6740/6740 [==============================] - 2s 271us/sample - loss: 0.0250 - accuracy: 0.9954 - val_loss: 0.3629 - val_accuracy: 0.8558\n","Epoch 96/100\n","6740/6740 [==============================] - 2s 273us/sample - loss: 0.0249 - accuracy: 0.9955 - val_loss: 0.3632 - val_accuracy: 0.8558\n","Epoch 97/100\n","6740/6740 [==============================] - 2s 277us/sample - loss: 0.0249 - accuracy: 0.9955 - val_loss: 0.3632 - val_accuracy: 0.8558\n","Epoch 98/100\n","6740/6740 [==============================] - 2s 263us/sample - loss: 0.0249 - accuracy: 0.9955 - val_loss: 0.3633 - val_accuracy: 0.8571\n","Epoch 99/100\n","6740/6740 [==============================] - 2s 268us/sample - loss: 0.0248 - accuracy: 0.9955 - val_loss: 0.3634 - val_accuracy: 0.8571\n","Epoch 100/100\n","6600/6740 [============================>.] - ETA: 0s - loss: 0.0248 - accuracy: 0.9955\n","Epoch 00100: Reducing Max LR on Plateau: new max lr will be 1.220703125e-07 (if not early_stopping).\n","6740/6740 [==============================] - 2s 260us/sample - loss: 0.0248 - accuracy: 0.9955 - val_loss: 0.3635 - val_accuracy: 0.8571\n","Weights from best epoch have been loaded into model.\n","                   precision    recall  f1-score   support\n","\n","           slight       0.84      0.76      0.79       268\n","   very_offensive       0.75      0.81      0.78       219\n","hateful_offensive       0.97      1.00      0.99       262\n","\n","         accuracy                           0.86       749\n","        macro avg       0.85      0.86      0.85       749\n","     weighted avg       0.86      0.86      0.86       749\n","\n","model /content/drive/My Drive/memotion/wgts/off22slight_very_offensive_hateful_offensive.h5\n","Model Saved\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Xjb_Esp5XkxE","colab_type":"code","outputId":"17a8a015-49ae-4491-dee3-66f06e7d6505","executionInfo":{"status":"ok","timestamp":1584079598908,"user_tz":-330,"elapsed":261888,"user":{"displayName":"Harsh Kataria","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimH0VsMEaiEzaEeo5nJzMe1WuJRQ2b1DK9fb3S=s64","userId":"07300304294680146354"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import ktrain\n","from ktrain import text\n","\n","\n","\n","modelslist = [['general','twisted_meaning','very_twisted']]\n","data_files = ['memotion_eq_onlysarcastic22.csv']\n","for columnclass, data_file in zip(modelslist, data_files):\n","\n","    \n","    columns=columnclass      \n","    DATA_PATH = '/content/drive/My Drive/memotion/' + data_file\n","    print(DATA_PATH)\n","\n","    NUM_WORDS = 90000\n","    MAXLEN = 250\n","\n","    h5name = '_'.join(columnclass)\n","    print('model', h5name)\n","\n","    (x_train, y_train), (x_test, y_test), preproc = text.texts_from_csv(DATA_PATH,\n","                          'cleaned_ocr',\n","                          label_columns = columns,\n","                          val_filepath=None, # if None, 10% of data will be used for validation\n","                          max_features=NUM_WORDS, maxlen=MAXLEN,\n","                        ngram_range=5,)\n","\n","\n","    model = text.text_classifier('logreg', (x_train, y_train), preproc=preproc,)\n","\n","    learner = ktrain.get_learner(model, train_data=(x_train, y_train), val_data=(x_test, y_test),batch_size=20)\n","    a='/content/drive/My Drive/Weight_file/'+h5name\n","    learner.autofit(0.001, 100,early_stopping=80, reduce_on_plateau=5,)\n","\n","    a=learner.validate(val_data=(x_test, y_test), class_names=columns)\n","  \n","    predictor = ktrain.get_predictor(learner.model, preproc)\n","\n","   # score = 'xyz'########################\n","    h5name = \"/content/drive/My Drive/memotion/wgts/sarcastic22\"+h5name +  '.h5'\n","    print('model', h5name)\n","\n","    predictor.save(h5name)\n","    print(\"Model Saved\")\n"],"execution_count":8,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/memotion/memotion_eq_onlysarcastic22.csv\n","model general_twisted_meaning_very_twisted\n","detected encoding: utf-8 (if wrong, set manually)\n","language: en\n","Word Counts: 9877\n","Nrows: 9132\n","9132 train sequences\n","train sequence lengths:\n","\tmean : 8\n","\t95percentile : 17\n","\t99percentile : 26\n","Adding 5-gram features\n","max_features changed to 196664 with addition of ngrams\n","Average train sequence length with ngrams: 30\n","train (w/ngrams) sequence lengths:\n","\tmean : 30\n","\t95percentile : 75\n","\t99percentile : 120\n","x_train shape: (9132,250)\n","y_train shape: (9132, 3)\n","1015 test sequences\n","test sequence lengths:\n","\tmean : 8\n","\t95percentile : 17\n","\t99percentile : 25\n","Average test sequence length with ngrams: 22\n","test (w/ngrams) sequence lengths:\n","\tmean : 23\n","\t95percentile : 65\n","\t99percentile : 110\n","x_test shape: (1015,250)\n","y_test shape: (1015, 3)\n","Is Multi-Label? False\n","compiling word ID features...\n","maxlen is 250\n","done.\n","\n","\n","begin training using triangular learning rate policy with max lr of 0.001...\n","Train on 9132 samples, validate on 1015 samples\n","Epoch 1/100\n","9132/9132 [==============================] - 4s 385us/sample - loss: 0.9592 - accuracy: 0.6612 - val_loss: 0.8410 - val_accuracy: 0.8236\n","Epoch 2/100\n","9132/9132 [==============================] - 3s 287us/sample - loss: 0.6661 - accuracy: 0.9548 - val_loss: 0.7018 - val_accuracy: 0.8759\n","Epoch 3/100\n","9132/9132 [==============================] - 3s 290us/sample - loss: 0.4956 - accuracy: 0.9778 - val_loss: 0.6141 - val_accuracy: 0.8867\n","Epoch 4/100\n","9132/9132 [==============================] - 3s 281us/sample - loss: 0.3902 - accuracy: 0.9830 - val_loss: 0.5537 - val_accuracy: 0.8966\n","Epoch 5/100\n","9132/9132 [==============================] - 3s 285us/sample - loss: 0.3189 - accuracy: 0.9860 - val_loss: 0.5093 - val_accuracy: 0.8966\n","Epoch 6/100\n","9132/9132 [==============================] - 3s 283us/sample - loss: 0.2676 - accuracy: 0.9881 - val_loss: 0.4754 - val_accuracy: 0.8985\n","Epoch 7/100\n","9132/9132 [==============================] - 3s 285us/sample - loss: 0.2290 - accuracy: 0.9893 - val_loss: 0.4463 - val_accuracy: 0.9044\n","Epoch 8/100\n","9132/9132 [==============================] - 3s 279us/sample - loss: 0.1987 - accuracy: 0.9909 - val_loss: 0.4234 - val_accuracy: 0.9005\n","Epoch 9/100\n","9132/9132 [==============================] - 3s 288us/sample - loss: 0.1744 - accuracy: 0.9921 - val_loss: 0.4040 - val_accuracy: 0.9005\n","Epoch 10/100\n","9132/9132 [==============================] - 3s 276us/sample - loss: 0.1545 - accuracy: 0.9927 - val_loss: 0.3873 - val_accuracy: 0.9054\n","Epoch 11/100\n","9132/9132 [==============================] - 3s 279us/sample - loss: 0.1380 - accuracy: 0.9936 - val_loss: 0.3720 - val_accuracy: 0.9054\n","Epoch 12/100\n","9132/9132 [==============================] - 3s 288us/sample - loss: 0.1238 - accuracy: 0.9940 - val_loss: 0.3595 - val_accuracy: 0.9025\n","Epoch 13/100\n","9132/9132 [==============================] - 3s 287us/sample - loss: 0.1120 - accuracy: 0.9945 - val_loss: 0.3487 - val_accuracy: 0.9034\n","Epoch 14/100\n","9132/9132 [==============================] - 3s 291us/sample - loss: 0.1017 - accuracy: 0.9950 - val_loss: 0.3385 - val_accuracy: 0.9044\n","Epoch 15/100\n","9132/9132 [==============================] - 3s 289us/sample - loss: 0.0931 - accuracy: 0.9952 - val_loss: 0.3300 - val_accuracy: 0.9044\n","Epoch 16/100\n","9132/9132 [==============================] - 3s 281us/sample - loss: 0.0854 - accuracy: 0.9950 - val_loss: 0.3217 - val_accuracy: 0.9034\n","Epoch 17/100\n","9132/9132 [==============================] - 3s 295us/sample - loss: 0.0786 - accuracy: 0.9952 - val_loss: 0.3154 - val_accuracy: 0.9054\n","Epoch 18/100\n","9132/9132 [==============================] - 3s 301us/sample - loss: 0.0727 - accuracy: 0.9950 - val_loss: 0.3086 - val_accuracy: 0.9025\n","Epoch 19/100\n","9132/9132 [==============================] - 3s 294us/sample - loss: 0.0672 - accuracy: 0.9954 - val_loss: 0.3031 - val_accuracy: 0.9025\n","Epoch 20/100\n","9132/9132 [==============================] - 3s 297us/sample - loss: 0.0626 - accuracy: 0.9954 - val_loss: 0.2969 - val_accuracy: 0.9054\n","Epoch 21/100\n","9132/9132 [==============================] - 3s 292us/sample - loss: 0.0583 - accuracy: 0.9953 - val_loss: 0.2928 - val_accuracy: 0.9084\n","Epoch 22/100\n","9132/9132 [==============================] - 3s 298us/sample - loss: 0.0546 - accuracy: 0.9953 - val_loss: 0.2893 - val_accuracy: 0.9064\n","Epoch 23/100\n","9132/9132 [==============================] - 3s 297us/sample - loss: 0.0513 - accuracy: 0.9953 - val_loss: 0.2853 - val_accuracy: 0.9064\n","Epoch 24/100\n","9132/9132 [==============================] - 3s 293us/sample - loss: 0.0482 - accuracy: 0.9955 - val_loss: 0.2821 - val_accuracy: 0.9034\n","Epoch 25/100\n","9132/9132 [==============================] - 3s 291us/sample - loss: 0.0455 - accuracy: 0.9956 - val_loss: 0.2783 - val_accuracy: 0.9054\n","Epoch 26/100\n","9132/9132 [==============================] - 3s 290us/sample - loss: 0.0431 - accuracy: 0.9957 - val_loss: 0.2764 - val_accuracy: 0.9064\n","Epoch 27/100\n","9132/9132 [==============================] - 3s 284us/sample - loss: 0.0408 - accuracy: 0.9958 - val_loss: 0.2739 - val_accuracy: 0.9054\n","Epoch 28/100\n","9132/9132 [==============================] - 3s 282us/sample - loss: 0.0386 - accuracy: 0.9957 - val_loss: 0.2728 - val_accuracy: 0.9034\n","Epoch 29/100\n","9132/9132 [==============================] - 3s 283us/sample - loss: 0.0368 - accuracy: 0.9958 - val_loss: 0.2704 - val_accuracy: 0.9025\n","Epoch 30/100\n","9132/9132 [==============================] - 3s 286us/sample - loss: 0.0349 - accuracy: 0.9961 - val_loss: 0.2692 - val_accuracy: 0.9005\n","Epoch 31/100\n","9132/9132 [==============================] - 3s 280us/sample - loss: 0.0334 - accuracy: 0.9959 - val_loss: 0.2674 - val_accuracy: 0.8995\n","Epoch 32/100\n","9132/9132 [==============================] - 3s 275us/sample - loss: 0.0320 - accuracy: 0.9963 - val_loss: 0.2668 - val_accuracy: 0.8995\n","Epoch 33/100\n","9132/9132 [==============================] - 3s 291us/sample - loss: 0.0305 - accuracy: 0.9965 - val_loss: 0.2649 - val_accuracy: 0.8975\n","Epoch 34/100\n","9132/9132 [==============================] - 3s 281us/sample - loss: 0.0293 - accuracy: 0.9964 - val_loss: 0.2643 - val_accuracy: 0.8966\n","Epoch 35/100\n","9132/9132 [==============================] - 3s 282us/sample - loss: 0.0280 - accuracy: 0.9964 - val_loss: 0.2641 - val_accuracy: 0.8956\n","Epoch 36/100\n","9132/9132 [==============================] - 3s 276us/sample - loss: 0.0271 - accuracy: 0.9962 - val_loss: 0.2628 - val_accuracy: 0.8966\n","Epoch 37/100\n","9132/9132 [==============================] - 3s 283us/sample - loss: 0.0261 - accuracy: 0.9962 - val_loss: 0.2622 - val_accuracy: 0.8946\n","Epoch 38/100\n","9132/9132 [==============================] - 3s 278us/sample - loss: 0.0249 - accuracy: 0.9965 - val_loss: 0.2620 - val_accuracy: 0.8936\n","Epoch 39/100\n","9132/9132 [==============================] - 2s 270us/sample - loss: 0.0241 - accuracy: 0.9963 - val_loss: 0.2613 - val_accuracy: 0.8936\n","Epoch 40/100\n","9132/9132 [==============================] - 3s 282us/sample - loss: 0.0234 - accuracy: 0.9963 - val_loss: 0.2611 - val_accuracy: 0.8926\n","Epoch 41/100\n","9132/9132 [==============================] - 3s 280us/sample - loss: 0.0229 - accuracy: 0.9959 - val_loss: 0.2614 - val_accuracy: 0.8956\n","Epoch 42/100\n","9132/9132 [==============================] - 3s 284us/sample - loss: 0.0220 - accuracy: 0.9962 - val_loss: 0.2615 - val_accuracy: 0.8946\n","Epoch 43/100\n","9132/9132 [==============================] - 3s 284us/sample - loss: 0.0213 - accuracy: 0.9959 - val_loss: 0.2618 - val_accuracy: 0.8946\n","Epoch 44/100\n","9132/9132 [==============================] - 2s 265us/sample - loss: 0.0207 - accuracy: 0.9963 - val_loss: 0.2619 - val_accuracy: 0.8946\n","Epoch 45/100\n","9060/9132 [============================>.] - ETA: 0s - loss: 0.0201 - accuracy: 0.9966\n","Epoch 00045: Reducing Max LR on Plateau: new max lr will be 0.0005 (if not early_stopping).\n","9132/9132 [==============================] - 2s 270us/sample - loss: 0.0200 - accuracy: 0.9966 - val_loss: 0.2626 - val_accuracy: 0.8946\n","Epoch 46/100\n","9132/9132 [==============================] - 3s 275us/sample - loss: 0.0195 - accuracy: 0.9966 - val_loss: 0.2626 - val_accuracy: 0.8956\n","Epoch 47/100\n","9132/9132 [==============================] - 3s 278us/sample - loss: 0.0191 - accuracy: 0.9965 - val_loss: 0.2629 - val_accuracy: 0.8956\n","Epoch 48/100\n","9132/9132 [==============================] - 3s 276us/sample - loss: 0.0188 - accuracy: 0.9965 - val_loss: 0.2629 - val_accuracy: 0.8956\n","Epoch 49/100\n","9132/9132 [==============================] - 3s 281us/sample - loss: 0.0185 - accuracy: 0.9967 - val_loss: 0.2632 - val_accuracy: 0.8956\n","Epoch 50/100\n","9120/9132 [============================>.] - ETA: 0s - loss: 0.0183 - accuracy: 0.9964\n","Epoch 00050: Reducing Max LR on Plateau: new max lr will be 0.00025 (if not early_stopping).\n","9132/9132 [==============================] - 3s 302us/sample - loss: 0.0183 - accuracy: 0.9964 - val_loss: 0.2638 - val_accuracy: 0.8956\n","Epoch 51/100\n","9132/9132 [==============================] - 3s 305us/sample - loss: 0.0180 - accuracy: 0.9965 - val_loss: 0.2639 - val_accuracy: 0.8956\n","Epoch 52/100\n","9132/9132 [==============================] - 3s 281us/sample - loss: 0.0178 - accuracy: 0.9965 - val_loss: 0.2641 - val_accuracy: 0.8956\n","Epoch 53/100\n","9132/9132 [==============================] - 3s 291us/sample - loss: 0.0177 - accuracy: 0.9966 - val_loss: 0.2644 - val_accuracy: 0.8956\n","Epoch 54/100\n","9132/9132 [==============================] - 3s 288us/sample - loss: 0.0175 - accuracy: 0.9967 - val_loss: 0.2648 - val_accuracy: 0.8956\n","Epoch 55/100\n","8980/9132 [============================>.] - ETA: 0s - loss: 0.0174 - accuracy: 0.9968\n","Epoch 00055: Reducing Max LR on Plateau: new max lr will be 0.000125 (if not early_stopping).\n","9132/9132 [==============================] - 3s 289us/sample - loss: 0.0174 - accuracy: 0.9967 - val_loss: 0.2652 - val_accuracy: 0.8956\n","Epoch 56/100\n","9132/9132 [==============================] - 3s 282us/sample - loss: 0.0172 - accuracy: 0.9967 - val_loss: 0.2653 - val_accuracy: 0.8956\n","Epoch 57/100\n","9132/9132 [==============================] - 3s 284us/sample - loss: 0.0171 - accuracy: 0.9967 - val_loss: 0.2655 - val_accuracy: 0.8956\n","Epoch 58/100\n","9132/9132 [==============================] - 3s 291us/sample - loss: 0.0171 - accuracy: 0.9967 - val_loss: 0.2657 - val_accuracy: 0.8956\n","Epoch 59/100\n","9132/9132 [==============================] - 3s 291us/sample - loss: 0.0170 - accuracy: 0.9967 - val_loss: 0.2659 - val_accuracy: 0.8956\n","Epoch 60/100\n","9080/9132 [============================>.] - ETA: 0s - loss: 0.0170 - accuracy: 0.9967\n","Epoch 00060: Reducing Max LR on Plateau: new max lr will be 6.25e-05 (if not early_stopping).\n","9132/9132 [==============================] - 3s 280us/sample - loss: 0.0169 - accuracy: 0.9967 - val_loss: 0.2660 - val_accuracy: 0.8956\n","Epoch 61/100\n","9132/9132 [==============================] - 3s 280us/sample - loss: 0.0168 - accuracy: 0.9967 - val_loss: 0.2662 - val_accuracy: 0.8956\n","Epoch 62/100\n","9132/9132 [==============================] - 2s 274us/sample - loss: 0.0168 - accuracy: 0.9967 - val_loss: 0.2663 - val_accuracy: 0.8956\n","Epoch 63/100\n","9132/9132 [==============================] - 3s 296us/sample - loss: 0.0167 - accuracy: 0.9967 - val_loss: 0.2664 - val_accuracy: 0.8956\n","Epoch 64/100\n","9132/9132 [==============================] - 3s 292us/sample - loss: 0.0167 - accuracy: 0.9967 - val_loss: 0.2666 - val_accuracy: 0.8956\n","Epoch 65/100\n","9100/9132 [============================>.] - ETA: 0s - loss: 0.0167 - accuracy: 0.9967\n","Epoch 00065: Reducing Max LR on Plateau: new max lr will be 3.125e-05 (if not early_stopping).\n","9132/9132 [==============================] - 3s 285us/sample - loss: 0.0166 - accuracy: 0.9967 - val_loss: 0.2667 - val_accuracy: 0.8956\n","Epoch 66/100\n","9132/9132 [==============================] - 3s 285us/sample - loss: 0.0166 - accuracy: 0.9967 - val_loss: 0.2668 - val_accuracy: 0.8956\n","Epoch 67/100\n","9132/9132 [==============================] - 3s 281us/sample - loss: 0.0166 - accuracy: 0.9967 - val_loss: 0.2669 - val_accuracy: 0.8956\n","Epoch 68/100\n","9132/9132 [==============================] - 3s 280us/sample - loss: 0.0165 - accuracy: 0.9967 - val_loss: 0.2669 - val_accuracy: 0.8956\n","Epoch 69/100\n","9132/9132 [==============================] - 3s 282us/sample - loss: 0.0165 - accuracy: 0.9967 - val_loss: 0.2671 - val_accuracy: 0.8956\n","Epoch 70/100\n","8980/9132 [============================>.] - ETA: 0s - loss: 0.0163 - accuracy: 0.9968\n","Epoch 00070: Reducing Max LR on Plateau: new max lr will be 1.5625e-05 (if not early_stopping).\n","9132/9132 [==============================] - 3s 285us/sample - loss: 0.0165 - accuracy: 0.9967 - val_loss: 0.2672 - val_accuracy: 0.8956\n","Epoch 71/100\n","9132/9132 [==============================] - 3s 287us/sample - loss: 0.0164 - accuracy: 0.9967 - val_loss: 0.2672 - val_accuracy: 0.8946\n","Epoch 72/100\n","9132/9132 [==============================] - 3s 288us/sample - loss: 0.0164 - accuracy: 0.9967 - val_loss: 0.2673 - val_accuracy: 0.8946\n","Epoch 73/100\n","9132/9132 [==============================] - 2s 273us/sample - loss: 0.0164 - accuracy: 0.9967 - val_loss: 0.2674 - val_accuracy: 0.8946\n","Epoch 74/100\n","9132/9132 [==============================] - 3s 277us/sample - loss: 0.0163 - accuracy: 0.9966 - val_loss: 0.2675 - val_accuracy: 0.8936\n","Epoch 75/100\n","9040/9132 [============================>.] - ETA: 0s - loss: 0.0164 - accuracy: 0.9967\n","Epoch 00075: Reducing Max LR on Plateau: new max lr will be 7.8125e-06 (if not early_stopping).\n","9132/9132 [==============================] - 2s 274us/sample - loss: 0.0163 - accuracy: 0.9967 - val_loss: 0.2675 - val_accuracy: 0.8936\n","Epoch 76/100\n","9132/9132 [==============================] - 3s 283us/sample - loss: 0.0163 - accuracy: 0.9966 - val_loss: 0.2676 - val_accuracy: 0.8936\n","Epoch 77/100\n","9132/9132 [==============================] - 3s 274us/sample - loss: 0.0163 - accuracy: 0.9966 - val_loss: 0.2676 - val_accuracy: 0.8926\n","Epoch 78/100\n","9132/9132 [==============================] - 2s 272us/sample - loss: 0.0162 - accuracy: 0.9966 - val_loss: 0.2677 - val_accuracy: 0.8926\n","Epoch 79/100\n","9132/9132 [==============================] - 3s 275us/sample - loss: 0.0162 - accuracy: 0.9966 - val_loss: 0.2678 - val_accuracy: 0.8926\n","Epoch 80/100\n","8940/9132 [============================>.] - ETA: 0s - loss: 0.0164 - accuracy: 0.9966\n","Epoch 00080: Reducing Max LR on Plateau: new max lr will be 3.90625e-06 (if not early_stopping).\n","9132/9132 [==============================] - 3s 278us/sample - loss: 0.0162 - accuracy: 0.9967 - val_loss: 0.2678 - val_accuracy: 0.8916\n","Epoch 81/100\n","9132/9132 [==============================] - 3s 275us/sample - loss: 0.0162 - accuracy: 0.9966 - val_loss: 0.2678 - val_accuracy: 0.8926\n","Epoch 82/100\n","9132/9132 [==============================] - 3s 280us/sample - loss: 0.0162 - accuracy: 0.9967 - val_loss: 0.2678 - val_accuracy: 0.8926\n","Epoch 83/100\n","9132/9132 [==============================] - 3s 279us/sample - loss: 0.0161 - accuracy: 0.9967 - val_loss: 0.2679 - val_accuracy: 0.8926\n","Epoch 84/100\n","9132/9132 [==============================] - 3s 278us/sample - loss: 0.0161 - accuracy: 0.9967 - val_loss: 0.2680 - val_accuracy: 0.8926\n","Epoch 85/100\n","9060/9132 [============================>.] - ETA: 0s - loss: 0.0162 - accuracy: 0.9967\n","Epoch 00085: Reducing Max LR on Plateau: new max lr will be 1.953125e-06 (if not early_stopping).\n","9132/9132 [==============================] - 2s 272us/sample - loss: 0.0161 - accuracy: 0.9967 - val_loss: 0.2681 - val_accuracy: 0.8926\n","Epoch 86/100\n","9132/9132 [==============================] - 2s 272us/sample - loss: 0.0161 - accuracy: 0.9966 - val_loss: 0.2683 - val_accuracy: 0.8926\n","Epoch 87/100\n","9132/9132 [==============================] - 3s 282us/sample - loss: 0.0160 - accuracy: 0.9967 - val_loss: 0.2684 - val_accuracy: 0.8926\n","Epoch 88/100\n","9132/9132 [==============================] - 3s 284us/sample - loss: 0.0160 - accuracy: 0.9966 - val_loss: 0.2684 - val_accuracy: 0.8916\n","Epoch 89/100\n","9132/9132 [==============================] - 3s 275us/sample - loss: 0.0160 - accuracy: 0.9967 - val_loss: 0.2685 - val_accuracy: 0.8926\n","Epoch 90/100\n","9000/9132 [============================>.] - ETA: 0s - loss: 0.0160 - accuracy: 0.9967\n","Epoch 00090: Reducing Max LR on Plateau: new max lr will be 9.765625e-07 (if not early_stopping).\n","9132/9132 [==============================] - 3s 275us/sample - loss: 0.0160 - accuracy: 0.9967 - val_loss: 0.2685 - val_accuracy: 0.8926\n","Epoch 91/100\n","9132/9132 [==============================] - 3s 276us/sample - loss: 0.0160 - accuracy: 0.9967 - val_loss: 0.2685 - val_accuracy: 0.8926\n","Epoch 92/100\n","9132/9132 [==============================] - 3s 278us/sample - loss: 0.0160 - accuracy: 0.9967 - val_loss: 0.2685 - val_accuracy: 0.8926\n","Epoch 93/100\n","9132/9132 [==============================] - 3s 282us/sample - loss: 0.0159 - accuracy: 0.9967 - val_loss: 0.2687 - val_accuracy: 0.8926\n","Epoch 94/100\n","9132/9132 [==============================] - 3s 283us/sample - loss: 0.0159 - accuracy: 0.9966 - val_loss: 0.2687 - val_accuracy: 0.8916\n","Epoch 95/100\n","9060/9132 [============================>.] - ETA: 0s - loss: 0.0159 - accuracy: 0.9967\n","Epoch 00095: Reducing Max LR on Plateau: new max lr will be 4.8828125e-07 (if not early_stopping).\n","9132/9132 [==============================] - 3s 279us/sample - loss: 0.0159 - accuracy: 0.9967 - val_loss: 0.2688 - val_accuracy: 0.8916\n","Epoch 96/100\n","9132/9132 [==============================] - 3s 274us/sample - loss: 0.0159 - accuracy: 0.9967 - val_loss: 0.2688 - val_accuracy: 0.8916\n","Epoch 97/100\n","9132/9132 [==============================] - 3s 274us/sample - loss: 0.0159 - accuracy: 0.9967 - val_loss: 0.2689 - val_accuracy: 0.8916\n","Epoch 98/100\n","9132/9132 [==============================] - 2s 272us/sample - loss: 0.0158 - accuracy: 0.9967 - val_loss: 0.2691 - val_accuracy: 0.8916\n","Epoch 99/100\n","9132/9132 [==============================] - 2s 271us/sample - loss: 0.0158 - accuracy: 0.9966 - val_loss: 0.2692 - val_accuracy: 0.8936\n","Epoch 100/100\n","8980/9132 [============================>.] - ETA: 0s - loss: 0.0158 - accuracy: 0.9968\n","Epoch 00100: Reducing Max LR on Plateau: new max lr will be 2.44140625e-07 (if not early_stopping).\n","9132/9132 [==============================] - 3s 274us/sample - loss: 0.0158 - accuracy: 0.9967 - val_loss: 0.2693 - val_accuracy: 0.8936\n","Weights from best epoch have been loaded into model.\n","                 precision    recall  f1-score   support\n","\n","        general       0.92      0.78      0.84       363\n","twisted_meaning       0.81      0.92      0.86       321\n","   very_twisted       0.97      1.00      0.98       331\n","\n","       accuracy                           0.89      1015\n","      macro avg       0.90      0.90      0.89      1015\n","   weighted avg       0.90      0.89      0.89      1015\n","\n","model /content/drive/My Drive/memotion/wgts/sarcastic22general_twisted_meaning_very_twisted.h5\n","Model Saved\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mnhMIfEqYLXt","colab_type":"code","outputId":"fc7326cd-7f09-4eba-a943-8a0ed40f9282","executionInfo":{"status":"error","timestamp":1584011470057,"user_tz":-330,"elapsed":118366,"user":{"displayName":"Harsh Kataria","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimH0VsMEaiEzaEeo5nJzMe1WuJRQ2b1DK9fb3S=s64","userId":"07300304294680146354"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import ktrain\n","from ktrain import text\n","\n","\n","\n","modelslist = [['slight','hateful_offensive','not_offensive','very_offensive']]\n","data_files = ['memotion_eq_alloffensive.csv']\n","for columnclass, data_file in zip(modelslist, data_files):\n","\n","    \n","    columns=columnclass      \n","    DATA_PATH = '/content/drive/My Drive/memotion/' + data_file\n","    print(DATA_PATH)\n","\n","    NUM_WORDS = 90000\n","    MAXLEN = 250\n","\n","    h5name = '_'.join(columnclass)\n","    print('model', h5name)\n","\n","    (x_train, y_train), (x_test, y_test), preproc = text.texts_from_csv(DATA_PATH,\n","                          'cleaned_ocr',\n","                          label_columns = columns,\n","                          val_filepath=None, # if None, 10% of data will be used for validation\n","                          max_features=NUM_WORDS, maxlen=MAXLEN,\n","                        ngram_range=9,)\n","\n","\n","    model = text.text_classifier('logreg', (x_train, y_train), preproc=preproc,)\n","\n","    learner = ktrain.get_learner(model, train_data=(x_train, y_train), val_data=(x_test, y_test),batch_size=20)\n","    a='/content/drive/My Drive/Weight_file/'+h5name\n","    learner.autofit(0.05, 100,early_stopping=80, reduce_on_plateau=5,)\n","\n","    a=learner.validate(val_data=(x_test, y_test), class_names=columns)\n","  \n","    predictor = ktrain.get_predictor(learner.model, preproc)\n","\n","   # score = 'xyz'########################\n","    h5name = \"/content/drive/My Drive/memotion/wgts/alloffensive\"+h5name +  '.h5'\n","    print('model', h5name)\n","\n","    predictor.save(h5name)\n","    print(\"Model Saved\")\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/memotion/memotion_eq_alloffensive.csv\n","model slight_hateful_offensive_not_offensive_very_offensive\n","detected encoding: utf-8 (if wrong, set manually)\n","language: en\n","Word Counts: 11446\n","Nrows: 9181\n","9181 train sequences\n","train sequence lengths:\n","\tmean : 8\n","\t95percentile : 18\n","\t99percentile : 27\n","Adding 9-gram features\n","max_features changed to 290488 with addition of ngrams\n","Average train sequence length with ngrams: 41\n","train (w/ngrams) sequence lengths:\n","\tmean : 42\n","\t95percentile : 126\n","\t99percentile : 207\n","x_train shape: (9181,250)\n","y_train shape: (9181, 4)\n","1021 test sequences\n","test sequence lengths:\n","\tmean : 7\n","\t95percentile : 16\n","\t99percentile : 26\n","Average test sequence length with ngrams: 23\n","test (w/ngrams) sequence lengths:\n","\tmean : 23\n","\t95percentile : 81\n","\t99percentile : 180\n","x_test shape: (1021,250)\n","y_test shape: (1021, 4)\n","Is Multi-Label? False\n","compiling word ID features...\n","maxlen is 250\n","done.\n","\n","\n","begin training using triangular learning rate policy with max lr of 0.05...\n","Train on 9181 samples, validate on 1021 samples\n","Epoch 1/100\n","9181/9181 [==============================] - 4s 452us/sample - loss: 1.3053 - accuracy: 0.5271 - val_loss: 1.1665 - val_accuracy: 0.6327\n","Epoch 2/100\n","9181/9181 [==============================] - 4s 388us/sample - loss: 0.2749 - accuracy: 0.9620 - val_loss: 1.1603 - val_accuracy: 0.6396\n","Epoch 3/100\n","9181/9181 [==============================] - 4s 394us/sample - loss: 0.2012 - accuracy: 0.9832 - val_loss: 1.1715 - val_accuracy: 0.6386\n","Epoch 4/100\n","9181/9181 [==============================] - 4s 395us/sample - loss: 0.1951 - accuracy: 0.9863 - val_loss: 1.1666 - val_accuracy: 0.6425\n","Epoch 5/100\n","9181/9181 [==============================] - 4s 390us/sample - loss: 0.1304 - accuracy: 0.9878 - val_loss: 1.1979 - val_accuracy: 0.6366\n","Epoch 6/100\n","9181/9181 [==============================] - 4s 394us/sample - loss: 0.1579 - accuracy: 0.9891 - val_loss: 1.2750 - val_accuracy: 0.6435\n","Epoch 7/100\n","9120/9181 [============================>.] - ETA: 0s - loss: 0.1294 - accuracy: 0.9897\n","Epoch 00007: Reducing Max LR on Plateau: new max lr will be 0.025 (if not early_stopping).\n","9181/9181 [==============================] - 4s 394us/sample - loss: 0.1287 - accuracy: 0.9897 - val_loss: 1.2556 - val_accuracy: 0.6298\n","Epoch 8/100\n","9181/9181 [==============================] - 4s 396us/sample - loss: 0.1289 - accuracy: 0.9897 - val_loss: 1.2429 - val_accuracy: 0.6425\n","Epoch 9/100\n","9181/9181 [==============================] - 4s 395us/sample - loss: 0.0885 - accuracy: 0.9893 - val_loss: 1.2580 - val_accuracy: 0.6445\n","Epoch 10/100\n","9181/9181 [==============================] - 4s 392us/sample - loss: 0.1002 - accuracy: 0.9897 - val_loss: 1.2691 - val_accuracy: 0.6386\n","Epoch 11/100\n","9181/9181 [==============================] - 4s 397us/sample - loss: 0.0817 - accuracy: 0.9894 - val_loss: 1.2804 - val_accuracy: 0.6415\n","Epoch 12/100\n","9120/9181 [============================>.] - ETA: 0s - loss: 0.0800 - accuracy: 0.9895\n","Epoch 00012: Reducing Max LR on Plateau: new max lr will be 0.0125 (if not early_stopping).\n","9181/9181 [==============================] - 4s 400us/sample - loss: 0.0798 - accuracy: 0.9893 - val_loss: 1.2890 - val_accuracy: 0.6435\n","Epoch 13/100\n","9181/9181 [==============================] - 4s 401us/sample - loss: 0.0587 - accuracy: 0.9895 - val_loss: 1.2979 - val_accuracy: 0.6405\n","Epoch 14/100\n","9181/9181 [==============================] - 4s 400us/sample - loss: 0.0597 - accuracy: 0.9889 - val_loss: 1.3119 - val_accuracy: 0.6366\n","Epoch 15/100\n","9181/9181 [==============================] - 4s 390us/sample - loss: 0.0513 - accuracy: 0.9889 - val_loss: 1.3281 - val_accuracy: 0.6376\n","Epoch 16/100\n","9181/9181 [==============================] - 4s 392us/sample - loss: 0.0526 - accuracy: 0.9891 - val_loss: 1.3383 - val_accuracy: 0.6376\n","Epoch 17/100\n","9060/9181 [============================>.] - ETA: 0s - loss: 0.0461 - accuracy: 0.9897\n","Epoch 00017: Reducing Max LR on Plateau: new max lr will be 0.00625 (if not early_stopping).\n","9181/9181 [==============================] - 4s 386us/sample - loss: 0.0456 - accuracy: 0.9899 - val_loss: 1.3547 - val_accuracy: 0.6347\n","Epoch 18/100\n","9181/9181 [==============================] - 4s 391us/sample - loss: 0.0462 - accuracy: 0.9898 - val_loss: 1.3586 - val_accuracy: 0.6376\n","Epoch 19/100\n","9181/9181 [==============================] - 4s 388us/sample - loss: 0.0345 - accuracy: 0.9888 - val_loss: 1.3657 - val_accuracy: 0.6376\n","Epoch 20/100\n","9181/9181 [==============================] - 4s 389us/sample - loss: 0.0340 - accuracy: 0.9891 - val_loss: 1.3789 - val_accuracy: 0.6396\n","Epoch 21/100\n","9181/9181 [==============================] - 4s 382us/sample - loss: 0.0295 - accuracy: 0.9895 - val_loss: 1.3930 - val_accuracy: 0.6357\n","Epoch 22/100\n","9080/9181 [============================>.] - ETA: 0s - loss: 0.0327 - accuracy: 0.9889\n","Epoch 00022: Reducing Max LR on Plateau: new max lr will be 0.003125 (if not early_stopping).\n","9181/9181 [==============================] - 4s 387us/sample - loss: 0.0324 - accuracy: 0.9890 - val_loss: 1.4016 - val_accuracy: 0.6357\n","Epoch 23/100\n","9181/9181 [==============================] - 4s 391us/sample - loss: 0.0224 - accuracy: 0.9905 - val_loss: 1.4093 - val_accuracy: 0.6347\n","Epoch 24/100\n","9181/9181 [==============================] - 4s 385us/sample - loss: 0.0243 - accuracy: 0.9903 - val_loss: 1.4201 - val_accuracy: 0.6347\n","Epoch 25/100\n","9181/9181 [==============================] - 4s 390us/sample - loss: 0.0248 - accuracy: 0.9897 - val_loss: 1.4290 - val_accuracy: 0.6337\n","Epoch 26/100\n","9181/9181 [==============================] - 3s 374us/sample - loss: 0.0264 - accuracy: 0.9890 - val_loss: 1.4429 - val_accuracy: 0.6337\n","Epoch 27/100\n","9160/9181 [============================>.] - ETA: 0s - loss: 0.0267 - accuracy: 0.9897\n","Epoch 00027: Reducing Max LR on Plateau: new max lr will be 0.0015625 (if not early_stopping).\n","9181/9181 [==============================] - 4s 382us/sample - loss: 0.0268 - accuracy: 0.9897 - val_loss: 1.4547 - val_accuracy: 0.6317\n","Epoch 28/100\n","9181/9181 [==============================] - 3s 376us/sample - loss: 0.0198 - accuracy: 0.9905 - val_loss: 1.4606 - val_accuracy: 0.6327\n","Epoch 29/100\n","9181/9181 [==============================] - 3s 375us/sample - loss: 0.0239 - accuracy: 0.9897 - val_loss: 1.4708 - val_accuracy: 0.6327\n","Epoch 30/100\n","9181/9181 [==============================] - 4s 390us/sample - loss: 0.0242 - accuracy: 0.9899 - val_loss: 1.4827 - val_accuracy: 0.6327\n","Epoch 31/100\n","9181/9181 [==============================] - 4s 390us/sample - loss: 0.0237 - accuracy: 0.9901 - val_loss: 1.4913 - val_accuracy: 0.6308\n","Epoch 32/100\n","8600/9181 [===========================>..] - ETA: 0s - loss: 0.0227 - accuracy: 0.9901"],"name":"stdout"},{"output_type":"error","ename":"Exception","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[0;34m(self, epoch, mode)\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m       \u001b[0;32myield\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    173\u001b[0m       \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m       \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mon_batch\u001b[0;34m(self, step, mode, size)\u001b[0m\n\u001b[1;32m    699\u001b[0m         self.callbacks._call_batch_hook(\n\u001b[0;32m--> 700\u001b[0;31m             mode, 'end', step, batch_logs)\n\u001b[0m\u001b[1;32m    701\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m     \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m     if (self._delta_t_batch > 0. and\n","\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mmedian\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mmedian\u001b[0;34m(a, axis, out, overwrite_input, keepdims)\u001b[0m\n\u001b[1;32m   3501\u001b[0m     r, k = _ureduce(a, func=_median, axis=axis, out=out,\n\u001b[0;32m-> 3502\u001b[0;31m                     overwrite_input=overwrite_input)\n\u001b[0m\u001b[1;32m   3503\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_ureduce\u001b[0;34m(a, func, **kwargs)\u001b[0m\n\u001b[1;32m   3409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3410\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3411\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_median\u001b[0;34m(a, axis, out, overwrite_input)\u001b[0m\n\u001b[1;32m   3554\u001b[0m         \u001b[0;31m# warn and return nans like mean would\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3555\u001b[0;31m         \u001b[0mrout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpart\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3556\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_median_nancheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mmean\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   3256\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0;32m-> 3257\u001b[0;31m                           out=out, **kwargs)\n\u001b[0m\u001b[1;32m   3258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-f515cd22d44e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mlearner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mktrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_learner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/My Drive/Weight_file/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mh5name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautofit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_on_plateau\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ktrain/core.py\u001b[0m in \u001b[0;36mautofit\u001b[0;34m(self, lr, epochs, early_stopping, reduce_on_plateau, reduce_factor, cycle_momentum, monitor, checkpoint_folder, verbose, class_weight, callbacks)\u001b[0m\n\u001b[1;32m    858\u001b[0m         hist = self.fit(lr, epochs, early_stopping=early_stopping,\n\u001b[1;32m    859\u001b[0m                         \u001b[0mcheckpoint_folder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_folder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m                         verbose=verbose, class_weight=class_weight, callbacks=kcallbacks)\n\u001b[0m\u001b[1;32m    861\u001b[0m         \u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m         \u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'iterations'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'iterations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ktrain/core.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, lr, n_cycles, cycle_len, cycle_mult, lr_decay, checkpoint_folder, early_stopping, verbose, class_weight, callbacks)\u001b[0m\n\u001b[1;32m    985\u001b[0m                                   \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m                                   \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 987\u001b[0;31m                                   callbacks=kcallbacks)\n\u001b[0m\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msgdr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msgdr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    370\u001b[0m                       total_epochs=1)\n\u001b[1;32m    371\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[0;32m--> 372\u001b[0;31m                                  prefix='val_')\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[0;34m(self, epoch, mode)\u001b[0m\n\u001b[1;32m    683\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0;31m# Epochs only apply to `fit`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m       \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ktrain/lroptimize/triangular.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mcurrent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cannot monitor %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonitor_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mException\u001b[0m: cannot monitor val_loss"]}]},{"cell_type":"code","metadata":{"id":"o5neyGiFaLe6","colab_type":"code","outputId":"52269238-8b8e-4c5f-f400-cc0613abefb3","executionInfo":{"status":"ok","timestamp":1584012451602,"user_tz":-330,"elapsed":524458,"user":{"displayName":"Harsh Kataria","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimH0VsMEaiEzaEeo5nJzMe1WuJRQ2b1DK9fb3S=s64","userId":"07300304294680146354"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import os\n","import pandas as pd\n","import ktrain\n","\n","\n","##main\n","\n","\n","answers_dict = {'hilarious':'3','very_funny':'2','funny':'1','general':'1', 'twisted_meaning':'2', 'very_twisted':'3', 'not_sarcastic':'0','not_funny':'0','motivational':'1','not_motivational':'0', 'positive':'1', 'neutral':'0', 'negative':'-1','not_offensive':'0','very_offensive':'2', 'slight':'1', 'hateful_offensive':'3'}\n","\n","\n","\n","sarcasticpredictor = ktrain.load_predictor('/content/drive/My Drive/memotion/wgts/allsarcasticnot_sarcastic_general_twisted_meaning_very_twisted.h5')\n","humourpredictor = ktrain.load_predictor('/content/drive/My Drive/memotion/wgts/funnotfunclassesnot_funny_fun.h5')\n","motivationpredictor = ktrain.load_predictor('/content/drive/My Drive/memotion/wgts/lastnewmotivational_not_motivational.h5')\n","sentipredictor = ktrain.load_predictor('/content/drive/My Drive/memotion/wgts/pnn3classespositive_negative_neutral.h5')\n","##onlyfunnypredictor = ktrain.load_predictor('/home/dgxuser136/ambuje1/senti_onlyfunny_ktrainbert.h5')\n","offensivepredictor = ktrain.load_predictor('/content/drive/My Drive/memotion/wgts/Copy of lastnewdozerooffensive_not_offensive.h5')\n","#onlysarcasticpredictor = ktrain.load_predictor('/content/drive/My Drive/memotion/wgts/onlysar3classesgeneral_twisted_meaning_very_twisted.h5')\n","onlyoffensivepredictor = ktrain.load_predictor('/content/drive/My Drive/memotion/wgts/onlyoff3classesslight_hateful_offensive_very_offensive.h5')\n","\n","df = pd.read_csv('/content/drive/My Drive/memotion/wgts/2000_testdata.csv')\n","\n","file1 = open(\"/content/drive/My Drive/memotion/wgts/answer_8models-1-4.txt\",\"w\")\n","\n","\n","for i in range(len(df)):\n","    print(i)\n","    text = str(df['corrected_text'][i])\n","    #text = pre_preprocess(str(df['corrected_text'][i]))\n","\n","    taskA = '9'\n","    taskB = ['9']*4\n","    taskC = ['9']*4\n","    \n","\n","    pred = sentipredictor.predict(text)\n","    #print(pred)\n","    \n","##    pred = tup2dict(pred)\n","##    pred = max(pred, key=pred.get)\n","    taskA = answers_dict[pred]\n","    \n","    pred = humourpredictor.predict(text)\n","    #print(pred)\n","##    pred = tup2dict(pred)\n","##    pred = max(pred, key=pred.get)\n","    if pred == 'fun':\n","        taskB[0] = '1'\n","    else:\n","        taskB[0] = '0'\n","        taskC[0] = '0'\n","##    taskB[0] = answers_dict[pred]\n","##    if taskB[0] == '1':\n","##        pred = onlyfunnypredictor.predict(text)\n","##        print(pred)\n","##        pred = tup2dict(pred)\n","##        pred = max(pred, key=pred.get)\n","##        taskC[0] = answers_dict[pred]\n","##    else:\n","##        taskC[0] = '0'\n","\n","    \n","    pred = sarcasticpredictor.predict(text)\n","    if pred != 'not_sarcastic':\n","        taskB[1] = '1'\n","        taskC[1] = answers_dict[pred]\n","    else:\n","        taskB[1] = '0'\n","        taskC[1] = '0'\n","\n","    \n","    \n","#     pred = sarcasticpredictor.predict(text)\n","#     #print(pred)\n","#     if pred == 'sarcastic':\n","#         taskB[1] = '1'\n","#         pred = onlysarcasticpredictor.predict(text)\n","#         #print(pred)\n","# ##        pred = tup2dict(pred)\n","# ##        pred = max(pred, key=pred.get)\n","#         #taskC[1] = answers_dict[pred]\n","#     else:\n","#         taskB[1] = '0'\n","#         taskC[1] = '0'\n","        \n","# ##    pred = tup2dict(pred)\n","# ##    pred = max(pred, key=pred.get)\n","# ##    taskB[1] = answers_dict[pred]\n","# ##    if taskB[1] == '1':\n","# ##        \n","# ##    else:\n","        \n","\n","    pred = offensivepredictor.predict(text)\n","    #print(pred)\n","    if pred == 'offensive':\n","        taskB[2] = '1'\n","        pred = onlyoffensivepredictor.predict(text)\n","        #print(pred)\n","        taskC[2] = answers_dict[pred]\n","##        pred = tup2dict(pred)\n","##        pred = max(pred, key=pred.get)\n","    else:\n","        taskB[2] = '0'\n","        taskC[2] = '0'\n","##    pred = tup2dict(pred)\n","##    pred = max(pred, key=pred.get)\n","##    taskB[2] = answers_dict[pred]\n","##    if taskB[2] == '1':\n","##        \n","##        \n","##    else:\n","        \n","\n","    pred = motivationpredictor.predict(text)\n","    #print(pred)\n","##    pred = tup2dict(pred)\n","##    pred = max(pred, key=pred.get)\n","    taskB[3] = answers_dict[pred]\n","    taskC[3] = taskB[3]   \n","\n","    #taskC = ['9']*4\n","    ans = taskA + '_' + ''.join(taskB) + '_' + ''.join(taskC) + '\\n'\n","    \n","    \n","\n","    file1.write(ans)\n","    \n","\n","file1.close()\n","\n","##pred = [('hilarious', 0.10454379), ('not_funny', 0.21206911), ('very_funny', 0.32491603), ('funny', 0.36210373), ('general', 0.55724233), ('not_sarcastic', 0.11089532), ('twisted_meaning', 0.24999025), ('very_twisted', 0.09583253), ('not_offensive', 0.10120422), ('very_offensive', 0.30312324), ('slight', 0.5820346), ('hateful_offensive', 0.04012201), ('not_motivational', 0.9636585), ('motivational', 0.036573086), ('positive', 0.5848341), ('neutral', 0.28037217), ('negative', 0.14398904)]\n","##pred = tup2dict(pred)\n","##print(answers(pred))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","20\n","21\n","22\n","23\n","24\n","25\n","26\n","27\n","28\n","29\n","30\n","31\n","32\n","33\n","34\n","35\n","36\n","37\n","38\n","39\n","40\n","41\n","42\n","43\n","44\n","45\n","46\n","47\n","48\n","49\n","50\n","51\n","52\n","53\n","54\n","55\n","56\n","57\n","58\n","59\n","60\n","61\n","62\n","63\n","64\n","65\n","66\n","67\n","68\n","69\n","70\n","71\n","72\n","73\n","74\n","75\n","76\n","77\n","78\n","79\n","80\n","81\n","82\n","83\n","84\n","85\n","86\n","87\n","88\n","89\n","90\n","91\n","92\n","93\n","94\n","95\n","96\n","97\n","98\n","99\n","100\n","101\n","102\n","103\n","104\n","105\n","106\n","107\n","108\n","109\n","110\n","111\n","112\n","113\n","114\n","115\n","116\n","117\n","118\n","119\n","120\n","121\n","122\n","123\n","124\n","125\n","126\n","127\n","128\n","129\n","130\n","131\n","132\n","133\n","134\n","135\n","136\n","137\n","138\n","139\n","140\n","141\n","142\n","143\n","144\n","145\n","146\n","147\n","148\n","149\n","150\n","151\n","152\n","153\n","154\n","155\n","156\n","157\n","158\n","159\n","160\n","161\n","162\n","163\n","164\n","165\n","166\n","167\n","168\n","169\n","170\n","171\n","172\n","173\n","174\n","175\n","176\n","177\n","178\n","179\n","180\n","181\n","182\n","183\n","184\n","185\n","186\n","187\n","188\n","189\n","190\n","191\n","192\n","193\n","194\n","195\n","196\n","197\n","198\n","199\n","200\n","201\n","202\n","203\n","204\n","205\n","206\n","207\n","208\n","209\n","210\n","211\n","212\n","213\n","214\n","215\n","216\n","217\n","218\n","219\n","220\n","221\n","222\n","223\n","224\n","225\n","226\n","227\n","228\n","229\n","230\n","231\n","232\n","233\n","234\n","235\n","236\n","237\n","238\n","239\n","240\n","241\n","242\n","243\n","244\n","245\n","246\n","247\n","248\n","249\n","250\n","251\n","252\n","253\n","254\n","255\n","256\n","257\n","258\n","259\n","260\n","261\n","262\n","263\n","264\n","265\n","266\n","267\n","268\n","269\n","270\n","271\n","272\n","273\n","274\n","275\n","276\n","277\n","278\n","279\n","280\n","281\n","282\n","283\n","284\n","285\n","286\n","287\n","288\n","289\n","290\n","291\n","292\n","293\n","294\n","295\n","296\n","297\n","298\n","299\n","300\n","301\n","302\n","303\n","304\n","305\n","306\n","307\n","308\n","309\n","310\n","311\n","312\n","313\n","314\n","315\n","316\n","317\n","318\n","319\n","320\n","321\n","322\n","323\n","324\n","325\n","326\n","327\n","328\n","329\n","330\n","331\n","332\n","333\n","334\n","335\n","336\n","337\n","338\n","339\n","340\n","341\n","342\n","343\n","344\n","345\n","346\n","347\n","348\n","349\n","350\n","351\n","352\n","353\n","354\n","355\n","356\n","357\n","358\n","359\n","360\n","361\n","362\n","363\n","364\n","365\n","366\n","367\n","368\n","369\n","370\n","371\n","372\n","373\n","374\n","375\n","376\n","377\n","378\n","379\n","380\n","381\n","382\n","383\n","384\n","385\n","386\n","387\n","388\n","389\n","390\n","391\n","392\n","393\n","394\n","395\n","396\n","397\n","398\n","399\n","400\n","401\n","402\n","403\n","404\n","405\n","406\n","407\n","408\n","409\n","410\n","411\n","412\n","413\n","414\n","415\n","416\n","417\n","418\n","419\n","420\n","421\n","422\n","423\n","424\n","425\n","426\n","427\n","428\n","429\n","430\n","431\n","432\n","433\n","434\n","435\n","436\n","437\n","438\n","439\n","440\n","441\n","442\n","443\n","444\n","445\n","446\n","447\n","448\n","449\n","450\n","451\n","452\n","453\n","454\n","455\n","456\n","457\n","458\n","459\n","460\n","461\n","462\n","463\n","464\n","465\n","466\n","467\n","468\n","469\n","470\n","471\n","472\n","473\n","474\n","475\n","476\n","477\n","478\n","479\n","480\n","481\n","482\n","483\n","484\n","485\n","486\n","487\n","488\n","489\n","490\n","491\n","492\n","493\n","494\n","495\n","496\n","497\n","498\n","499\n","500\n","501\n","502\n","503\n","504\n","505\n","506\n","507\n","508\n","509\n","510\n","511\n","512\n","513\n","514\n","515\n","516\n","517\n","518\n","519\n","520\n","521\n","522\n","523\n","524\n","525\n","526\n","527\n","528\n","529\n","530\n","531\n","532\n","533\n","534\n","535\n","536\n","537\n","538\n","539\n","540\n","541\n","542\n","543\n","544\n","545\n","546\n","547\n","548\n","549\n","550\n","551\n","552\n","553\n","554\n","555\n","556\n","557\n","558\n","559\n","560\n","561\n","562\n","563\n","564\n","565\n","566\n","567\n","568\n","569\n","570\n","571\n","572\n","573\n","574\n","575\n","576\n","577\n","578\n","579\n","580\n","581\n","582\n","583\n","584\n","585\n","586\n","587\n","588\n","589\n","590\n","591\n","592\n","593\n","594\n","595\n","596\n","597\n","598\n","599\n","600\n","601\n","602\n","603\n","604\n","605\n","606\n","607\n","608\n","609\n","610\n","611\n","612\n","613\n","614\n","615\n","616\n","617\n","618\n","619\n","620\n","621\n","622\n","623\n","624\n","625\n","626\n","627\n","628\n","629\n","630\n","631\n","632\n","633\n","634\n","635\n","636\n","637\n","638\n","639\n","640\n","641\n","642\n","643\n","644\n","645\n","646\n","647\n","648\n","649\n","650\n","651\n","652\n","653\n","654\n","655\n","656\n","657\n","658\n","659\n","660\n","661\n","662\n","663\n","664\n","665\n","666\n","667\n","668\n","669\n","670\n","671\n","672\n","673\n","674\n","675\n","676\n","677\n","678\n","679\n","680\n","681\n","682\n","683\n","684\n","685\n","686\n","687\n","688\n","689\n","690\n","691\n","692\n","693\n","694\n","695\n","696\n","697\n","698\n","699\n","700\n","701\n","702\n","703\n","704\n","705\n","706\n","707\n","708\n","709\n","710\n","711\n","712\n","713\n","714\n","715\n","716\n","717\n","718\n","719\n","720\n","721\n","722\n","723\n","724\n","725\n","726\n","727\n","728\n","729\n","730\n","731\n","732\n","733\n","734\n","735\n","736\n","737\n","738\n","739\n","740\n","741\n","742\n","743\n","744\n","745\n","746\n","747\n","748\n","749\n","750\n","751\n","752\n","753\n","754\n","755\n","756\n","757\n","758\n","759\n","760\n","761\n","762\n","763\n","764\n","765\n","766\n","767\n","768\n","769\n","770\n","771\n","772\n","773\n","774\n","775\n","776\n","777\n","778\n","779\n","780\n","781\n","782\n","783\n","784\n","785\n","786\n","787\n","788\n","789\n","790\n","791\n","792\n","793\n","794\n","795\n","796\n","797\n","798\n","799\n","800\n","801\n","802\n","803\n","804\n","805\n","806\n","807\n","808\n","809\n","810\n","811\n","812\n","813\n","814\n","815\n","816\n","817\n","818\n","819\n","820\n","821\n","822\n","823\n","824\n","825\n","826\n","827\n","828\n","829\n","830\n","831\n","832\n","833\n","834\n","835\n","836\n","837\n","838\n","839\n","840\n","841\n","842\n","843\n","844\n","845\n","846\n","847\n","848\n","849\n","850\n","851\n","852\n","853\n","854\n","855\n","856\n","857\n","858\n","859\n","860\n","861\n","862\n","863\n","864\n","865\n","866\n","867\n","868\n","869\n","870\n","871\n","872\n","873\n","874\n","875\n","876\n","877\n","878\n","879\n","880\n","881\n","882\n","883\n","884\n","885\n","886\n","887\n","888\n","889\n","890\n","891\n","892\n","893\n","894\n","895\n","896\n","897\n","898\n","899\n","900\n","901\n","902\n","903\n","904\n","905\n","906\n","907\n","908\n","909\n","910\n","911\n","912\n","913\n","914\n","915\n","916\n","917\n","918\n","919\n","920\n","921\n","922\n","923\n","924\n","925\n","926\n","927\n","928\n","929\n","930\n","931\n","932\n","933\n","934\n","935\n","936\n","937\n","938\n","939\n","940\n","941\n","942\n","943\n","944\n","945\n","946\n","947\n","948\n","949\n","950\n","951\n","952\n","953\n","954\n","955\n","956\n","957\n","958\n","959\n","960\n","961\n","962\n","963\n","964\n","965\n","966\n","967\n","968\n","969\n","970\n","971\n","972\n","973\n","974\n","975\n","976\n","977\n","978\n","979\n","980\n","981\n","982\n","983\n","984\n","985\n","986\n","987\n","988\n","989\n","990\n","991\n","992\n","993\n","994\n","995\n","996\n","997\n","998\n","999\n","1000\n","1001\n","1002\n","1003\n","1004\n","1005\n","1006\n","1007\n","1008\n","1009\n","1010\n","1011\n","1012\n","1013\n","1014\n","1015\n","1016\n","1017\n","1018\n","1019\n","1020\n","1021\n","1022\n","1023\n","1024\n","1025\n","1026\n","1027\n","1028\n","1029\n","1030\n","1031\n","1032\n","1033\n","1034\n","1035\n","1036\n","1037\n","1038\n","1039\n","1040\n","1041\n","1042\n","1043\n","1044\n","1045\n","1046\n","1047\n","1048\n","1049\n","1050\n","1051\n","1052\n","1053\n","1054\n","1055\n","1056\n","1057\n","1058\n","1059\n","1060\n","1061\n","1062\n","1063\n","1064\n","1065\n","1066\n","1067\n","1068\n","1069\n","1070\n","1071\n","1072\n","1073\n","1074\n","1075\n","1076\n","1077\n","1078\n","1079\n","1080\n","1081\n","1082\n","1083\n","1084\n","1085\n","1086\n","1087\n","1088\n","1089\n","1090\n","1091\n","1092\n","1093\n","1094\n","1095\n","1096\n","1097\n","1098\n","1099\n","1100\n","1101\n","1102\n","1103\n","1104\n","1105\n","1106\n","1107\n","1108\n","1109\n","1110\n","1111\n","1112\n","1113\n","1114\n","1115\n","1116\n","1117\n","1118\n","1119\n","1120\n","1121\n","1122\n","1123\n","1124\n","1125\n","1126\n","1127\n","1128\n","1129\n","1130\n","1131\n","1132\n","1133\n","1134\n","1135\n","1136\n","1137\n","1138\n","1139\n","1140\n","1141\n","1142\n","1143\n","1144\n","1145\n","1146\n","1147\n","1148\n","1149\n","1150\n","1151\n","1152\n","1153\n","1154\n","1155\n","1156\n","1157\n","1158\n","1159\n","1160\n","1161\n","1162\n","1163\n","1164\n","1165\n","1166\n","1167\n","1168\n","1169\n","1170\n","1171\n","1172\n","1173\n","1174\n","1175\n","1176\n","1177\n","1178\n","1179\n","1180\n","1181\n","1182\n","1183\n","1184\n","1185\n","1186\n","1187\n","1188\n","1189\n","1190\n","1191\n","1192\n","1193\n","1194\n","1195\n","1196\n","1197\n","1198\n","1199\n","1200\n","1201\n","1202\n","1203\n","1204\n","1205\n","1206\n","1207\n","1208\n","1209\n","1210\n","1211\n","1212\n","1213\n","1214\n","1215\n","1216\n","1217\n","1218\n","1219\n","1220\n","1221\n","1222\n","1223\n","1224\n","1225\n","1226\n","1227\n","1228\n","1229\n","1230\n","1231\n","1232\n","1233\n","1234\n","1235\n","1236\n","1237\n","1238\n","1239\n","1240\n","1241\n","1242\n","1243\n","1244\n","1245\n","1246\n","1247\n","1248\n","1249\n","1250\n","1251\n","1252\n","1253\n","1254\n","1255\n","1256\n","1257\n","1258\n","1259\n","1260\n","1261\n","1262\n","1263\n","1264\n","1265\n","1266\n","1267\n","1268\n","1269\n","1270\n","1271\n","1272\n","1273\n","1274\n","1275\n","1276\n","1277\n","1278\n","1279\n","1280\n","1281\n","1282\n","1283\n","1284\n","1285\n","1286\n","1287\n","1288\n","1289\n","1290\n","1291\n","1292\n","1293\n","1294\n","1295\n","1296\n","1297\n","1298\n","1299\n","1300\n","1301\n","1302\n","1303\n","1304\n","1305\n","1306\n","1307\n","1308\n","1309\n","1310\n","1311\n","1312\n","1313\n","1314\n","1315\n","1316\n","1317\n","1318\n","1319\n","1320\n","1321\n","1322\n","1323\n","1324\n","1325\n","1326\n","1327\n","1328\n","1329\n","1330\n","1331\n","1332\n","1333\n","1334\n","1335\n","1336\n","1337\n","1338\n","1339\n","1340\n","1341\n","1342\n","1343\n","1344\n","1345\n","1346\n","1347\n","1348\n","1349\n","1350\n","1351\n","1352\n","1353\n","1354\n","1355\n","1356\n","1357\n","1358\n","1359\n","1360\n","1361\n","1362\n","1363\n","1364\n","1365\n","1366\n","1367\n","1368\n","1369\n","1370\n","1371\n","1372\n","1373\n","1374\n","1375\n","1376\n","1377\n","1378\n","1379\n","1380\n","1381\n","1382\n","1383\n","1384\n","1385\n","1386\n","1387\n","1388\n","1389\n","1390\n","1391\n","1392\n","1393\n","1394\n","1395\n","1396\n","1397\n","1398\n","1399\n","1400\n","1401\n","1402\n","1403\n","1404\n","1405\n","1406\n","1407\n","1408\n","1409\n","1410\n","1411\n","1412\n","1413\n","1414\n","1415\n","1416\n","1417\n","1418\n","1419\n","1420\n","1421\n","1422\n","1423\n","1424\n","1425\n","1426\n","1427\n","1428\n","1429\n","1430\n","1431\n","1432\n","1433\n","1434\n","1435\n","1436\n","1437\n","1438\n","1439\n","1440\n","1441\n","1442\n","1443\n","1444\n","1445\n","1446\n","1447\n","1448\n","1449\n","1450\n","1451\n","1452\n","1453\n","1454\n","1455\n","1456\n","1457\n","1458\n","1459\n","1460\n","1461\n","1462\n","1463\n","1464\n","1465\n","1466\n","1467\n","1468\n","1469\n","1470\n","1471\n","1472\n","1473\n","1474\n","1475\n","1476\n","1477\n","1478\n","1479\n","1480\n","1481\n","1482\n","1483\n","1484\n","1485\n","1486\n","1487\n","1488\n","1489\n","1490\n","1491\n","1492\n","1493\n","1494\n","1495\n","1496\n","1497\n","1498\n","1499\n","1500\n","1501\n","1502\n","1503\n","1504\n","1505\n","1506\n","1507\n","1508\n","1509\n","1510\n","1511\n","1512\n","1513\n","1514\n","1515\n","1516\n","1517\n","1518\n","1519\n","1520\n","1521\n","1522\n","1523\n","1524\n","1525\n","1526\n","1527\n","1528\n","1529\n","1530\n","1531\n","1532\n","1533\n","1534\n","1535\n","1536\n","1537\n","1538\n","1539\n","1540\n","1541\n","1542\n","1543\n","1544\n","1545\n","1546\n","1547\n","1548\n","1549\n","1550\n","1551\n","1552\n","1553\n","1554\n","1555\n","1556\n","1557\n","1558\n","1559\n","1560\n","1561\n","1562\n","1563\n","1564\n","1565\n","1566\n","1567\n","1568\n","1569\n","1570\n","1571\n","1572\n","1573\n","1574\n","1575\n","1576\n","1577\n","1578\n","1579\n","1580\n","1581\n","1582\n","1583\n","1584\n","1585\n","1586\n","1587\n","1588\n","1589\n","1590\n","1591\n","1592\n","1593\n","1594\n","1595\n","1596\n","1597\n","1598\n","1599\n","1600\n","1601\n","1602\n","1603\n","1604\n","1605\n","1606\n","1607\n","1608\n","1609\n","1610\n","1611\n","1612\n","1613\n","1614\n","1615\n","1616\n","1617\n","1618\n","1619\n","1620\n","1621\n","1622\n","1623\n","1624\n","1625\n","1626\n","1627\n","1628\n","1629\n","1630\n","1631\n","1632\n","1633\n","1634\n","1635\n","1636\n","1637\n","1638\n","1639\n","1640\n","1641\n","1642\n","1643\n","1644\n","1645\n","1646\n","1647\n","1648\n","1649\n","1650\n","1651\n","1652\n","1653\n","1654\n","1655\n","1656\n","1657\n","1658\n","1659\n","1660\n","1661\n","1662\n","1663\n","1664\n","1665\n","1666\n","1667\n","1668\n","1669\n","1670\n","1671\n","1672\n","1673\n","1674\n","1675\n","1676\n","1677\n","1678\n","1679\n","1680\n","1681\n","1682\n","1683\n","1684\n","1685\n","1686\n","1687\n","1688\n","1689\n","1690\n","1691\n","1692\n","1693\n","1694\n","1695\n","1696\n","1697\n","1698\n","1699\n","1700\n","1701\n","1702\n","1703\n","1704\n","1705\n","1706\n","1707\n","1708\n","1709\n","1710\n","1711\n","1712\n","1713\n","1714\n","1715\n","1716\n","1717\n","1718\n","1719\n","1720\n","1721\n","1722\n","1723\n","1724\n","1725\n","1726\n","1727\n","1728\n","1729\n","1730\n","1731\n","1732\n","1733\n","1734\n","1735\n","1736\n","1737\n","1738\n","1739\n","1740\n","1741\n","1742\n","1743\n","1744\n","1745\n","1746\n","1747\n","1748\n","1749\n","1750\n","1751\n","1752\n","1753\n","1754\n","1755\n","1756\n","1757\n","1758\n","1759\n","1760\n","1761\n","1762\n","1763\n","1764\n","1765\n","1766\n","1767\n","1768\n","1769\n","1770\n","1771\n","1772\n","1773\n","1774\n","1775\n","1776\n","1777\n","1778\n","1779\n","1780\n","1781\n","1782\n","1783\n","1784\n","1785\n","1786\n","1787\n","1788\n","1789\n","1790\n","1791\n","1792\n","1793\n","1794\n","1795\n","1796\n","1797\n","1798\n","1799\n","1800\n","1801\n","1802\n","1803\n","1804\n","1805\n","1806\n","1807\n","1808\n","1809\n","1810\n","1811\n","1812\n","1813\n","1814\n","1815\n","1816\n","1817\n","1818\n","1819\n","1820\n","1821\n","1822\n","1823\n","1824\n","1825\n","1826\n","1827\n","1828\n","1829\n","1830\n","1831\n","1832\n","1833\n","1834\n","1835\n","1836\n","1837\n","1838\n","1839\n","1840\n","1841\n","1842\n","1843\n","1844\n","1845\n","1846\n","1847\n","1848\n","1849\n","1850\n","1851\n","1852\n","1853\n","1854\n","1855\n","1856\n","1857\n","1858\n","1859\n","1860\n","1861\n","1862\n","1863\n","1864\n","1865\n","1866\n","1867\n","1868\n","1869\n","1870\n","1871\n","1872\n","1873\n","1874\n","1875\n","1876\n","1877\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"F5BnmyLkn5NQ","colab_type":"code","colab":{}},"source":["from zipfile import ZipFile as zp"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bkpXL__2eRew","colab_type":"code","outputId":"65c3c662-8c87-4991-81db-aa5b34e1ac3d","executionInfo":{"status":"ok","timestamp":1584085082685,"user_tz":-330,"elapsed":511051,"user":{"displayName":"Harsh Kataria","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimH0VsMEaiEzaEeo5nJzMe1WuJRQ2b1DK9fb3S=s64","userId":"07300304294680146354"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import os\n","import pandas as pd\n","import ktrain\n","\n","\n","##main\n","\n","\n","answers_dict = {'hilarious':'3','very_funny':'2','funny':'1','general':'1', 'twisted_meaning':'2', 'very_twisted':'3', 'not_sarcastic':'0','not_funny':'0','motivational':'1','not_motivational':'0', 'positive':'1', 'neutral':'0', 'negative':'-1','not_offensive':'0','very_offensive':'2', 'slight':'1', 'hateful_offensive':'3'}\n","\n","\n","\n","sarcasticpredictor = ktrain.load_predictor('/content/drive/My Drive/memotion/wgts/sarcasticnotsarcasticclassesnot_sarcastic_sarcastic.h5')\n","humourpredictor = ktrain.load_predictor('/content/drive/My Drive/memotion/wgts/funnotfunclassesnot_funny_fun.h5')\n","#motivationpredictor = ktrain.load_predictor('/content/drive/My Drive/memotion/wgts/lastnewmotivational_not_motivational.h5')\n","sentipredictor = ktrain.load_predictor('/content/drive/My Drive/memotion/wgts/pnn3classespositive_negative_neutral.h5')\n","##onlyfunnypredictor = ktrain.load_predictor('/home/dgxuser136/ambuje1/senti_onlyfunny_ktrainbert.h5')\n","offensivepredictor = ktrain.load_predictor('/content/drive/My Drive/memotion/wgts/Copy of lastnewdozerooffensive_not_offensive.h5')\n","onlysarcasticpredictor = ktrain.load_predictor('/content/drive/My Drive/memotion/wgts/sarcastic22general_twisted_meaning_very_twisted.h5')\n","onlyoffensivepredictor = ktrain.load_predictor('/content/drive/My Drive/memotion/wgts/off22slight_very_offensive_hateful_offensive.h5')\n","\n","df = pd.read_csv('/content/drive/My Drive/memotion/wgts/2000_testdata.csv')\n","\n","file1 = open(\"/content/drive/My Drive/memotion/wgts/answer.txt\",\"w\")\n","\n","\n","for i in range(len(df)):\n","    print(i)\n","    text = str(df['corrected_text'][i])\n","    #text = pre_preprocess(str(df['corrected_text'][i]))\n","\n","    taskA = '9'\n","    taskB = ['9']*4\n","    taskC = ['9']*4\n","    \n","\n","    pred = sentipredictor.predict(text)\n","    #print(pred)\n","    \n","##    pred = tup2dict(pred)\n","##    pred = max(pred, key=pred.get)\n","    taskA = answers_dict[pred]\n","    \n","#     pred = humourpredictor.predict(text)\n","#     #print(pred)\n","# ##    pred = tup2dict(pred)\n","# ##    pred = max(pred, key=pred.get)\n","#     if pred == 'fun':\n","#         taskB[0] = '1'\n","#     else:\n","#         taskB[0] = '0'\n","#         taskC[0] = '0'\n","##    taskB[0] = answers_dict[pred]\n","##    if taskB[0] == '1':\n","##        pred = onlyfunnypredictor.predict(text)\n","##        print(pred)\n","##        pred = tup2dict(pred)\n","##        pred = max(pred, key=pred.get)\n","##        taskC[0] = answers_dict[pred]\n","##    else:\n","##        taskC[0] = '0'\n","\n","    \n","    # pred = sarcasticpredictor.predict(text)\n","    # if pred != 'not_sarcastic':\n","    #     taskB[1] = '1'\n","    #     #taskC[1] = answers_dict[pred]\n","    # else:\n","    #     taskB[1] = '0'\n","    #     taskC[1] = '0'\n","\n","    \n","    \n","    pred = sarcasticpredictor.predict(text)\n","    #print(pred)\n","    if pred == 'sarcastic':\n","        taskB[1] = '1'\n","        pred = onlysarcasticpredictor.predict(text)\n","        #print(pred)\n","##        pred = tup2dict(pred)\n","##        pred = max(pred, key=pred.get)\n","        #taskC[1] = answers_dict[pred]\n","    else:\n","        taskB[1] = '0'\n","        # taskC[1] = '0'\n","        \n","#    pred = tup2dict(pred)\n","#    pred = max(pred, key=pred.get)\n","#    taskB[1] = answers_dict[pred]\n","#    if taskB[1] == '1':\n","#        \n","#    else:\n","        \n","\n","    pred = offensivepredictor.predict(text)\n","    #print(pred)\n","    if pred == 'offensive':\n","        taskB[2] = '1'\n","        pred = onlyoffensivepredictor.predict(text)\n","        #print(pred)\n","        #taskC[2] = answers_dict[pred]\n","##        pred = tup2dict(pred)\n","##        pred = max(pred, key=pred.get)\n","    else:\n","        taskB[2] = '0'\n","        # taskC[2] = '0'\n","#    pred = tup2dict(pred)\n","#    pred = max(pred, key=pred.get)\n","#    taskB[2] = answers_dict[pred]\n","#    if taskB[2] == '1':\n","#        \n","#        \n","#    else:\n","        \n","\n","    pred = motivationpredictor.predict(text)\n","    #print(pred)\n","##    pred = tup2dict(pred)\n","##    pred = max(pred, key=pred.get)\n","    taskB[3] = answers_dict[pred]\n","    # taskC[3] = taskB[3]   \n","\n","    #taskC = ['9']*4\n","    ans = taskA + '_' + ''.join(taskB) + '_' + ''.join(taskC) + '\\n'\n","    \n","    \n","\n","    file1.write(ans)\n","    \n","\n","file1.close()\n","\n","# zpo = zp('/content/drive/My Drive/memotion/wgts/res.zip','w')\n","# zpo.write('answer.txt')\n","# zpo.close()\n","\n","##pred = [('hilarious', 0.10454379), ('not_funny', 0.21206911), ('very_funny', 0.32491603), ('funny', 0.36210373), ('general', 0.55724233), ('not_sarcastic', 0.11089532), ('twisted_meaning', 0.24999025), ('very_twisted', 0.09583253), ('not_offensive', 0.10120422), ('very_offensive', 0.30312324), ('slight', 0.5820346), ('hateful_offensive', 0.04012201), ('not_motivational', 0.9636585), ('motivational', 0.036573086), ('positive', 0.5848341), ('neutral', 0.28037217), ('negative', 0.14398904)]\n","##pred = tup2dict(pred)\n","##print(answers(pred))\n"],"execution_count":17,"outputs":[{"output_type":"stream","text":["0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","20\n","21\n","22\n","23\n","24\n","25\n","26\n","27\n","28\n","29\n","30\n","31\n","32\n","33\n","34\n","35\n","36\n","37\n","38\n","39\n","40\n","41\n","42\n","43\n","44\n","45\n","46\n","47\n","48\n","49\n","50\n","51\n","52\n","53\n","54\n","55\n","56\n","57\n","58\n","59\n","60\n","61\n","62\n","63\n","64\n","65\n","66\n","67\n","68\n","69\n","70\n","71\n","72\n","73\n","74\n","75\n","76\n","77\n","78\n","79\n","80\n","81\n","82\n","83\n","84\n","85\n","86\n","87\n","88\n","89\n","90\n","91\n","92\n","93\n","94\n","95\n","96\n","97\n","98\n","99\n","100\n","101\n","102\n","103\n","104\n","105\n","106\n","107\n","108\n","109\n","110\n","111\n","112\n","113\n","114\n","115\n","116\n","117\n","118\n","119\n","120\n","121\n","122\n","123\n","124\n","125\n","126\n","127\n","128\n","129\n","130\n","131\n","132\n","133\n","134\n","135\n","136\n","137\n","138\n","139\n","140\n","141\n","142\n","143\n","144\n","145\n","146\n","147\n","148\n","149\n","150\n","151\n","152\n","153\n","154\n","155\n","156\n","157\n","158\n","159\n","160\n","161\n","162\n","163\n","164\n","165\n","166\n","167\n","168\n","169\n","170\n","171\n","172\n","173\n","174\n","175\n","176\n","177\n","178\n","179\n","180\n","181\n","182\n","183\n","184\n","185\n","186\n","187\n","188\n","189\n","190\n","191\n","192\n","193\n","194\n","195\n","196\n","197\n","198\n","199\n","200\n","201\n","202\n","203\n","204\n","205\n","206\n","207\n","208\n","209\n","210\n","211\n","212\n","213\n","214\n","215\n","216\n","217\n","218\n","219\n","220\n","221\n","222\n","223\n","224\n","225\n","226\n","227\n","228\n","229\n","230\n","231\n","232\n","233\n","234\n","235\n","236\n","237\n","238\n","239\n","240\n","241\n","242\n","243\n","244\n","245\n","246\n","247\n","248\n","249\n","250\n","251\n","252\n","253\n","254\n","255\n","256\n","257\n","258\n","259\n","260\n","261\n","262\n","263\n","264\n","265\n","266\n","267\n","268\n","269\n","270\n","271\n","272\n","273\n","274\n","275\n","276\n","277\n","278\n","279\n","280\n","281\n","282\n","283\n","284\n","285\n","286\n","287\n","288\n","289\n","290\n","291\n","292\n","293\n","294\n","295\n","296\n","297\n","298\n","299\n","300\n","301\n","302\n","303\n","304\n","305\n","306\n","307\n","308\n","309\n","310\n","311\n","312\n","313\n","314\n","315\n","316\n","317\n","318\n","319\n","320\n","321\n","322\n","323\n","324\n","325\n","326\n","327\n","328\n","329\n","330\n","331\n","332\n","333\n","334\n","335\n","336\n","337\n","338\n","339\n","340\n","341\n","342\n","343\n","344\n","345\n","346\n","347\n","348\n","349\n","350\n","351\n","352\n","353\n","354\n","355\n","356\n","357\n","358\n","359\n","360\n","361\n","362\n","363\n","364\n","365\n","366\n","367\n","368\n","369\n","370\n","371\n","372\n","373\n","374\n","375\n","376\n","377\n","378\n","379\n","380\n","381\n","382\n","383\n","384\n","385\n","386\n","387\n","388\n","389\n","390\n","391\n","392\n","393\n","394\n","395\n","396\n","397\n","398\n","399\n","400\n","401\n","402\n","403\n","404\n","405\n","406\n","407\n","408\n","409\n","410\n","411\n","412\n","413\n","414\n","415\n","416\n","417\n","418\n","419\n","420\n","421\n","422\n","423\n","424\n","425\n","426\n","427\n","428\n","429\n","430\n","431\n","432\n","433\n","434\n","435\n","436\n","437\n","438\n","439\n","440\n","441\n","442\n","443\n","444\n","445\n","446\n","447\n","448\n","449\n","450\n","451\n","452\n","453\n","454\n","455\n","456\n","457\n","458\n","459\n","460\n","461\n","462\n","463\n","464\n","465\n","466\n","467\n","468\n","469\n","470\n","471\n","472\n","473\n","474\n","475\n","476\n","477\n","478\n","479\n","480\n","481\n","482\n","483\n","484\n","485\n","486\n","487\n","488\n","489\n","490\n","491\n","492\n","493\n","494\n","495\n","496\n","497\n","498\n","499\n","500\n","501\n","502\n","503\n","504\n","505\n","506\n","507\n","508\n","509\n","510\n","511\n","512\n","513\n","514\n","515\n","516\n","517\n","518\n","519\n","520\n","521\n","522\n","523\n","524\n","525\n","526\n","527\n","528\n","529\n","530\n","531\n","532\n","533\n","534\n","535\n","536\n","537\n","538\n","539\n","540\n","541\n","542\n","543\n","544\n","545\n","546\n","547\n","548\n","549\n","550\n","551\n","552\n","553\n","554\n","555\n","556\n","557\n","558\n","559\n","560\n","561\n","562\n","563\n","564\n","565\n","566\n","567\n","568\n","569\n","570\n","571\n","572\n","573\n","574\n","575\n","576\n","577\n","578\n","579\n","580\n","581\n","582\n","583\n","584\n","585\n","586\n","587\n","588\n","589\n","590\n","591\n","592\n","593\n","594\n","595\n","596\n","597\n","598\n","599\n","600\n","601\n","602\n","603\n","604\n","605\n","606\n","607\n","608\n","609\n","610\n","611\n","612\n","613\n","614\n","615\n","616\n","617\n","618\n","619\n","620\n","621\n","622\n","623\n","624\n","625\n","626\n","627\n","628\n","629\n","630\n","631\n","632\n","633\n","634\n","635\n","636\n","637\n","638\n","639\n","640\n","641\n","642\n","643\n","644\n","645\n","646\n","647\n","648\n","649\n","650\n","651\n","652\n","653\n","654\n","655\n","656\n","657\n","658\n","659\n","660\n","661\n","662\n","663\n","664\n","665\n","666\n","667\n","668\n","669\n","670\n","671\n","672\n","673\n","674\n","675\n","676\n","677\n","678\n","679\n","680\n","681\n","682\n","683\n","684\n","685\n","686\n","687\n","688\n","689\n","690\n","691\n","692\n","693\n","694\n","695\n","696\n","697\n","698\n","699\n","700\n","701\n","702\n","703\n","704\n","705\n","706\n","707\n","708\n","709\n","710\n","711\n","712\n","713\n","714\n","715\n","716\n","717\n","718\n","719\n","720\n","721\n","722\n","723\n","724\n","725\n","726\n","727\n","728\n","729\n","730\n","731\n","732\n","733\n","734\n","735\n","736\n","737\n","738\n","739\n","740\n","741\n","742\n","743\n","744\n","745\n","746\n","747\n","748\n","749\n","750\n","751\n","752\n","753\n","754\n","755\n","756\n","757\n","758\n","759\n","760\n","761\n","762\n","763\n","764\n","765\n","766\n","767\n","768\n","769\n","770\n","771\n","772\n","773\n","774\n","775\n","776\n","777\n","778\n","779\n","780\n","781\n","782\n","783\n","784\n","785\n","786\n","787\n","788\n","789\n","790\n","791\n","792\n","793\n","794\n","795\n","796\n","797\n","798\n","799\n","800\n","801\n","802\n","803\n","804\n","805\n","806\n","807\n","808\n","809\n","810\n","811\n","812\n","813\n","814\n","815\n","816\n","817\n","818\n","819\n","820\n","821\n","822\n","823\n","824\n","825\n","826\n","827\n","828\n","829\n","830\n","831\n","832\n","833\n","834\n","835\n","836\n","837\n","838\n","839\n","840\n","841\n","842\n","843\n","844\n","845\n","846\n","847\n","848\n","849\n","850\n","851\n","852\n","853\n","854\n","855\n","856\n","857\n","858\n","859\n","860\n","861\n","862\n","863\n","864\n","865\n","866\n","867\n","868\n","869\n","870\n","871\n","872\n","873\n","874\n","875\n","876\n","877\n","878\n","879\n","880\n","881\n","882\n","883\n","884\n","885\n","886\n","887\n","888\n","889\n","890\n","891\n","892\n","893\n","894\n","895\n","896\n","897\n","898\n","899\n","900\n","901\n","902\n","903\n","904\n","905\n","906\n","907\n","908\n","909\n","910\n","911\n","912\n","913\n","914\n","915\n","916\n","917\n","918\n","919\n","920\n","921\n","922\n","923\n","924\n","925\n","926\n","927\n","928\n","929\n","930\n","931\n","932\n","933\n","934\n","935\n","936\n","937\n","938\n","939\n","940\n","941\n","942\n","943\n","944\n","945\n","946\n","947\n","948\n","949\n","950\n","951\n","952\n","953\n","954\n","955\n","956\n","957\n","958\n","959\n","960\n","961\n","962\n","963\n","964\n","965\n","966\n","967\n","968\n","969\n","970\n","971\n","972\n","973\n","974\n","975\n","976\n","977\n","978\n","979\n","980\n","981\n","982\n","983\n","984\n","985\n","986\n","987\n","988\n","989\n","990\n","991\n","992\n","993\n","994\n","995\n","996\n","997\n","998\n","999\n","1000\n","1001\n","1002\n","1003\n","1004\n","1005\n","1006\n","1007\n","1008\n","1009\n","1010\n","1011\n","1012\n","1013\n","1014\n","1015\n","1016\n","1017\n","1018\n","1019\n","1020\n","1021\n","1022\n","1023\n","1024\n","1025\n","1026\n","1027\n","1028\n","1029\n","1030\n","1031\n","1032\n","1033\n","1034\n","1035\n","1036\n","1037\n","1038\n","1039\n","1040\n","1041\n","1042\n","1043\n","1044\n","1045\n","1046\n","1047\n","1048\n","1049\n","1050\n","1051\n","1052\n","1053\n","1054\n","1055\n","1056\n","1057\n","1058\n","1059\n","1060\n","1061\n","1062\n","1063\n","1064\n","1065\n","1066\n","1067\n","1068\n","1069\n","1070\n","1071\n","1072\n","1073\n","1074\n","1075\n","1076\n","1077\n","1078\n","1079\n","1080\n","1081\n","1082\n","1083\n","1084\n","1085\n","1086\n","1087\n","1088\n","1089\n","1090\n","1091\n","1092\n","1093\n","1094\n","1095\n","1096\n","1097\n","1098\n","1099\n","1100\n","1101\n","1102\n","1103\n","1104\n","1105\n","1106\n","1107\n","1108\n","1109\n","1110\n","1111\n","1112\n","1113\n","1114\n","1115\n","1116\n","1117\n","1118\n","1119\n","1120\n","1121\n","1122\n","1123\n","1124\n","1125\n","1126\n","1127\n","1128\n","1129\n","1130\n","1131\n","1132\n","1133\n","1134\n","1135\n","1136\n","1137\n","1138\n","1139\n","1140\n","1141\n","1142\n","1143\n","1144\n","1145\n","1146\n","1147\n","1148\n","1149\n","1150\n","1151\n","1152\n","1153\n","1154\n","1155\n","1156\n","1157\n","1158\n","1159\n","1160\n","1161\n","1162\n","1163\n","1164\n","1165\n","1166\n","1167\n","1168\n","1169\n","1170\n","1171\n","1172\n","1173\n","1174\n","1175\n","1176\n","1177\n","1178\n","1179\n","1180\n","1181\n","1182\n","1183\n","1184\n","1185\n","1186\n","1187\n","1188\n","1189\n","1190\n","1191\n","1192\n","1193\n","1194\n","1195\n","1196\n","1197\n","1198\n","1199\n","1200\n","1201\n","1202\n","1203\n","1204\n","1205\n","1206\n","1207\n","1208\n","1209\n","1210\n","1211\n","1212\n","1213\n","1214\n","1215\n","1216\n","1217\n","1218\n","1219\n","1220\n","1221\n","1222\n","1223\n","1224\n","1225\n","1226\n","1227\n","1228\n","1229\n","1230\n","1231\n","1232\n","1233\n","1234\n","1235\n","1236\n","1237\n","1238\n","1239\n","1240\n","1241\n","1242\n","1243\n","1244\n","1245\n","1246\n","1247\n","1248\n","1249\n","1250\n","1251\n","1252\n","1253\n","1254\n","1255\n","1256\n","1257\n","1258\n","1259\n","1260\n","1261\n","1262\n","1263\n","1264\n","1265\n","1266\n","1267\n","1268\n","1269\n","1270\n","1271\n","1272\n","1273\n","1274\n","1275\n","1276\n","1277\n","1278\n","1279\n","1280\n","1281\n","1282\n","1283\n","1284\n","1285\n","1286\n","1287\n","1288\n","1289\n","1290\n","1291\n","1292\n","1293\n","1294\n","1295\n","1296\n","1297\n","1298\n","1299\n","1300\n","1301\n","1302\n","1303\n","1304\n","1305\n","1306\n","1307\n","1308\n","1309\n","1310\n","1311\n","1312\n","1313\n","1314\n","1315\n","1316\n","1317\n","1318\n","1319\n","1320\n","1321\n","1322\n","1323\n","1324\n","1325\n","1326\n","1327\n","1328\n","1329\n","1330\n","1331\n","1332\n","1333\n","1334\n","1335\n","1336\n","1337\n","1338\n","1339\n","1340\n","1341\n","1342\n","1343\n","1344\n","1345\n","1346\n","1347\n","1348\n","1349\n","1350\n","1351\n","1352\n","1353\n","1354\n","1355\n","1356\n","1357\n","1358\n","1359\n","1360\n","1361\n","1362\n","1363\n","1364\n","1365\n","1366\n","1367\n","1368\n","1369\n","1370\n","1371\n","1372\n","1373\n","1374\n","1375\n","1376\n","1377\n","1378\n","1379\n","1380\n","1381\n","1382\n","1383\n","1384\n","1385\n","1386\n","1387\n","1388\n","1389\n","1390\n","1391\n","1392\n","1393\n","1394\n","1395\n","1396\n","1397\n","1398\n","1399\n","1400\n","1401\n","1402\n","1403\n","1404\n","1405\n","1406\n","1407\n","1408\n","1409\n","1410\n","1411\n","1412\n","1413\n","1414\n","1415\n","1416\n","1417\n","1418\n","1419\n","1420\n","1421\n","1422\n","1423\n","1424\n","1425\n","1426\n","1427\n","1428\n","1429\n","1430\n","1431\n","1432\n","1433\n","1434\n","1435\n","1436\n","1437\n","1438\n","1439\n","1440\n","1441\n","1442\n","1443\n","1444\n","1445\n","1446\n","1447\n","1448\n","1449\n","1450\n","1451\n","1452\n","1453\n","1454\n","1455\n","1456\n","1457\n","1458\n","1459\n","1460\n","1461\n","1462\n","1463\n","1464\n","1465\n","1466\n","1467\n","1468\n","1469\n","1470\n","1471\n","1472\n","1473\n","1474\n","1475\n","1476\n","1477\n","1478\n","1479\n","1480\n","1481\n","1482\n","1483\n","1484\n","1485\n","1486\n","1487\n","1488\n","1489\n","1490\n","1491\n","1492\n","1493\n","1494\n","1495\n","1496\n","1497\n","1498\n","1499\n","1500\n","1501\n","1502\n","1503\n","1504\n","1505\n","1506\n","1507\n","1508\n","1509\n","1510\n","1511\n","1512\n","1513\n","1514\n","1515\n","1516\n","1517\n","1518\n","1519\n","1520\n","1521\n","1522\n","1523\n","1524\n","1525\n","1526\n","1527\n","1528\n","1529\n","1530\n","1531\n","1532\n","1533\n","1534\n","1535\n","1536\n","1537\n","1538\n","1539\n","1540\n","1541\n","1542\n","1543\n","1544\n","1545\n","1546\n","1547\n","1548\n","1549\n","1550\n","1551\n","1552\n","1553\n","1554\n","1555\n","1556\n","1557\n","1558\n","1559\n","1560\n","1561\n","1562\n","1563\n","1564\n","1565\n","1566\n","1567\n","1568\n","1569\n","1570\n","1571\n","1572\n","1573\n","1574\n","1575\n","1576\n","1577\n","1578\n","1579\n","1580\n","1581\n","1582\n","1583\n","1584\n","1585\n","1586\n","1587\n","1588\n","1589\n","1590\n","1591\n","1592\n","1593\n","1594\n","1595\n","1596\n","1597\n","1598\n","1599\n","1600\n","1601\n","1602\n","1603\n","1604\n","1605\n","1606\n","1607\n","1608\n","1609\n","1610\n","1611\n","1612\n","1613\n","1614\n","1615\n","1616\n","1617\n","1618\n","1619\n","1620\n","1621\n","1622\n","1623\n","1624\n","1625\n","1626\n","1627\n","1628\n","1629\n","1630\n","1631\n","1632\n","1633\n","1634\n","1635\n","1636\n","1637\n","1638\n","1639\n","1640\n","1641\n","1642\n","1643\n","1644\n","1645\n","1646\n","1647\n","1648\n","1649\n","1650\n","1651\n","1652\n","1653\n","1654\n","1655\n","1656\n","1657\n","1658\n","1659\n","1660\n","1661\n","1662\n","1663\n","1664\n","1665\n","1666\n","1667\n","1668\n","1669\n","1670\n","1671\n","1672\n","1673\n","1674\n","1675\n","1676\n","1677\n","1678\n","1679\n","1680\n","1681\n","1682\n","1683\n","1684\n","1685\n","1686\n","1687\n","1688\n","1689\n","1690\n","1691\n","1692\n","1693\n","1694\n","1695\n","1696\n","1697\n","1698\n","1699\n","1700\n","1701\n","1702\n","1703\n","1704\n","1705\n","1706\n","1707\n","1708\n","1709\n","1710\n","1711\n","1712\n","1713\n","1714\n","1715\n","1716\n","1717\n","1718\n","1719\n","1720\n","1721\n","1722\n","1723\n","1724\n","1725\n","1726\n","1727\n","1728\n","1729\n","1730\n","1731\n","1732\n","1733\n","1734\n","1735\n","1736\n","1737\n","1738\n","1739\n","1740\n","1741\n","1742\n","1743\n","1744\n","1745\n","1746\n","1747\n","1748\n","1749\n","1750\n","1751\n","1752\n","1753\n","1754\n","1755\n","1756\n","1757\n","1758\n","1759\n","1760\n","1761\n","1762\n","1763\n","1764\n","1765\n","1766\n","1767\n","1768\n","1769\n","1770\n","1771\n","1772\n","1773\n","1774\n","1775\n","1776\n","1777\n","1778\n","1779\n","1780\n","1781\n","1782\n","1783\n","1784\n","1785\n","1786\n","1787\n","1788\n","1789\n","1790\n","1791\n","1792\n","1793\n","1794\n","1795\n","1796\n","1797\n","1798\n","1799\n","1800\n","1801\n","1802\n","1803\n","1804\n","1805\n","1806\n","1807\n","1808\n","1809\n","1810\n","1811\n","1812\n","1813\n","1814\n","1815\n","1816\n","1817\n","1818\n","1819\n","1820\n","1821\n","1822\n","1823\n","1824\n","1825\n","1826\n","1827\n","1828\n","1829\n","1830\n","1831\n","1832\n","1833\n","1834\n","1835\n","1836\n","1837\n","1838\n","1839\n","1840\n","1841\n","1842\n","1843\n","1844\n","1845\n","1846\n","1847\n","1848\n","1849\n","1850\n","1851\n","1852\n","1853\n","1854\n","1855\n","1856\n","1857\n","1858\n","1859\n","1860\n","1861\n","1862\n","1863\n","1864\n","1865\n","1866\n","1867\n","1868\n","1869\n","1870\n","1871\n","1872\n","1873\n","1874\n","1875\n","1876\n","1877\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mjTXpCLGPGlQ","colab_type":"code","outputId":"37bbb946-ff04-465f-f2f1-35617786d285","executionInfo":{"status":"ok","timestamp":1583959764594,"user_tz":-330,"elapsed":1580754,"user":{"displayName":"Harsh Kataria","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimH0VsMEaiEzaEeo5nJzMe1WuJRQ2b1DK9fb3S=s64","userId":"07300304294680146354"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import ktrain\n","from ktrain import text\n","\n","\n","\n","modelslist = [['not_sarcastic','general','twisted_meaning','very_twisted']]\n","data_files = ['memotion_eq_allsarcastic.csv']\n","for columnclass, data_file in zip(modelslist, data_files):\n","\n","    \n","    columns=columnclass      \n","    DATA_PATH = '/content/drive/My Drive/memotion/' + data_file\n","    print(DATA_PATH)\n","\n","    NUM_WORDS = 90000\n","    MAXLEN = 250\n","\n","    h5name = '_'.join(columnclass)\n","    print('model', h5name)\n","\n","    (x_train, y_train), (x_test, y_test), preproc = text.texts_from_csv(DATA_PATH,\n","                          'cleaned_ocr',\n","                          label_columns = columns,\n","                          val_filepath=None, # if None, 10% of data will be used for validation\n","                          max_features=NUM_WORDS, maxlen=MAXLEN,\n","                        ngram_range=5,)\n","\n","\n","    model = text.text_classifier('logreg', (x_train, y_train), preproc=preproc,)\n","\n","    learner = ktrain.get_learner(model, train_data=(x_train, y_train), val_data=(x_test, y_test),batch_size=20)\n","    a='/content/drive/My Drive/Weight_file/'+h5name\n","    learner.autofit(0.00001, 500,early_stopping=300, reduce_on_plateau=5,)\n","\n","    a=learner.validate(val_data=(x_test, y_test), class_names=columns)\n","  \n","    predictor = ktrain.get_predictor(learner.model, preproc)\n","\n","   # score = 'xyz'########################\n","    h5name = \"/content/drive/My Drive/memotion/wgts/allsarcastic\"+h5name +  '.h5'\n","    print('model', h5name)\n","\n","    predictor.save(h5name)\n","    print(\"Model Saved\")\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/memotion/memotion_eq_onlysarcastic2.csv\n","model general_twisted_meaning_very_twisted\n","detected encoding: utf-8 (if wrong, set manually)\n","language: en\n","Word Counts: 11484\n","Nrows: 10521\n","10521 train sequences\n","train sequence lengths:\n","\tmean : 8\n","\t95percentile : 17\n","\t99percentile : 25\n","Adding 5-gram features\n","max_features changed to 224469 with addition of ngrams\n","Average train sequence length with ngrams: 30\n","train (w/ngrams) sequence lengths:\n","\tmean : 30\n","\t95percentile : 75\n","\t99percentile : 115\n","x_train shape: (10521,250)\n","y_train shape: (10521, 3)\n","1170 test sequences\n","test sequence lengths:\n","\tmean : 8\n","\t95percentile : 17\n","\t99percentile : 23\n","Average test sequence length with ngrams: 21\n","test (w/ngrams) sequence lengths:\n","\tmean : 21\n","\t95percentile : 60\n","\t99percentile : 102\n","x_test shape: (1170,250)\n","y_test shape: (1170, 3)\n","Is Multi-Label? False\n","compiling word ID features...\n","maxlen is 250\n","done.\n","\n","\n","begin training using triangular learning rate policy with max lr of 1e-05...\n","Train on 10521 samples, validate on 1170 samples\n","Epoch 1/500\n","10521/10521 [==============================] - 4s 359us/sample - loss: 0.9519 - accuracy: 0.3847 - val_loss: 0.9502 - val_accuracy: 0.4333\n","Epoch 2/500\n","10521/10521 [==============================] - 3s 318us/sample - loss: 0.9464 - accuracy: 0.5032 - val_loss: 0.9462 - val_accuracy: 0.5137\n","Epoch 3/500\n","10521/10521 [==============================] - 3s 308us/sample - loss: 0.9406 - accuracy: 0.6083 - val_loss: 0.9419 - val_accuracy: 0.5709\n","Epoch 4/500\n","10521/10521 [==============================] - 3s 308us/sample - loss: 0.9348 - accuracy: 0.6747 - val_loss: 0.9379 - val_accuracy: 0.6009\n","Epoch 5/500\n","10521/10521 [==============================] - 3s 311us/sample - loss: 0.9290 - accuracy: 0.7247 - val_loss: 0.9337 - val_accuracy: 0.6368\n","Epoch 6/500\n","10521/10521 [==============================] - 3s 318us/sample - loss: 0.9232 - accuracy: 0.7645 - val_loss: 0.9296 - val_accuracy: 0.6573\n","Epoch 7/500\n","10521/10521 [==============================] - 3s 313us/sample - loss: 0.9175 - accuracy: 0.7936 - val_loss: 0.9256 - val_accuracy: 0.6821\n","Epoch 8/500\n","10521/10521 [==============================] - 3s 310us/sample - loss: 0.9118 - accuracy: 0.8163 - val_loss: 0.9216 - val_accuracy: 0.6915\n","Epoch 9/500\n","10521/10521 [==============================] - 3s 310us/sample - loss: 0.9061 - accuracy: 0.8349 - val_loss: 0.9176 - val_accuracy: 0.7034\n","Epoch 10/500\n","10521/10521 [==============================] - 3s 314us/sample - loss: 0.9006 - accuracy: 0.8476 - val_loss: 0.9138 - val_accuracy: 0.7128\n","Epoch 11/500\n","10521/10521 [==============================] - 3s 309us/sample - loss: 0.8951 - accuracy: 0.8594 - val_loss: 0.9099 - val_accuracy: 0.7197\n","Epoch 12/500\n","10521/10521 [==============================] - 3s 300us/sample - loss: 0.8896 - accuracy: 0.8693 - val_loss: 0.9061 - val_accuracy: 0.7256\n","Epoch 13/500\n","10521/10521 [==============================] - 3s 307us/sample - loss: 0.8842 - accuracy: 0.8788 - val_loss: 0.9024 - val_accuracy: 0.7350\n","Epoch 14/500\n","10521/10521 [==============================] - 3s 309us/sample - loss: 0.8788 - accuracy: 0.8857 - val_loss: 0.8987 - val_accuracy: 0.7436\n","Epoch 15/500\n","10521/10521 [==============================] - 3s 304us/sample - loss: 0.8735 - accuracy: 0.8904 - val_loss: 0.8950 - val_accuracy: 0.7504\n","Epoch 16/500\n","10521/10521 [==============================] - 3s 309us/sample - loss: 0.8683 - accuracy: 0.8943 - val_loss: 0.8914 - val_accuracy: 0.7564\n","Epoch 17/500\n","10521/10521 [==============================] - 3s 303us/sample - loss: 0.8631 - accuracy: 0.8972 - val_loss: 0.8878 - val_accuracy: 0.7573\n","Epoch 18/500\n","10521/10521 [==============================] - 3s 303us/sample - loss: 0.8579 - accuracy: 0.8994 - val_loss: 0.8843 - val_accuracy: 0.7615\n","Epoch 19/500\n","10521/10521 [==============================] - 3s 305us/sample - loss: 0.8528 - accuracy: 0.9024 - val_loss: 0.8807 - val_accuracy: 0.7632\n","Epoch 20/500\n","10521/10521 [==============================] - 3s 309us/sample - loss: 0.8477 - accuracy: 0.9042 - val_loss: 0.8773 - val_accuracy: 0.7675\n","Epoch 21/500\n","10521/10521 [==============================] - 3s 302us/sample - loss: 0.8427 - accuracy: 0.9066 - val_loss: 0.8738 - val_accuracy: 0.7718\n","Epoch 22/500\n","10521/10521 [==============================] - 3s 300us/sample - loss: 0.8377 - accuracy: 0.9088 - val_loss: 0.8704 - val_accuracy: 0.7778\n","Epoch 23/500\n","10521/10521 [==============================] - 3s 304us/sample - loss: 0.8328 - accuracy: 0.9103 - val_loss: 0.8669 - val_accuracy: 0.7786\n","Epoch 24/500\n","10521/10521 [==============================] - 3s 304us/sample - loss: 0.8279 - accuracy: 0.9108 - val_loss: 0.8636 - val_accuracy: 0.7821\n","Epoch 25/500\n","10521/10521 [==============================] - 3s 300us/sample - loss: 0.8231 - accuracy: 0.9139 - val_loss: 0.8602 - val_accuracy: 0.7855\n","Epoch 26/500\n","10521/10521 [==============================] - 3s 303us/sample - loss: 0.8183 - accuracy: 0.9146 - val_loss: 0.8570 - val_accuracy: 0.7923\n","Epoch 27/500\n","10521/10521 [==============================] - 3s 301us/sample - loss: 0.8135 - accuracy: 0.9159 - val_loss: 0.8536 - val_accuracy: 0.7923\n","Epoch 28/500\n","10521/10521 [==============================] - 3s 304us/sample - loss: 0.8088 - accuracy: 0.9165 - val_loss: 0.8504 - val_accuracy: 0.7940\n","Epoch 29/500\n","10521/10521 [==============================] - 3s 304us/sample - loss: 0.8042 - accuracy: 0.9179 - val_loss: 0.8472 - val_accuracy: 0.7957\n","Epoch 30/500\n","10521/10521 [==============================] - 3s 297us/sample - loss: 0.7996 - accuracy: 0.9184 - val_loss: 0.8440 - val_accuracy: 0.7957\n","Epoch 31/500\n","10521/10521 [==============================] - 3s 306us/sample - loss: 0.7950 - accuracy: 0.9191 - val_loss: 0.8409 - val_accuracy: 0.7966\n","Epoch 32/500\n","10521/10521 [==============================] - 3s 302us/sample - loss: 0.7905 - accuracy: 0.9203 - val_loss: 0.8378 - val_accuracy: 0.7983\n","Epoch 33/500\n","10521/10521 [==============================] - 3s 301us/sample - loss: 0.7860 - accuracy: 0.9219 - val_loss: 0.8347 - val_accuracy: 0.8009\n","Epoch 34/500\n","10521/10521 [==============================] - 3s 302us/sample - loss: 0.7816 - accuracy: 0.9224 - val_loss: 0.8317 - val_accuracy: 0.8000\n","Epoch 35/500\n","10521/10521 [==============================] - 3s 301us/sample - loss: 0.7772 - accuracy: 0.9227 - val_loss: 0.8287 - val_accuracy: 0.8000\n","Epoch 36/500\n","10521/10521 [==============================] - 3s 305us/sample - loss: 0.7728 - accuracy: 0.9228 - val_loss: 0.8257 - val_accuracy: 0.8009\n","Epoch 37/500\n","10521/10521 [==============================] - 3s 301us/sample - loss: 0.7685 - accuracy: 0.9242 - val_loss: 0.8227 - val_accuracy: 0.8009\n","Epoch 38/500\n","10521/10521 [==============================] - 3s 302us/sample - loss: 0.7643 - accuracy: 0.9246 - val_loss: 0.8198 - val_accuracy: 0.8017\n","Epoch 39/500\n","10521/10521 [==============================] - 3s 301us/sample - loss: 0.7601 - accuracy: 0.9258 - val_loss: 0.8170 - val_accuracy: 0.8034\n","Epoch 40/500\n","10521/10521 [==============================] - 3s 303us/sample - loss: 0.7559 - accuracy: 0.9268 - val_loss: 0.8141 - val_accuracy: 0.8060\n","Epoch 41/500\n","10521/10521 [==============================] - 3s 299us/sample - loss: 0.7517 - accuracy: 0.9278 - val_loss: 0.8112 - val_accuracy: 0.8051\n","Epoch 42/500\n","10521/10521 [==============================] - 3s 306us/sample - loss: 0.7476 - accuracy: 0.9282 - val_loss: 0.8083 - val_accuracy: 0.8085\n","Epoch 43/500\n","10521/10521 [==============================] - 3s 307us/sample - loss: 0.7435 - accuracy: 0.9288 - val_loss: 0.8055 - val_accuracy: 0.8103\n","Epoch 44/500\n","10521/10521 [==============================] - 3s 301us/sample - loss: 0.7394 - accuracy: 0.9291 - val_loss: 0.8027 - val_accuracy: 0.8128\n","Epoch 45/500\n","10521/10521 [==============================] - 3s 299us/sample - loss: 0.7354 - accuracy: 0.9292 - val_loss: 0.7999 - val_accuracy: 0.8128\n","Epoch 46/500\n","10521/10521 [==============================] - 3s 302us/sample - loss: 0.7314 - accuracy: 0.9297 - val_loss: 0.7972 - val_accuracy: 0.8137\n","Epoch 47/500\n","10521/10521 [==============================] - 3s 309us/sample - loss: 0.7274 - accuracy: 0.9300 - val_loss: 0.7944 - val_accuracy: 0.8137\n","Epoch 48/500\n","10521/10521 [==============================] - 3s 298us/sample - loss: 0.7235 - accuracy: 0.9305 - val_loss: 0.7918 - val_accuracy: 0.8154\n","Epoch 49/500\n","10521/10521 [==============================] - 3s 298us/sample - loss: 0.7197 - accuracy: 0.9312 - val_loss: 0.7892 - val_accuracy: 0.8154\n","Epoch 50/500\n","10521/10521 [==============================] - 3s 302us/sample - loss: 0.7158 - accuracy: 0.9320 - val_loss: 0.7865 - val_accuracy: 0.8171\n","Epoch 51/500\n","10521/10521 [==============================] - 3s 306us/sample - loss: 0.7120 - accuracy: 0.9326 - val_loss: 0.7838 - val_accuracy: 0.8145\n","Epoch 52/500\n","10521/10521 [==============================] - 3s 306us/sample - loss: 0.7082 - accuracy: 0.9327 - val_loss: 0.7812 - val_accuracy: 0.8154\n","Epoch 53/500\n","10521/10521 [==============================] - 3s 331us/sample - loss: 0.7044 - accuracy: 0.9336 - val_loss: 0.7786 - val_accuracy: 0.8171\n","Epoch 54/500\n","10521/10521 [==============================] - 4s 348us/sample - loss: 0.7007 - accuracy: 0.9340 - val_loss: 0.7761 - val_accuracy: 0.8179\n","Epoch 55/500\n","10521/10521 [==============================] - 4s 337us/sample - loss: 0.6971 - accuracy: 0.9339 - val_loss: 0.7735 - val_accuracy: 0.8179\n","Epoch 56/500\n","10521/10521 [==============================] - 3s 302us/sample - loss: 0.6934 - accuracy: 0.9346 - val_loss: 0.7710 - val_accuracy: 0.8179\n","Epoch 57/500\n","10521/10521 [==============================] - 3s 303us/sample - loss: 0.6898 - accuracy: 0.9350 - val_loss: 0.7685 - val_accuracy: 0.8197\n","Epoch 58/500\n","10521/10521 [==============================] - 3s 307us/sample - loss: 0.6862 - accuracy: 0.9354 - val_loss: 0.7660 - val_accuracy: 0.8239\n","Epoch 59/500\n","10521/10521 [==============================] - 3s 298us/sample - loss: 0.6826 - accuracy: 0.9361 - val_loss: 0.7636 - val_accuracy: 0.8239\n","Epoch 60/500\n","10521/10521 [==============================] - 3s 295us/sample - loss: 0.6791 - accuracy: 0.9367 - val_loss: 0.7612 - val_accuracy: 0.8248\n","Epoch 61/500\n","10521/10521 [==============================] - 3s 305us/sample - loss: 0.6756 - accuracy: 0.9366 - val_loss: 0.7588 - val_accuracy: 0.8265\n","Epoch 62/500\n","10521/10521 [==============================] - 3s 302us/sample - loss: 0.6721 - accuracy: 0.9376 - val_loss: 0.7564 - val_accuracy: 0.8274\n","Epoch 63/500\n","10521/10521 [==============================] - 3s 306us/sample - loss: 0.6687 - accuracy: 0.9383 - val_loss: 0.7540 - val_accuracy: 0.8291\n","Epoch 64/500\n","10521/10521 [==============================] - 3s 305us/sample - loss: 0.6653 - accuracy: 0.9386 - val_loss: 0.7516 - val_accuracy: 0.8291\n","Epoch 65/500\n","10521/10521 [==============================] - 3s 305us/sample - loss: 0.6618 - accuracy: 0.9390 - val_loss: 0.7493 - val_accuracy: 0.8325\n","Epoch 66/500\n","10521/10521 [==============================] - 3s 303us/sample - loss: 0.6585 - accuracy: 0.9396 - val_loss: 0.7470 - val_accuracy: 0.8316\n","Epoch 67/500\n","10521/10521 [==============================] - 3s 298us/sample - loss: 0.6552 - accuracy: 0.9397 - val_loss: 0.7447 - val_accuracy: 0.8316\n","Epoch 68/500\n","10521/10521 [==============================] - 3s 296us/sample - loss: 0.6519 - accuracy: 0.9401 - val_loss: 0.7424 - val_accuracy: 0.8333\n","Epoch 69/500\n","10521/10521 [==============================] - 3s 295us/sample - loss: 0.6486 - accuracy: 0.9404 - val_loss: 0.7402 - val_accuracy: 0.8350\n","Epoch 70/500\n","10521/10521 [==============================] - 3s 306us/sample - loss: 0.6453 - accuracy: 0.9410 - val_loss: 0.7379 - val_accuracy: 0.8350\n","Epoch 71/500\n","10521/10521 [==============================] - 3s 303us/sample - loss: 0.6421 - accuracy: 0.9414 - val_loss: 0.7357 - val_accuracy: 0.8350\n","Epoch 72/500\n","10521/10521 [==============================] - 3s 298us/sample - loss: 0.6389 - accuracy: 0.9414 - val_loss: 0.7335 - val_accuracy: 0.8350\n","Epoch 73/500\n","10521/10521 [==============================] - 3s 296us/sample - loss: 0.6358 - accuracy: 0.9423 - val_loss: 0.7313 - val_accuracy: 0.8359\n","Epoch 74/500\n","10521/10521 [==============================] - 3s 298us/sample - loss: 0.6326 - accuracy: 0.9425 - val_loss: 0.7292 - val_accuracy: 0.8376\n","Epoch 75/500\n","10521/10521 [==============================] - 3s 299us/sample - loss: 0.6295 - accuracy: 0.9427 - val_loss: 0.7270 - val_accuracy: 0.8368\n","Epoch 76/500\n","10521/10521 [==============================] - 3s 308us/sample - loss: 0.6264 - accuracy: 0.9431 - val_loss: 0.7249 - val_accuracy: 0.8393\n","Epoch 77/500\n","10521/10521 [==============================] - 3s 301us/sample - loss: 0.6233 - accuracy: 0.9434 - val_loss: 0.7228 - val_accuracy: 0.8393\n","Epoch 78/500\n","10521/10521 [==============================] - 3s 303us/sample - loss: 0.6202 - accuracy: 0.9438 - val_loss: 0.7207 - val_accuracy: 0.8410\n","Epoch 79/500\n","10521/10521 [==============================] - 3s 303us/sample - loss: 0.6172 - accuracy: 0.9441 - val_loss: 0.7186 - val_accuracy: 0.8410\n","Epoch 80/500\n","10521/10521 [==============================] - 3s 306us/sample - loss: 0.6142 - accuracy: 0.9441 - val_loss: 0.7165 - val_accuracy: 0.8419\n","Epoch 81/500\n","10521/10521 [==============================] - 3s 296us/sample - loss: 0.6112 - accuracy: 0.9440 - val_loss: 0.7145 - val_accuracy: 0.8436\n","Epoch 82/500\n","10521/10521 [==============================] - 3s 299us/sample - loss: 0.6083 - accuracy: 0.9448 - val_loss: 0.7124 - val_accuracy: 0.8436\n","Epoch 83/500\n","10521/10521 [==============================] - 4s 356us/sample - loss: 0.6054 - accuracy: 0.9448 - val_loss: 0.7105 - val_accuracy: 0.8436\n","Epoch 84/500\n","10521/10521 [==============================] - 4s 355us/sample - loss: 0.6025 - accuracy: 0.9448 - val_loss: 0.7084 - val_accuracy: 0.8436\n","Epoch 85/500\n","10521/10521 [==============================] - 3s 307us/sample - loss: 0.5996 - accuracy: 0.9453 - val_loss: 0.7065 - val_accuracy: 0.8436\n","Epoch 86/500\n","10521/10521 [==============================] - 3s 297us/sample - loss: 0.5967 - accuracy: 0.9453 - val_loss: 0.7045 - val_accuracy: 0.8453\n","Epoch 87/500\n","10521/10521 [==============================] - 3s 301us/sample - loss: 0.5939 - accuracy: 0.9457 - val_loss: 0.7026 - val_accuracy: 0.8453\n","Epoch 88/500\n","10521/10521 [==============================] - 3s 296us/sample - loss: 0.5911 - accuracy: 0.9456 - val_loss: 0.7006 - val_accuracy: 0.8470\n","Epoch 89/500\n","10521/10521 [==============================] - 3s 291us/sample - loss: 0.5883 - accuracy: 0.9461 - val_loss: 0.6987 - val_accuracy: 0.8487\n","Epoch 90/500\n","10521/10521 [==============================] - 3s 295us/sample - loss: 0.5855 - accuracy: 0.9463 - val_loss: 0.6968 - val_accuracy: 0.8479\n","Epoch 91/500\n","10521/10521 [==============================] - 3s 296us/sample - loss: 0.5828 - accuracy: 0.9467 - val_loss: 0.6949 - val_accuracy: 0.8496\n","Epoch 92/500\n","10521/10521 [==============================] - 3s 301us/sample - loss: 0.5800 - accuracy: 0.9468 - val_loss: 0.6930 - val_accuracy: 0.8504\n","Epoch 93/500\n","10521/10521 [==============================] - 3s 294us/sample - loss: 0.5773 - accuracy: 0.9472 - val_loss: 0.6912 - val_accuracy: 0.8504\n","Epoch 94/500\n","10521/10521 [==============================] - 3s 296us/sample - loss: 0.5746 - accuracy: 0.9474 - val_loss: 0.6893 - val_accuracy: 0.8504\n","Epoch 95/500\n","10521/10521 [==============================] - 3s 296us/sample - loss: 0.5720 - accuracy: 0.9476 - val_loss: 0.6875 - val_accuracy: 0.8513\n","Epoch 96/500\n","10521/10521 [==============================] - 3s 290us/sample - loss: 0.5693 - accuracy: 0.9479 - val_loss: 0.6857 - val_accuracy: 0.8513\n","Epoch 97/500\n","10521/10521 [==============================] - 3s 299us/sample - loss: 0.5667 - accuracy: 0.9482 - val_loss: 0.6839 - val_accuracy: 0.8521\n","Epoch 98/500\n","10521/10521 [==============================] - 3s 294us/sample - loss: 0.5641 - accuracy: 0.9483 - val_loss: 0.6821 - val_accuracy: 0.8538\n","Epoch 99/500\n","10521/10521 [==============================] - 3s 294us/sample - loss: 0.5615 - accuracy: 0.9487 - val_loss: 0.6803 - val_accuracy: 0.8547\n","Epoch 100/500\n","10521/10521 [==============================] - 3s 302us/sample - loss: 0.5590 - accuracy: 0.9488 - val_loss: 0.6785 - val_accuracy: 0.8564\n","Epoch 101/500\n","10521/10521 [==============================] - 3s 294us/sample - loss: 0.5564 - accuracy: 0.9491 - val_loss: 0.6767 - val_accuracy: 0.8573\n","Epoch 102/500\n","10521/10521 [==============================] - 3s 295us/sample - loss: 0.5539 - accuracy: 0.9491 - val_loss: 0.6750 - val_accuracy: 0.8581\n","Epoch 103/500\n","10521/10521 [==============================] - 3s 299us/sample - loss: 0.5514 - accuracy: 0.9495 - val_loss: 0.6732 - val_accuracy: 0.8581\n","Epoch 104/500\n","10521/10521 [==============================] - 3s 297us/sample - loss: 0.5489 - accuracy: 0.9496 - val_loss: 0.6715 - val_accuracy: 0.8590\n","Epoch 105/500\n","10521/10521 [==============================] - 3s 293us/sample - loss: 0.5464 - accuracy: 0.9497 - val_loss: 0.6698 - val_accuracy: 0.8590\n","Epoch 106/500\n","10521/10521 [==============================] - 3s 299us/sample - loss: 0.5439 - accuracy: 0.9503 - val_loss: 0.6681 - val_accuracy: 0.8598\n","Epoch 107/500\n","10521/10521 [==============================] - 3s 304us/sample - loss: 0.5415 - accuracy: 0.9504 - val_loss: 0.6664 - val_accuracy: 0.8607\n","Epoch 108/500\n","10521/10521 [==============================] - 3s 296us/sample - loss: 0.5391 - accuracy: 0.9509 - val_loss: 0.6648 - val_accuracy: 0.8615\n","Epoch 109/500\n","10521/10521 [==============================] - 3s 299us/sample - loss: 0.5367 - accuracy: 0.9511 - val_loss: 0.6631 - val_accuracy: 0.8607\n","Epoch 110/500\n","10521/10521 [==============================] - 3s 296us/sample - loss: 0.5343 - accuracy: 0.9512 - val_loss: 0.6615 - val_accuracy: 0.8607\n","Epoch 111/500\n","10521/10521 [==============================] - 3s 291us/sample - loss: 0.5319 - accuracy: 0.9516 - val_loss: 0.6598 - val_accuracy: 0.8607\n","Epoch 112/500\n","10521/10521 [==============================] - 3s 299us/sample - loss: 0.5296 - accuracy: 0.9517 - val_loss: 0.6582 - val_accuracy: 0.8615\n","Epoch 113/500\n","10521/10521 [==============================] - 3s 293us/sample - loss: 0.5272 - accuracy: 0.9522 - val_loss: 0.6566 - val_accuracy: 0.8624\n","Epoch 114/500\n","10521/10521 [==============================] - 3s 294us/sample - loss: 0.5249 - accuracy: 0.9524 - val_loss: 0.6550 - val_accuracy: 0.8624\n","Epoch 115/500\n","10521/10521 [==============================] - 3s 296us/sample - loss: 0.5226 - accuracy: 0.9528 - val_loss: 0.6534 - val_accuracy: 0.8632\n","Epoch 116/500\n","10521/10521 [==============================] - 3s 294us/sample - loss: 0.5204 - accuracy: 0.9528 - val_loss: 0.6518 - val_accuracy: 0.8624\n","Epoch 117/500\n","10521/10521 [==============================] - 3s 305us/sample - loss: 0.5181 - accuracy: 0.9525 - val_loss: 0.6503 - val_accuracy: 0.8624\n","Epoch 118/500\n","10521/10521 [==============================] - 3s 298us/sample - loss: 0.5158 - accuracy: 0.9526 - val_loss: 0.6487 - val_accuracy: 0.8624\n","Epoch 119/500\n","10521/10521 [==============================] - 3s 291us/sample - loss: 0.5136 - accuracy: 0.9527 - val_loss: 0.6472 - val_accuracy: 0.8624\n","Epoch 120/500\n","10521/10521 [==============================] - 3s 294us/sample - loss: 0.5114 - accuracy: 0.9527 - val_loss: 0.6456 - val_accuracy: 0.8624\n","Epoch 121/500\n","10521/10521 [==============================] - 3s 293us/sample - loss: 0.5092 - accuracy: 0.9533 - val_loss: 0.6441 - val_accuracy: 0.8632\n","Epoch 122/500\n","10521/10521 [==============================] - 3s 295us/sample - loss: 0.5070 - accuracy: 0.9531 - val_loss: 0.6426 - val_accuracy: 0.8632\n","Epoch 123/500\n","10521/10521 [==============================] - 3s 299us/sample - loss: 0.5048 - accuracy: 0.9532 - val_loss: 0.6411 - val_accuracy: 0.8632\n","Epoch 124/500\n","10521/10521 [==============================] - 3s 302us/sample - loss: 0.5027 - accuracy: 0.9534 - val_loss: 0.6396 - val_accuracy: 0.8632\n","Epoch 125/500\n","10521/10521 [==============================] - 3s 300us/sample - loss: 0.5005 - accuracy: 0.9536 - val_loss: 0.6381 - val_accuracy: 0.8624\n","Epoch 126/500\n","10521/10521 [==============================] - 3s 298us/sample - loss: 0.4984 - accuracy: 0.9537 - val_loss: 0.6366 - val_accuracy: 0.8632\n","Epoch 127/500\n","10521/10521 [==============================] - 3s 300us/sample - loss: 0.4963 - accuracy: 0.9539 - val_loss: 0.6351 - val_accuracy: 0.8632\n","Epoch 128/500\n","10521/10521 [==============================] - 3s 296us/sample - loss: 0.4942 - accuracy: 0.9541 - val_loss: 0.6336 - val_accuracy: 0.8632\n","Epoch 129/500\n","10521/10521 [==============================] - 3s 295us/sample - loss: 0.4921 - accuracy: 0.9542 - val_loss: 0.6322 - val_accuracy: 0.8641\n","Epoch 130/500\n","10521/10521 [==============================] - 3s 300us/sample - loss: 0.4900 - accuracy: 0.9544 - val_loss: 0.6307 - val_accuracy: 0.8641\n","Epoch 131/500\n","10521/10521 [==============================] - 3s 301us/sample - loss: 0.4880 - accuracy: 0.9548 - val_loss: 0.6293 - val_accuracy: 0.8641\n","Epoch 132/500\n","10521/10521 [==============================] - 3s 300us/sample - loss: 0.4859 - accuracy: 0.9549 - val_loss: 0.6279 - val_accuracy: 0.8641\n","Epoch 133/500\n","10521/10521 [==============================] - 3s 295us/sample - loss: 0.4839 - accuracy: 0.9549 - val_loss: 0.6265 - val_accuracy: 0.8650\n","Epoch 134/500\n","10521/10521 [==============================] - 3s 294us/sample - loss: 0.4819 - accuracy: 0.9553 - val_loss: 0.6251 - val_accuracy: 0.8650\n","Epoch 135/500\n","10521/10521 [==============================] - 3s 298us/sample - loss: 0.4799 - accuracy: 0.9558 - val_loss: 0.6237 - val_accuracy: 0.8650\n","Epoch 136/500\n","10521/10521 [==============================] - 3s 292us/sample - loss: 0.4779 - accuracy: 0.9554 - val_loss: 0.6223 - val_accuracy: 0.8650\n","Epoch 137/500\n","10521/10521 [==============================] - 3s 301us/sample - loss: 0.4759 - accuracy: 0.9561 - val_loss: 0.6209 - val_accuracy: 0.8650\n","Epoch 138/500\n","10521/10521 [==============================] - 3s 306us/sample - loss: 0.4740 - accuracy: 0.9562 - val_loss: 0.6195 - val_accuracy: 0.8650\n","Epoch 139/500\n","10521/10521 [==============================] - 3s 295us/sample - loss: 0.4720 - accuracy: 0.9564 - val_loss: 0.6182 - val_accuracy: 0.8650\n","Epoch 140/500\n","10521/10521 [==============================] - 3s 294us/sample - loss: 0.4701 - accuracy: 0.9565 - val_loss: 0.6168 - val_accuracy: 0.8650\n","Epoch 141/500\n","10521/10521 [==============================] - 3s 299us/sample - loss: 0.4682 - accuracy: 0.9565 - val_loss: 0.6154 - val_accuracy: 0.8650\n","Epoch 142/500\n","10521/10521 [==============================] - 3s 297us/sample - loss: 0.4663 - accuracy: 0.9566 - val_loss: 0.6141 - val_accuracy: 0.8658\n","Epoch 143/500\n","10521/10521 [==============================] - 3s 295us/sample - loss: 0.4644 - accuracy: 0.9568 - val_loss: 0.6128 - val_accuracy: 0.8658\n","Epoch 144/500\n","10521/10521 [==============================] - 3s 292us/sample - loss: 0.4625 - accuracy: 0.9570 - val_loss: 0.6115 - val_accuracy: 0.8658\n","Epoch 145/500\n","10521/10521 [==============================] - 3s 296us/sample - loss: 0.4607 - accuracy: 0.9572 - val_loss: 0.6101 - val_accuracy: 0.8667\n","Epoch 146/500\n","10521/10521 [==============================] - 3s 299us/sample - loss: 0.4588 - accuracy: 0.9573 - val_loss: 0.6089 - val_accuracy: 0.8675\n","Epoch 147/500\n","10521/10521 [==============================] - 3s 310us/sample - loss: 0.4570 - accuracy: 0.9574 - val_loss: 0.6076 - val_accuracy: 0.8675\n","Epoch 148/500\n","10521/10521 [==============================] - 3s 302us/sample - loss: 0.4552 - accuracy: 0.9575 - val_loss: 0.6062 - val_accuracy: 0.8675\n","Epoch 149/500\n","10521/10521 [==============================] - 3s 299us/sample - loss: 0.4533 - accuracy: 0.9575 - val_loss: 0.6050 - val_accuracy: 0.8684\n","Epoch 150/500\n","10521/10521 [==============================] - 3s 299us/sample - loss: 0.4515 - accuracy: 0.9577 - val_loss: 0.6037 - val_accuracy: 0.8692\n","Epoch 151/500\n","10521/10521 [==============================] - 3s 321us/sample - loss: 0.4498 - accuracy: 0.9578 - val_loss: 0.6024 - val_accuracy: 0.8692\n","Epoch 152/500\n","10521/10521 [==============================] - 3s 327us/sample - loss: 0.4480 - accuracy: 0.9579 - val_loss: 0.6011 - val_accuracy: 0.8692\n","Epoch 153/500\n","10521/10521 [==============================] - 3s 330us/sample - loss: 0.4462 - accuracy: 0.9582 - val_loss: 0.5998 - val_accuracy: 0.8692\n","Epoch 154/500\n","10521/10521 [==============================] - 3s 306us/sample - loss: 0.4444 - accuracy: 0.9581 - val_loss: 0.5986 - val_accuracy: 0.8692\n","Epoch 155/500\n","10521/10521 [==============================] - 3s 297us/sample - loss: 0.4427 - accuracy: 0.9582 - val_loss: 0.5973 - val_accuracy: 0.8701\n","Epoch 156/500\n","10521/10521 [==============================] - 3s 293us/sample - loss: 0.4410 - accuracy: 0.9584 - val_loss: 0.5961 - val_accuracy: 0.8709\n","Epoch 157/500\n","10521/10521 [==============================] - 3s 296us/sample - loss: 0.4392 - accuracy: 0.9585 - val_loss: 0.5949 - val_accuracy: 0.8709\n","Epoch 158/500\n","10521/10521 [==============================] - 3s 293us/sample - loss: 0.4375 - accuracy: 0.9584 - val_loss: 0.5937 - val_accuracy: 0.8709\n","Epoch 159/500\n","10521/10521 [==============================] - 3s 295us/sample - loss: 0.4358 - accuracy: 0.9587 - val_loss: 0.5925 - val_accuracy: 0.8709\n","Epoch 160/500\n","10521/10521 [==============================] - 3s 297us/sample - loss: 0.4341 - accuracy: 0.9586 - val_loss: 0.5913 - val_accuracy: 0.8709\n","Epoch 161/500\n","10521/10521 [==============================] - 3s 296us/sample - loss: 0.4325 - accuracy: 0.9587 - val_loss: 0.5901 - val_accuracy: 0.8709\n","Epoch 162/500\n","10521/10521 [==============================] - 3s 290us/sample - loss: 0.4308 - accuracy: 0.9589 - val_loss: 0.5889 - val_accuracy: 0.8718\n","Epoch 163/500\n","10521/10521 [==============================] - 3s 295us/sample - loss: 0.4291 - accuracy: 0.9592 - val_loss: 0.5877 - val_accuracy: 0.8718\n","Epoch 164/500\n","10521/10521 [==============================] - 3s 294us/sample - loss: 0.4275 - accuracy: 0.9592 - val_loss: 0.5866 - val_accuracy: 0.8718\n","Epoch 165/500\n","10521/10521 [==============================] - 3s 306us/sample - loss: 0.4259 - accuracy: 0.9592 - val_loss: 0.5854 - val_accuracy: 0.8718\n","Epoch 166/500\n","10521/10521 [==============================] - 3s 306us/sample - loss: 0.4242 - accuracy: 0.9593 - val_loss: 0.5842 - val_accuracy: 0.8726\n","Epoch 167/500\n","10521/10521 [==============================] - 3s 293us/sample - loss: 0.4226 - accuracy: 0.9593 - val_loss: 0.5831 - val_accuracy: 0.8726\n","Epoch 168/500\n","10521/10521 [==============================] - 3s 299us/sample - loss: 0.4210 - accuracy: 0.9593 - val_loss: 0.5819 - val_accuracy: 0.8726\n","Epoch 169/500\n","10521/10521 [==============================] - 3s 303us/sample - loss: 0.4194 - accuracy: 0.9591 - val_loss: 0.5808 - val_accuracy: 0.8735\n","Epoch 170/500\n","10521/10521 [==============================] - 3s 299us/sample - loss: 0.4179 - accuracy: 0.9594 - val_loss: 0.5797 - val_accuracy: 0.8726\n","Epoch 171/500\n","10521/10521 [==============================] - 3s 302us/sample - loss: 0.4163 - accuracy: 0.9595 - val_loss: 0.5786 - val_accuracy: 0.8735\n","Epoch 172/500\n","10521/10521 [==============================] - 3s 297us/sample - loss: 0.4147 - accuracy: 0.9595 - val_loss: 0.5775 - val_accuracy: 0.8735\n","Epoch 173/500\n","10521/10521 [==============================] - 3s 295us/sample - loss: 0.4132 - accuracy: 0.9594 - val_loss: 0.5763 - val_accuracy: 0.8735\n","Epoch 174/500\n","10521/10521 [==============================] - 3s 297us/sample - loss: 0.4117 - accuracy: 0.9595 - val_loss: 0.5752 - val_accuracy: 0.8735\n","Epoch 175/500\n","10521/10521 [==============================] - 3s 296us/sample - loss: 0.4101 - accuracy: 0.9596 - val_loss: 0.5742 - val_accuracy: 0.8735\n","Epoch 176/500\n","10521/10521 [==============================] - 3s 294us/sample - loss: 0.4086 - accuracy: 0.9597 - val_loss: 0.5731 - val_accuracy: 0.8735\n","Epoch 177/500\n","10521/10521 [==============================] - 3s 287us/sample - loss: 0.4071 - accuracy: 0.9597 - val_loss: 0.5720 - val_accuracy: 0.8735\n","Epoch 178/500\n","10521/10521 [==============================] - 3s 294us/sample - loss: 0.4056 - accuracy: 0.9599 - val_loss: 0.5709 - val_accuracy: 0.8735\n","Epoch 179/500\n","10521/10521 [==============================] - 3s 295us/sample - loss: 0.4041 - accuracy: 0.9598 - val_loss: 0.5699 - val_accuracy: 0.8735\n","Epoch 180/500\n","10521/10521 [==============================] - 3s 298us/sample - loss: 0.4026 - accuracy: 0.9601 - val_loss: 0.5688 - val_accuracy: 0.8735\n","Epoch 181/500\n","10521/10521 [==============================] - 3s 316us/sample - loss: 0.4012 - accuracy: 0.9600 - val_loss: 0.5677 - val_accuracy: 0.8718\n","Epoch 182/500\n","10521/10521 [==============================] - 3s 324us/sample - loss: 0.3997 - accuracy: 0.9602 - val_loss: 0.5667 - val_accuracy: 0.8726\n","Epoch 183/500\n","10521/10521 [==============================] - 3s 298us/sample - loss: 0.3983 - accuracy: 0.9600 - val_loss: 0.5656 - val_accuracy: 0.8726\n","Epoch 184/500\n","10521/10521 [==============================] - 3s 287us/sample - loss: 0.3968 - accuracy: 0.9601 - val_loss: 0.5646 - val_accuracy: 0.8718\n","Epoch 185/500\n","10521/10521 [==============================] - 3s 300us/sample - loss: 0.3954 - accuracy: 0.9601 - val_loss: 0.5636 - val_accuracy: 0.8718\n","Epoch 186/500\n","10521/10521 [==============================] - 3s 302us/sample - loss: 0.3940 - accuracy: 0.9602 - val_loss: 0.5626 - val_accuracy: 0.8726\n","Epoch 187/500\n","10521/10521 [==============================] - 3s 303us/sample - loss: 0.3925 - accuracy: 0.9604 - val_loss: 0.5616 - val_accuracy: 0.8735\n","Epoch 188/500\n","10521/10521 [==============================] - 3s 305us/sample - loss: 0.3911 - accuracy: 0.9607 - val_loss: 0.5605 - val_accuracy: 0.8735\n","Epoch 189/500\n","10521/10521 [==============================] - 3s 297us/sample - loss: 0.3897 - accuracy: 0.9607 - val_loss: 0.5595 - val_accuracy: 0.8735\n","Epoch 190/500\n","10521/10521 [==============================] - 3s 296us/sample - loss: 0.3884 - accuracy: 0.9606 - val_loss: 0.5584 - val_accuracy: 0.8735\n","Epoch 191/500\n","10521/10521 [==============================] - 3s 294us/sample - loss: 0.3870 - accuracy: 0.9607 - val_loss: 0.5575 - val_accuracy: 0.8735\n","Epoch 192/500\n","10521/10521 [==============================] - 3s 297us/sample - loss: 0.3856 - accuracy: 0.9608 - val_loss: 0.5565 - val_accuracy: 0.8744\n","Epoch 193/500\n","10521/10521 [==============================] - 3s 290us/sample - loss: 0.3842 - accuracy: 0.9610 - val_loss: 0.5555 - val_accuracy: 0.8744\n","Epoch 194/500\n","10521/10521 [==============================] - 3s 295us/sample - loss: 0.3829 - accuracy: 0.9610 - val_loss: 0.5545 - val_accuracy: 0.8744\n","Epoch 195/500\n","10521/10521 [==============================] - 3s 301us/sample - loss: 0.3815 - accuracy: 0.9612 - val_loss: 0.5535 - val_accuracy: 0.8744\n","Epoch 196/500\n","10521/10521 [==============================] - 3s 293us/sample - loss: 0.3802 - accuracy: 0.9612 - val_loss: 0.5525 - val_accuracy: 0.8752\n","Epoch 197/500\n","10521/10521 [==============================] - 3s 296us/sample - loss: 0.3789 - accuracy: 0.9613 - val_loss: 0.5516 - val_accuracy: 0.8761\n","Epoch 198/500\n","10521/10521 [==============================] - 3s 301us/sample - loss: 0.3775 - accuracy: 0.9614 - val_loss: 0.5506 - val_accuracy: 0.8761\n","Epoch 199/500\n","10521/10521 [==============================] - 3s 295us/sample - loss: 0.3762 - accuracy: 0.9615 - val_loss: 0.5496 - val_accuracy: 0.8761\n","Epoch 200/500\n","10521/10521 [==============================] - 3s 296us/sample - loss: 0.3749 - accuracy: 0.9609 - val_loss: 0.5487 - val_accuracy: 0.8761\n","Epoch 201/500\n","10521/10521 [==============================] - 3s 296us/sample - loss: 0.3736 - accuracy: 0.9610 - val_loss: 0.5477 - val_accuracy: 0.8752\n","Epoch 202/500\n","10521/10521 [==============================] - 3s 301us/sample - loss: 0.3723 - accuracy: 0.9614 - val_loss: 0.5468 - val_accuracy: 0.8761\n","Epoch 203/500\n","10521/10521 [==============================] - 3s 292us/sample - loss: 0.3710 - accuracy: 0.9612 - val_loss: 0.5459 - val_accuracy: 0.8761\n","Epoch 204/500\n","10521/10521 [==============================] - 3s 290us/sample - loss: 0.3697 - accuracy: 0.9616 - val_loss: 0.5450 - val_accuracy: 0.8761\n","Epoch 205/500\n","10521/10521 [==============================] - 3s 291us/sample - loss: 0.3685 - accuracy: 0.9616 - val_loss: 0.5440 - val_accuracy: 0.8761\n","Epoch 206/500\n","10521/10521 [==============================] - 3s 297us/sample - loss: 0.3672 - accuracy: 0.9617 - val_loss: 0.5431 - val_accuracy: 0.8761\n","Epoch 207/500\n","10521/10521 [==============================] - 3s 300us/sample - loss: 0.3660 - accuracy: 0.9619 - val_loss: 0.5422 - val_accuracy: 0.8761\n","Epoch 208/500\n","10521/10521 [==============================] - 3s 303us/sample - loss: 0.3647 - accuracy: 0.9621 - val_loss: 0.5413 - val_accuracy: 0.8761\n","Epoch 209/500\n","10521/10521 [==============================] - 3s 297us/sample - loss: 0.3635 - accuracy: 0.9620 - val_loss: 0.5404 - val_accuracy: 0.8761\n","Epoch 210/500\n","10521/10521 [==============================] - 3s 299us/sample - loss: 0.3622 - accuracy: 0.9620 - val_loss: 0.5395 - val_accuracy: 0.8769\n","Epoch 211/500\n","10521/10521 [==============================] - 3s 292us/sample - loss: 0.3610 - accuracy: 0.9622 - val_loss: 0.5386 - val_accuracy: 0.8769\n","Epoch 212/500\n","10521/10521 [==============================] - 3s 296us/sample - loss: 0.3598 - accuracy: 0.9623 - val_loss: 0.5377 - val_accuracy: 0.8769\n","Epoch 213/500\n","10521/10521 [==============================] - 3s 293us/sample - loss: 0.3586 - accuracy: 0.9622 - val_loss: 0.5368 - val_accuracy: 0.8769\n","Epoch 214/500\n","10521/10521 [==============================] - 3s 294us/sample - loss: 0.3574 - accuracy: 0.9622 - val_loss: 0.5359 - val_accuracy: 0.8769\n","Epoch 215/500\n","10521/10521 [==============================] - 3s 294us/sample - loss: 0.3562 - accuracy: 0.9624 - val_loss: 0.5351 - val_accuracy: 0.8769\n","Epoch 216/500\n","10521/10521 [==============================] - 3s 292us/sample - loss: 0.3550 - accuracy: 0.9624 - val_loss: 0.5342 - val_accuracy: 0.8769\n","Epoch 217/500\n","10521/10521 [==============================] - 3s 299us/sample - loss: 0.3538 - accuracy: 0.9624 - val_loss: 0.5333 - val_accuracy: 0.8769\n","Epoch 218/500\n","10521/10521 [==============================] - 3s 298us/sample - loss: 0.3526 - accuracy: 0.9624 - val_loss: 0.5325 - val_accuracy: 0.8778\n","Epoch 219/500\n","10521/10521 [==============================] - 3s 296us/sample - loss: 0.3515 - accuracy: 0.9624 - val_loss: 0.5316 - val_accuracy: 0.8778\n","Epoch 220/500\n","10521/10521 [==============================] - 3s 292us/sample - loss: 0.3503 - accuracy: 0.9624 - val_loss: 0.5308 - val_accuracy: 0.8778\n","Epoch 221/500\n","10521/10521 [==============================] - 3s 293us/sample - loss: 0.3492 - accuracy: 0.9624 - val_loss: 0.5299 - val_accuracy: 0.8778\n","Epoch 222/500\n","10521/10521 [==============================] - 3s 303us/sample - loss: 0.3480 - accuracy: 0.9625 - val_loss: 0.5291 - val_accuracy: 0.8778\n","Epoch 223/500\n","10521/10521 [==============================] - 3s 293us/sample - loss: 0.3469 - accuracy: 0.9624 - val_loss: 0.5282 - val_accuracy: 0.8786\n","Epoch 224/500\n","10521/10521 [==============================] - 3s 293us/sample - loss: 0.3457 - accuracy: 0.9626 - val_loss: 0.5274 - val_accuracy: 0.8786\n","Epoch 225/500\n","10521/10521 [==============================] - 3s 290us/sample - loss: 0.3446 - accuracy: 0.9626 - val_loss: 0.5266 - val_accuracy: 0.8786\n","Epoch 226/500\n","10521/10521 [==============================] - 3s 297us/sample - loss: 0.3435 - accuracy: 0.9626 - val_loss: 0.5258 - val_accuracy: 0.8786\n","Epoch 227/500\n","10521/10521 [==============================] - 3s 296us/sample - loss: 0.3424 - accuracy: 0.9626 - val_loss: 0.5249 - val_accuracy: 0.8786\n","Epoch 228/500\n","10521/10521 [==============================] - 3s 301us/sample - loss: 0.3413 - accuracy: 0.9628 - val_loss: 0.5241 - val_accuracy: 0.8786\n","Epoch 229/500\n","10521/10521 [==============================] - 3s 298us/sample - loss: 0.3402 - accuracy: 0.9632 - val_loss: 0.5233 - val_accuracy: 0.8795\n","Epoch 230/500\n","10521/10521 [==============================] - 3s 296us/sample - loss: 0.3391 - accuracy: 0.9634 - val_loss: 0.5225 - val_accuracy: 0.8795\n","Epoch 231/500\n","10521/10521 [==============================] - 3s 298us/sample - loss: 0.3380 - accuracy: 0.9635 - val_loss: 0.5217 - val_accuracy: 0.8803\n","Epoch 232/500\n","10521/10521 [==============================] - 3s 297us/sample - loss: 0.3369 - accuracy: 0.9633 - val_loss: 0.5209 - val_accuracy: 0.8803\n","Epoch 233/500\n","10521/10521 [==============================] - 3s 291us/sample - loss: 0.3358 - accuracy: 0.9632 - val_loss: 0.5201 - val_accuracy: 0.8803\n","Epoch 234/500\n","10521/10521 [==============================] - 3s 295us/sample - loss: 0.3348 - accuracy: 0.9632 - val_loss: 0.5194 - val_accuracy: 0.8803\n","Epoch 235/500\n","10521/10521 [==============================] - 3s 296us/sample - loss: 0.3337 - accuracy: 0.9632 - val_loss: 0.5186 - val_accuracy: 0.8821\n","Epoch 236/500\n","10521/10521 [==============================] - 3s 297us/sample - loss: 0.3326 - accuracy: 0.9632 - val_loss: 0.5178 - val_accuracy: 0.8821\n","Epoch 237/500\n","10521/10521 [==============================] - 3s 290us/sample - loss: 0.3316 - accuracy: 0.9633 - val_loss: 0.5170 - val_accuracy: 0.8812\n","Epoch 238/500\n","10521/10521 [==============================] - 3s 299us/sample - loss: 0.3305 - accuracy: 0.9634 - val_loss: 0.5162 - val_accuracy: 0.8812\n","Epoch 239/500\n","10521/10521 [==============================] - 3s 296us/sample - loss: 0.3295 - accuracy: 0.9634 - val_loss: 0.5155 - val_accuracy: 0.8812\n","Epoch 240/500\n","10521/10521 [==============================] - 3s 290us/sample - loss: 0.3284 - accuracy: 0.9635 - val_loss: 0.5147 - val_accuracy: 0.8812\n","Epoch 241/500\n","10521/10521 [==============================] - 3s 299us/sample - loss: 0.3274 - accuracy: 0.9636 - val_loss: 0.5139 - val_accuracy: 0.8803\n","Epoch 242/500\n","10521/10521 [==============================] - 3s 297us/sample - loss: 0.3264 - accuracy: 0.9636 - val_loss: 0.5131 - val_accuracy: 0.8803\n","Epoch 243/500\n","10521/10521 [==============================] - 3s 301us/sample - loss: 0.3254 - accuracy: 0.9638 - val_loss: 0.5124 - val_accuracy: 0.8803\n","Epoch 244/500\n","10521/10521 [==============================] - 3s 297us/sample - loss: 0.3244 - accuracy: 0.9638 - val_loss: 0.5116 - val_accuracy: 0.8803\n","Epoch 245/500\n","10521/10521 [==============================] - 3s 298us/sample - loss: 0.3234 - accuracy: 0.9638 - val_loss: 0.5109 - val_accuracy: 0.8812\n","Epoch 246/500\n","10521/10521 [==============================] - 3s 295us/sample - loss: 0.3223 - accuracy: 0.9638 - val_loss: 0.5101 - val_accuracy: 0.8812\n","Epoch 247/500\n","10521/10521 [==============================] - 3s 302us/sample - loss: 0.3214 - accuracy: 0.9638 - val_loss: 0.5094 - val_accuracy: 0.8812\n","Epoch 248/500\n","10521/10521 [==============================] - 3s 296us/sample - loss: 0.3204 - accuracy: 0.9638 - val_loss: 0.5086 - val_accuracy: 0.8803\n","Epoch 249/500\n","10521/10521 [==============================] - 3s 296us/sample - loss: 0.3194 - accuracy: 0.9638 - val_loss: 0.5079 - val_accuracy: 0.8803\n","Epoch 250/500\n","10521/10521 [==============================] - 3s 304us/sample - loss: 0.3184 - accuracy: 0.9638 - val_loss: 0.5071 - val_accuracy: 0.8803\n","Epoch 251/500\n","10521/10521 [==============================] - 3s 326us/sample - loss: 0.3174 - accuracy: 0.9638 - val_loss: 0.5064 - val_accuracy: 0.8803\n","Epoch 252/500\n","10521/10521 [==============================] - 3s 330us/sample - loss: 0.3165 - accuracy: 0.9641 - val_loss: 0.5057 - val_accuracy: 0.8803\n","Epoch 253/500\n","10521/10521 [==============================] - 3s 325us/sample - loss: 0.3155 - accuracy: 0.9641 - val_loss: 0.5050 - val_accuracy: 0.8803\n","Epoch 254/500\n","10521/10521 [==============================] - 3s 295us/sample - loss: 0.3145 - accuracy: 0.9641 - val_loss: 0.5043 - val_accuracy: 0.8803\n","Epoch 255/500\n","10521/10521 [==============================] - 3s 301us/sample - loss: 0.3136 - accuracy: 0.9640 - val_loss: 0.5036 - val_accuracy: 0.8803\n","Epoch 256/500\n","10521/10521 [==============================] - 3s 297us/sample - loss: 0.3126 - accuracy: 0.9640 - val_loss: 0.5029 - val_accuracy: 0.8812\n","Epoch 257/500\n","10521/10521 [==============================] - 3s 299us/sample - loss: 0.3117 - accuracy: 0.9641 - val_loss: 0.5022 - val_accuracy: 0.8821\n","Epoch 258/500\n","10521/10521 [==============================] - 3s 303us/sample - loss: 0.3107 - accuracy: 0.9642 - val_loss: 0.5015 - val_accuracy: 0.8821\n","Epoch 259/500\n","10521/10521 [==============================] - 3s 291us/sample - loss: 0.3098 - accuracy: 0.9641 - val_loss: 0.5008 - val_accuracy: 0.8812\n","Epoch 260/500\n","10521/10521 [==============================] - 3s 301us/sample - loss: 0.3089 - accuracy: 0.9641 - val_loss: 0.5001 - val_accuracy: 0.8803\n","Epoch 261/500\n","10521/10521 [==============================] - 3s 290us/sample - loss: 0.3080 - accuracy: 0.9641 - val_loss: 0.4994 - val_accuracy: 0.8803\n","Epoch 262/500\n","10521/10521 [==============================] - 3s 287us/sample - loss: 0.3070 - accuracy: 0.9642 - val_loss: 0.4987 - val_accuracy: 0.8803\n","Epoch 263/500\n","10521/10521 [==============================] - 3s 294us/sample - loss: 0.3061 - accuracy: 0.9642 - val_loss: 0.4980 - val_accuracy: 0.8803\n","Epoch 264/500\n","10521/10521 [==============================] - 3s 294us/sample - loss: 0.3052 - accuracy: 0.9645 - val_loss: 0.4973 - val_accuracy: 0.8803\n","Epoch 265/500\n","10521/10521 [==============================] - 3s 291us/sample - loss: 0.3043 - accuracy: 0.9645 - val_loss: 0.4966 - val_accuracy: 0.8803\n","Epoch 266/500\n","10521/10521 [==============================] - 3s 297us/sample - loss: 0.3034 - accuracy: 0.9645 - val_loss: 0.4959 - val_accuracy: 0.8803\n","Epoch 267/500\n","10521/10521 [==============================] - 3s 300us/sample - loss: 0.3025 - accuracy: 0.9645 - val_loss: 0.4952 - val_accuracy: 0.8803\n","Epoch 268/500\n","10521/10521 [==============================] - 3s 301us/sample - loss: 0.3016 - accuracy: 0.9645 - val_loss: 0.4946 - val_accuracy: 0.8803\n","Epoch 269/500\n","10521/10521 [==============================] - 3s 288us/sample - loss: 0.3007 - accuracy: 0.9647 - val_loss: 0.4939 - val_accuracy: 0.8803\n","Epoch 270/500\n","10521/10521 [==============================] - 3s 292us/sample - loss: 0.2999 - accuracy: 0.9649 - val_loss: 0.4932 - val_accuracy: 0.8803\n","Epoch 271/500\n","10521/10521 [==============================] - 3s 294us/sample - loss: 0.2990 - accuracy: 0.9649 - val_loss: 0.4926 - val_accuracy: 0.8803\n","Epoch 272/500\n","10521/10521 [==============================] - 3s 297us/sample - loss: 0.2981 - accuracy: 0.9648 - val_loss: 0.4919 - val_accuracy: 0.8803\n","Epoch 273/500\n","10521/10521 [==============================] - 3s 293us/sample - loss: 0.2973 - accuracy: 0.9647 - val_loss: 0.4913 - val_accuracy: 0.8795\n","Epoch 274/500\n","10521/10521 [==============================] - 3s 296us/sample - loss: 0.2964 - accuracy: 0.9648 - val_loss: 0.4906 - val_accuracy: 0.8795\n","Epoch 275/500\n","10521/10521 [==============================] - 3s 293us/sample - loss: 0.2956 - accuracy: 0.9649 - val_loss: 0.4900 - val_accuracy: 0.8795\n","Epoch 276/500\n","10521/10521 [==============================] - 3s 302us/sample - loss: 0.2947 - accuracy: 0.9648 - val_loss: 0.4893 - val_accuracy: 0.8795\n","Epoch 277/500\n","10521/10521 [==============================] - 3s 302us/sample - loss: 0.2939 - accuracy: 0.9649 - val_loss: 0.4887 - val_accuracy: 0.8795\n","Epoch 278/500\n","10521/10521 [==============================] - 3s 304us/sample - loss: 0.2930 - accuracy: 0.9648 - val_loss: 0.4881 - val_accuracy: 0.8795\n","Epoch 279/500\n","10521/10521 [==============================] - 3s 322us/sample - loss: 0.2922 - accuracy: 0.9648 - val_loss: 0.4874 - val_accuracy: 0.8795\n","Epoch 280/500\n","10521/10521 [==============================] - 3s 328us/sample - loss: 0.2913 - accuracy: 0.9647 - val_loss: 0.4868 - val_accuracy: 0.8786\n","Epoch 281/500\n","10521/10521 [==============================] - 3s 300us/sample - loss: 0.2905 - accuracy: 0.9647 - val_loss: 0.4862 - val_accuracy: 0.8795\n","Epoch 282/500\n","10521/10521 [==============================] - 3s 297us/sample - loss: 0.2897 - accuracy: 0.9648 - val_loss: 0.4855 - val_accuracy: 0.8795\n","Epoch 283/500\n","10521/10521 [==============================] - 3s 300us/sample - loss: 0.2889 - accuracy: 0.9648 - val_loss: 0.4849 - val_accuracy: 0.8795\n","Epoch 284/500\n","10521/10521 [==============================] - 3s 311us/sample - loss: 0.2880 - accuracy: 0.9647 - val_loss: 0.4843 - val_accuracy: 0.8795\n","Epoch 285/500\n","10521/10521 [==============================] - 3s 302us/sample - loss: 0.2872 - accuracy: 0.9648 - val_loss: 0.4837 - val_accuracy: 0.8795\n","Epoch 286/500\n","10521/10521 [==============================] - 3s 301us/sample - loss: 0.2864 - accuracy: 0.9650 - val_loss: 0.4831 - val_accuracy: 0.8795\n","Epoch 287/500\n","10521/10521 [==============================] - 3s 301us/sample - loss: 0.2856 - accuracy: 0.9653 - val_loss: 0.4824 - val_accuracy: 0.8795\n","Epoch 288/500\n","10521/10521 [==============================] - 3s 305us/sample - loss: 0.2848 - accuracy: 0.9652 - val_loss: 0.4818 - val_accuracy: 0.8795\n","Epoch 289/500\n","10521/10521 [==============================] - 3s 306us/sample - loss: 0.2840 - accuracy: 0.9653 - val_loss: 0.4812 - val_accuracy: 0.8795\n","Epoch 290/500\n","10521/10521 [==============================] - 3s 304us/sample - loss: 0.2832 - accuracy: 0.9653 - val_loss: 0.4806 - val_accuracy: 0.8803\n","Epoch 291/500\n","10521/10521 [==============================] - 3s 298us/sample - loss: 0.2824 - accuracy: 0.9655 - val_loss: 0.4801 - val_accuracy: 0.8803\n","Epoch 292/500\n","10521/10521 [==============================] - 3s 304us/sample - loss: 0.2817 - accuracy: 0.9654 - val_loss: 0.4795 - val_accuracy: 0.8803\n","Epoch 293/500\n","10521/10521 [==============================] - 3s 298us/sample - loss: 0.2809 - accuracy: 0.9655 - val_loss: 0.4789 - val_accuracy: 0.8803\n","Epoch 294/500\n","10521/10521 [==============================] - 3s 297us/sample - loss: 0.2801 - accuracy: 0.9654 - val_loss: 0.4783 - val_accuracy: 0.8803\n","Epoch 295/500\n","10521/10521 [==============================] - 3s 300us/sample - loss: 0.2793 - accuracy: 0.9655 - val_loss: 0.4777 - val_accuracy: 0.8803\n","Epoch 296/500\n","10521/10521 [==============================] - 3s 301us/sample - loss: 0.2785 - accuracy: 0.9655 - val_loss: 0.4771 - val_accuracy: 0.8821\n","Epoch 297/500\n","10521/10521 [==============================] - 3s 301us/sample - loss: 0.2778 - accuracy: 0.9655 - val_loss: 0.4765 - val_accuracy: 0.8821\n","Epoch 298/500\n","10521/10521 [==============================] - 3s 298us/sample - loss: 0.2770 - accuracy: 0.9654 - val_loss: 0.4760 - val_accuracy: 0.8821\n","Epoch 299/500\n","10521/10521 [==============================] - 3s 303us/sample - loss: 0.2763 - accuracy: 0.9654 - val_loss: 0.4754 - val_accuracy: 0.8821\n","Epoch 300/500\n","10521/10521 [==============================] - 3s 300us/sample - loss: 0.2755 - accuracy: 0.9654 - val_loss: 0.4748 - val_accuracy: 0.8821\n","Epoch 301/500\n","10521/10521 [==============================] - 3s 297us/sample - loss: 0.2748 - accuracy: 0.9654 - val_loss: 0.4743 - val_accuracy: 0.8821\n","Epoch 302/500\n","10521/10521 [==============================] - 3s 289us/sample - loss: 0.2740 - accuracy: 0.9654 - val_loss: 0.4737 - val_accuracy: 0.8821\n","Epoch 303/500\n","10521/10521 [==============================] - 3s 297us/sample - loss: 0.2733 - accuracy: 0.9654 - val_loss: 0.4731 - val_accuracy: 0.8821\n","Epoch 304/500\n","10521/10521 [==============================] - 3s 301us/sample - loss: 0.2725 - accuracy: 0.9654 - val_loss: 0.4725 - val_accuracy: 0.8821\n","Epoch 305/500\n","10521/10521 [==============================] - 3s 298us/sample - loss: 0.2718 - accuracy: 0.9654 - val_loss: 0.4720 - val_accuracy: 0.8821\n","Epoch 306/500\n","10521/10521 [==============================] - 3s 297us/sample - loss: 0.2711 - accuracy: 0.9654 - val_loss: 0.4714 - val_accuracy: 0.8821\n","Epoch 307/500\n","10521/10521 [==============================] - 3s 291us/sample - loss: 0.2703 - accuracy: 0.9654 - val_loss: 0.4708 - val_accuracy: 0.8821\n","Epoch 308/500\n","10521/10521 [==============================] - 3s 291us/sample - loss: 0.2696 - accuracy: 0.9653 - val_loss: 0.4703 - val_accuracy: 0.8812\n","Epoch 309/500\n","10521/10521 [==============================] - 3s 298us/sample - loss: 0.2689 - accuracy: 0.9655 - val_loss: 0.4697 - val_accuracy: 0.8821\n","Epoch 310/500\n","10521/10521 [==============================] - 3s 293us/sample - loss: 0.2682 - accuracy: 0.9655 - val_loss: 0.4692 - val_accuracy: 0.8821\n","Epoch 311/500\n","10521/10521 [==============================] - 3s 300us/sample - loss: 0.2674 - accuracy: 0.9656 - val_loss: 0.4686 - val_accuracy: 0.8821\n","Epoch 312/500\n","10521/10521 [==============================] - 3s 304us/sample - loss: 0.2667 - accuracy: 0.9655 - val_loss: 0.4680 - val_accuracy: 0.8829\n","Epoch 313/500\n","10521/10521 [==============================] - 3s 298us/sample - loss: 0.2660 - accuracy: 0.9657 - val_loss: 0.4675 - val_accuracy: 0.8829\n","Epoch 314/500\n","10521/10521 [==============================] - 3s 294us/sample - loss: 0.2653 - accuracy: 0.9657 - val_loss: 0.4670 - val_accuracy: 0.8829\n","Epoch 315/500\n","10521/10521 [==============================] - 3s 295us/sample - loss: 0.2646 - accuracy: 0.9657 - val_loss: 0.4664 - val_accuracy: 0.8829\n","Epoch 316/500\n","10521/10521 [==============================] - 3s 297us/sample - loss: 0.2639 - accuracy: 0.9658 - val_loss: 0.4659 - val_accuracy: 0.8829\n","Epoch 317/500\n","10521/10521 [==============================] - 3s 300us/sample - loss: 0.2632 - accuracy: 0.9658 - val_loss: 0.4654 - val_accuracy: 0.8829\n","Epoch 318/500\n","10521/10521 [==============================] - 3s 293us/sample - loss: 0.2625 - accuracy: 0.9658 - val_loss: 0.4648 - val_accuracy: 0.8829\n","Epoch 319/500\n","10521/10521 [==============================] - 3s 304us/sample - loss: 0.2618 - accuracy: 0.9658 - val_loss: 0.4643 - val_accuracy: 0.8829\n","Epoch 320/500\n","10521/10521 [==============================] - 3s 299us/sample - loss: 0.2612 - accuracy: 0.9659 - val_loss: 0.4638 - val_accuracy: 0.8829\n","Epoch 321/500\n","10521/10521 [==============================] - 3s 294us/sample - loss: 0.2605 - accuracy: 0.9661 - val_loss: 0.4632 - val_accuracy: 0.8829\n","Epoch 322/500\n","10521/10521 [==============================] - 3s 304us/sample - loss: 0.2598 - accuracy: 0.9660 - val_loss: 0.4627 - val_accuracy: 0.8829\n","Epoch 323/500\n","10521/10521 [==============================] - 3s 304us/sample - loss: 0.2591 - accuracy: 0.9661 - val_loss: 0.4622 - val_accuracy: 0.8829\n","Epoch 324/500\n","10521/10521 [==============================] - 3s 300us/sample - loss: 0.2585 - accuracy: 0.9660 - val_loss: 0.4617 - val_accuracy: 0.8829\n","Epoch 325/500\n","10521/10521 [==============================] - 3s 293us/sample - loss: 0.2578 - accuracy: 0.9662 - val_loss: 0.4612 - val_accuracy: 0.8829\n","Epoch 326/500\n","10521/10521 [==============================] - 3s 297us/sample - loss: 0.2571 - accuracy: 0.9663 - val_loss: 0.4606 - val_accuracy: 0.8838\n","Epoch 327/500\n","10521/10521 [==============================] - 3s 302us/sample - loss: 0.2565 - accuracy: 0.9662 - val_loss: 0.4601 - val_accuracy: 0.8838\n","Epoch 328/500\n","10521/10521 [==============================] - 3s 299us/sample - loss: 0.2558 - accuracy: 0.9663 - val_loss: 0.4596 - val_accuracy: 0.8838\n","Epoch 329/500\n","10521/10521 [==============================] - 3s 305us/sample - loss: 0.2552 - accuracy: 0.9659 - val_loss: 0.4591 - val_accuracy: 0.8838\n","Epoch 330/500\n","10521/10521 [==============================] - 3s 295us/sample - loss: 0.2545 - accuracy: 0.9659 - val_loss: 0.4586 - val_accuracy: 0.8838\n","Epoch 331/500\n","10521/10521 [==============================] - 3s 298us/sample - loss: 0.2539 - accuracy: 0.9659 - val_loss: 0.4581 - val_accuracy: 0.8838\n","Epoch 332/500\n","10521/10521 [==============================] - 3s 299us/sample - loss: 0.2532 - accuracy: 0.9659 - val_loss: 0.4576 - val_accuracy: 0.8829\n","Epoch 333/500\n","10521/10521 [==============================] - 3s 306us/sample - loss: 0.2526 - accuracy: 0.9660 - val_loss: 0.4570 - val_accuracy: 0.8846\n","Epoch 334/500\n","10521/10521 [==============================] - 3s 305us/sample - loss: 0.2519 - accuracy: 0.9660 - val_loss: 0.4565 - val_accuracy: 0.8846\n","Epoch 335/500\n","10521/10521 [==============================] - 3s 298us/sample - loss: 0.2513 - accuracy: 0.9660 - val_loss: 0.4560 - val_accuracy: 0.8846\n","Epoch 336/500\n","10521/10521 [==============================] - 3s 298us/sample - loss: 0.2506 - accuracy: 0.9659 - val_loss: 0.4556 - val_accuracy: 0.8846\n","Epoch 337/500\n","10521/10521 [==============================] - 3s 302us/sample - loss: 0.2500 - accuracy: 0.9660 - val_loss: 0.4551 - val_accuracy: 0.8846\n","Epoch 338/500\n","10521/10521 [==============================] - 3s 305us/sample - loss: 0.2494 - accuracy: 0.9661 - val_loss: 0.4546 - val_accuracy: 0.8846\n","Epoch 339/500\n","10521/10521 [==============================] - 3s 302us/sample - loss: 0.2487 - accuracy: 0.9660 - val_loss: 0.4541 - val_accuracy: 0.8846\n","Epoch 340/500\n","10521/10521 [==============================] - 3s 299us/sample - loss: 0.2481 - accuracy: 0.9661 - val_loss: 0.4536 - val_accuracy: 0.8846\n","Epoch 341/500\n","10521/10521 [==============================] - 3s 308us/sample - loss: 0.2475 - accuracy: 0.9660 - val_loss: 0.4531 - val_accuracy: 0.8846\n","Epoch 342/500\n","10521/10521 [==============================] - 3s 299us/sample - loss: 0.2469 - accuracy: 0.9660 - val_loss: 0.4526 - val_accuracy: 0.8846\n","Epoch 343/500\n","10521/10521 [==============================] - 3s 299us/sample - loss: 0.2463 - accuracy: 0.9659 - val_loss: 0.4521 - val_accuracy: 0.8846\n","Epoch 344/500\n","10521/10521 [==============================] - 3s 300us/sample - loss: 0.2457 - accuracy: 0.9661 - val_loss: 0.4517 - val_accuracy: 0.8846\n","Epoch 345/500\n","10521/10521 [==============================] - 3s 299us/sample - loss: 0.2451 - accuracy: 0.9661 - val_loss: 0.4512 - val_accuracy: 0.8846\n","Epoch 346/500\n","10521/10521 [==============================] - 3s 295us/sample - loss: 0.2444 - accuracy: 0.9661 - val_loss: 0.4507 - val_accuracy: 0.8846\n","Epoch 347/500\n","10521/10521 [==============================] - 3s 300us/sample - loss: 0.2438 - accuracy: 0.9661 - val_loss: 0.4503 - val_accuracy: 0.8846\n","Epoch 348/500\n","10521/10521 [==============================] - 3s 301us/sample - loss: 0.2432 - accuracy: 0.9661 - val_loss: 0.4498 - val_accuracy: 0.8846\n","Epoch 349/500\n","10521/10521 [==============================] - 3s 319us/sample - loss: 0.2426 - accuracy: 0.9661 - val_loss: 0.4493 - val_accuracy: 0.8846\n","Epoch 350/500\n","10521/10521 [==============================] - 3s 326us/sample - loss: 0.2420 - accuracy: 0.9661 - val_loss: 0.4488 - val_accuracy: 0.8846\n","Epoch 351/500\n","10521/10521 [==============================] - 3s 330us/sample - loss: 0.2415 - accuracy: 0.9662 - val_loss: 0.4484 - val_accuracy: 0.8846\n","Epoch 352/500\n","10521/10521 [==============================] - 3s 308us/sample - loss: 0.2409 - accuracy: 0.9661 - val_loss: 0.4479 - val_accuracy: 0.8846\n","Epoch 353/500\n","10521/10521 [==============================] - 3s 308us/sample - loss: 0.2403 - accuracy: 0.9661 - val_loss: 0.4474 - val_accuracy: 0.8838\n","Epoch 354/500\n","10521/10521 [==============================] - 3s 305us/sample - loss: 0.2397 - accuracy: 0.9661 - val_loss: 0.4470 - val_accuracy: 0.8838\n","Epoch 355/500\n","10521/10521 [==============================] - 3s 301us/sample - loss: 0.2391 - accuracy: 0.9660 - val_loss: 0.4465 - val_accuracy: 0.8838\n","Epoch 356/500\n","10521/10521 [==============================] - 3s 303us/sample - loss: 0.2385 - accuracy: 0.9661 - val_loss: 0.4460 - val_accuracy: 0.8838\n","Epoch 357/500\n","10521/10521 [==============================] - 3s 304us/sample - loss: 0.2380 - accuracy: 0.9660 - val_loss: 0.4456 - val_accuracy: 0.8838\n","Epoch 358/500\n","10521/10521 [==============================] - 3s 300us/sample - loss: 0.2374 - accuracy: 0.9661 - val_loss: 0.4451 - val_accuracy: 0.8838\n","Epoch 359/500\n","10521/10521 [==============================] - 3s 299us/sample - loss: 0.2368 - accuracy: 0.9660 - val_loss: 0.4447 - val_accuracy: 0.8838\n","Epoch 360/500\n","10521/10521 [==============================] - 3s 303us/sample - loss: 0.2362 - accuracy: 0.9660 - val_loss: 0.4442 - val_accuracy: 0.8838\n","Epoch 361/500\n","10521/10521 [==============================] - 3s 298us/sample - loss: 0.2357 - accuracy: 0.9661 - val_loss: 0.4438 - val_accuracy: 0.8838\n","Epoch 362/500\n","10521/10521 [==============================] - 3s 304us/sample - loss: 0.2351 - accuracy: 0.9661 - val_loss: 0.4433 - val_accuracy: 0.8846\n","Epoch 363/500\n","10521/10521 [==============================] - 3s 303us/sample - loss: 0.2345 - accuracy: 0.9659 - val_loss: 0.4429 - val_accuracy: 0.8846\n","Epoch 364/500\n","10521/10521 [==============================] - 3s 300us/sample - loss: 0.2340 - accuracy: 0.9660 - val_loss: 0.4424 - val_accuracy: 0.8855\n","Epoch 365/500\n","10521/10521 [==============================] - 3s 298us/sample - loss: 0.2334 - accuracy: 0.9660 - val_loss: 0.4420 - val_accuracy: 0.8855\n","Epoch 366/500\n","10521/10521 [==============================] - 3s 297us/sample - loss: 0.2328 - accuracy: 0.9660 - val_loss: 0.4416 - val_accuracy: 0.8855\n","Epoch 367/500\n","10521/10521 [==============================] - 3s 295us/sample - loss: 0.2323 - accuracy: 0.9661 - val_loss: 0.4411 - val_accuracy: 0.8855\n","Epoch 368/500\n","10521/10521 [==============================] - 3s 296us/sample - loss: 0.2317 - accuracy: 0.9661 - val_loss: 0.4407 - val_accuracy: 0.8855\n","Epoch 369/500\n","10521/10521 [==============================] - 3s 298us/sample - loss: 0.2312 - accuracy: 0.9661 - val_loss: 0.4402 - val_accuracy: 0.8855\n","Epoch 370/500\n","10521/10521 [==============================] - 3s 298us/sample - loss: 0.2306 - accuracy: 0.9661 - val_loss: 0.4398 - val_accuracy: 0.8855\n","Epoch 371/500\n","10521/10521 [==============================] - 3s 303us/sample - loss: 0.2301 - accuracy: 0.9660 - val_loss: 0.4394 - val_accuracy: 0.8855\n","Epoch 372/500\n","10521/10521 [==============================] - 3s 301us/sample - loss: 0.2296 - accuracy: 0.9660 - val_loss: 0.4389 - val_accuracy: 0.8863\n","Epoch 373/500\n","10521/10521 [==============================] - 3s 297us/sample - loss: 0.2290 - accuracy: 0.9659 - val_loss: 0.4385 - val_accuracy: 0.8855\n","Epoch 374/500\n","10521/10521 [==============================] - 4s 356us/sample - loss: 0.2285 - accuracy: 0.9660 - val_loss: 0.4381 - val_accuracy: 0.8863\n","Epoch 375/500\n","10521/10521 [==============================] - 3s 308us/sample - loss: 0.2279 - accuracy: 0.9660 - val_loss: 0.4376 - val_accuracy: 0.8863\n","Epoch 376/500\n","10521/10521 [==============================] - 3s 328us/sample - loss: 0.2274 - accuracy: 0.9659 - val_loss: 0.4372 - val_accuracy: 0.8846\n","Epoch 377/500\n","10521/10521 [==============================] - 3s 317us/sample - loss: 0.2269 - accuracy: 0.9659 - val_loss: 0.4368 - val_accuracy: 0.8855\n","Epoch 378/500\n","10521/10521 [==============================] - 3s 298us/sample - loss: 0.2263 - accuracy: 0.9659 - val_loss: 0.4364 - val_accuracy: 0.8855\n","Epoch 379/500\n","10521/10521 [==============================] - 3s 299us/sample - loss: 0.2258 - accuracy: 0.9659 - val_loss: 0.4360 - val_accuracy: 0.8855\n","Epoch 380/500\n","10521/10521 [==============================] - 3s 299us/sample - loss: 0.2253 - accuracy: 0.9659 - val_loss: 0.4356 - val_accuracy: 0.8855\n","Epoch 381/500\n","10521/10521 [==============================] - 3s 295us/sample - loss: 0.2248 - accuracy: 0.9660 - val_loss: 0.4351 - val_accuracy: 0.8855\n","Epoch 382/500\n","10521/10521 [==============================] - 3s 291us/sample - loss: 0.2243 - accuracy: 0.9661 - val_loss: 0.4347 - val_accuracy: 0.8855\n","Epoch 383/500\n","10521/10521 [==============================] - 3s 294us/sample - loss: 0.2237 - accuracy: 0.9661 - val_loss: 0.4343 - val_accuracy: 0.8855\n","Epoch 384/500\n","10521/10521 [==============================] - 3s 295us/sample - loss: 0.2232 - accuracy: 0.9662 - val_loss: 0.4339 - val_accuracy: 0.8855\n","Epoch 385/500\n","10521/10521 [==============================] - 3s 298us/sample - loss: 0.2227 - accuracy: 0.9664 - val_loss: 0.4335 - val_accuracy: 0.8855\n","Epoch 386/500\n","10521/10521 [==============================] - 3s 301us/sample - loss: 0.2222 - accuracy: 0.9664 - val_loss: 0.4331 - val_accuracy: 0.8855\n","Epoch 387/500\n","10521/10521 [==============================] - 3s 305us/sample - loss: 0.2217 - accuracy: 0.9664 - val_loss: 0.4327 - val_accuracy: 0.8855\n","Epoch 388/500\n","10521/10521 [==============================] - 3s 307us/sample - loss: 0.2212 - accuracy: 0.9664 - val_loss: 0.4323 - val_accuracy: 0.8855\n","Epoch 389/500\n","10521/10521 [==============================] - 3s 302us/sample - loss: 0.2207 - accuracy: 0.9664 - val_loss: 0.4319 - val_accuracy: 0.8855\n","Epoch 390/500\n","10521/10521 [==============================] - 3s 302us/sample - loss: 0.2202 - accuracy: 0.9664 - val_loss: 0.4314 - val_accuracy: 0.8855\n","Epoch 391/500\n","10521/10521 [==============================] - 3s 299us/sample - loss: 0.2197 - accuracy: 0.9664 - val_loss: 0.4311 - val_accuracy: 0.8855\n","Epoch 392/500\n","10521/10521 [==============================] - 3s 302us/sample - loss: 0.2192 - accuracy: 0.9664 - val_loss: 0.4307 - val_accuracy: 0.8855\n","Epoch 393/500\n","10521/10521 [==============================] - 3s 298us/sample - loss: 0.2187 - accuracy: 0.9664 - val_loss: 0.4303 - val_accuracy: 0.8855\n","Epoch 394/500\n","10521/10521 [==============================] - 3s 300us/sample - loss: 0.2182 - accuracy: 0.9664 - val_loss: 0.4299 - val_accuracy: 0.8855\n","Epoch 395/500\n","10521/10521 [==============================] - 3s 303us/sample - loss: 0.2177 - accuracy: 0.9664 - val_loss: 0.4295 - val_accuracy: 0.8855\n","Epoch 396/500\n","10521/10521 [==============================] - 3s 295us/sample - loss: 0.2172 - accuracy: 0.9664 - val_loss: 0.4291 - val_accuracy: 0.8863\n","Epoch 397/500\n","10521/10521 [==============================] - 3s 297us/sample - loss: 0.2167 - accuracy: 0.9664 - val_loss: 0.4287 - val_accuracy: 0.8863\n","Epoch 398/500\n","10521/10521 [==============================] - 3s 296us/sample - loss: 0.2162 - accuracy: 0.9664 - val_loss: 0.4283 - val_accuracy: 0.8863\n","Epoch 399/500\n","10521/10521 [==============================] - 3s 297us/sample - loss: 0.2157 - accuracy: 0.9664 - val_loss: 0.4279 - val_accuracy: 0.8872\n","Epoch 400/500\n","10521/10521 [==============================] - 3s 301us/sample - loss: 0.2153 - accuracy: 0.9664 - val_loss: 0.4275 - val_accuracy: 0.8863\n","Epoch 401/500\n","10521/10521 [==============================] - 3s 294us/sample - loss: 0.2148 - accuracy: 0.9665 - val_loss: 0.4271 - val_accuracy: 0.8863\n","Epoch 402/500\n","10521/10521 [==============================] - 3s 290us/sample - loss: 0.2143 - accuracy: 0.9664 - val_loss: 0.4268 - val_accuracy: 0.8863\n","Epoch 403/500\n","10521/10521 [==============================] - 3s 297us/sample - loss: 0.2138 - accuracy: 0.9664 - val_loss: 0.4264 - val_accuracy: 0.8863\n","Epoch 404/500\n","10521/10521 [==============================] - 3s 295us/sample - loss: 0.2133 - accuracy: 0.9666 - val_loss: 0.4260 - val_accuracy: 0.8863\n","Epoch 405/500\n","10521/10521 [==============================] - 3s 295us/sample - loss: 0.2129 - accuracy: 0.9665 - val_loss: 0.4256 - val_accuracy: 0.8863\n","Epoch 406/500\n","10521/10521 [==============================] - 3s 296us/sample - loss: 0.2124 - accuracy: 0.9665 - val_loss: 0.4252 - val_accuracy: 0.8863\n","Epoch 407/500\n","10521/10521 [==============================] - 3s 296us/sample - loss: 0.2119 - accuracy: 0.9666 - val_loss: 0.4249 - val_accuracy: 0.8863\n","Epoch 408/500\n","10521/10521 [==============================] - 3s 298us/sample - loss: 0.2115 - accuracy: 0.9665 - val_loss: 0.4245 - val_accuracy: 0.8863\n","Epoch 409/500\n","10521/10521 [==============================] - 3s 297us/sample - loss: 0.2110 - accuracy: 0.9666 - val_loss: 0.4241 - val_accuracy: 0.8863\n","Epoch 410/500\n","10521/10521 [==============================] - 3s 296us/sample - loss: 0.2105 - accuracy: 0.9666 - val_loss: 0.4238 - val_accuracy: 0.8863\n","Epoch 411/500\n","10521/10521 [==============================] - 3s 294us/sample - loss: 0.2101 - accuracy: 0.9667 - val_loss: 0.4234 - val_accuracy: 0.8872\n","Epoch 412/500\n","10521/10521 [==============================] - 3s 292us/sample - loss: 0.2096 - accuracy: 0.9666 - val_loss: 0.4230 - val_accuracy: 0.8872\n","Epoch 413/500\n","10521/10521 [==============================] - 3s 293us/sample - loss: 0.2092 - accuracy: 0.9666 - val_loss: 0.4227 - val_accuracy: 0.8872\n","Epoch 414/500\n","10521/10521 [==============================] - 3s 295us/sample - loss: 0.2087 - accuracy: 0.9666 - val_loss: 0.4223 - val_accuracy: 0.8872\n","Epoch 415/500\n","10521/10521 [==============================] - 3s 296us/sample - loss: 0.2082 - accuracy: 0.9666 - val_loss: 0.4219 - val_accuracy: 0.8872\n","Epoch 416/500\n","10521/10521 [==============================] - 3s 301us/sample - loss: 0.2078 - accuracy: 0.9666 - val_loss: 0.4216 - val_accuracy: 0.8872\n","Epoch 417/500\n","10521/10521 [==============================] - 3s 297us/sample - loss: 0.2073 - accuracy: 0.9667 - val_loss: 0.4212 - val_accuracy: 0.8872\n","Epoch 418/500\n","10521/10521 [==============================] - 3s 294us/sample - loss: 0.2069 - accuracy: 0.9667 - val_loss: 0.4208 - val_accuracy: 0.8863\n","Epoch 419/500\n","10521/10521 [==============================] - 3s 294us/sample - loss: 0.2064 - accuracy: 0.9667 - val_loss: 0.4205 - val_accuracy: 0.8863\n","Epoch 420/500\n","10521/10521 [==============================] - 3s 291us/sample - loss: 0.2060 - accuracy: 0.9667 - val_loss: 0.4201 - val_accuracy: 0.8863\n","Epoch 421/500\n","10521/10521 [==============================] - 3s 294us/sample - loss: 0.2056 - accuracy: 0.9668 - val_loss: 0.4197 - val_accuracy: 0.8863\n","Epoch 422/500\n","10521/10521 [==============================] - 3s 296us/sample - loss: 0.2051 - accuracy: 0.9668 - val_loss: 0.4193 - val_accuracy: 0.8863\n","Epoch 423/500\n","10521/10521 [==============================] - 3s 297us/sample - loss: 0.2047 - accuracy: 0.9667 - val_loss: 0.4190 - val_accuracy: 0.8863\n","Epoch 424/500\n","10521/10521 [==============================] - 3s 293us/sample - loss: 0.2042 - accuracy: 0.9667 - val_loss: 0.4186 - val_accuracy: 0.8855\n","Epoch 425/500\n","10521/10521 [==============================] - 3s 296us/sample - loss: 0.2038 - accuracy: 0.9669 - val_loss: 0.4183 - val_accuracy: 0.8855\n","Epoch 426/500\n","10521/10521 [==============================] - 3s 287us/sample - loss: 0.2034 - accuracy: 0.9669 - val_loss: 0.4179 - val_accuracy: 0.8855\n","Epoch 427/500\n","10521/10521 [==============================] - 3s 294us/sample - loss: 0.2029 - accuracy: 0.9670 - val_loss: 0.4176 - val_accuracy: 0.8855\n","Epoch 428/500\n","10521/10521 [==============================] - 3s 295us/sample - loss: 0.2025 - accuracy: 0.9669 - val_loss: 0.4172 - val_accuracy: 0.8855\n","Epoch 429/500\n","10521/10521 [==============================] - 3s 293us/sample - loss: 0.2021 - accuracy: 0.9668 - val_loss: 0.4169 - val_accuracy: 0.8855\n","Epoch 430/500\n","10521/10521 [==============================] - 3s 288us/sample - loss: 0.2016 - accuracy: 0.9668 - val_loss: 0.4166 - val_accuracy: 0.8855\n","Epoch 431/500\n","10521/10521 [==============================] - 3s 289us/sample - loss: 0.2012 - accuracy: 0.9669 - val_loss: 0.4162 - val_accuracy: 0.8846\n","Epoch 432/500\n","10521/10521 [==============================] - 3s 291us/sample - loss: 0.2008 - accuracy: 0.9670 - val_loss: 0.4159 - val_accuracy: 0.8846\n","Epoch 433/500\n","10521/10521 [==============================] - 3s 291us/sample - loss: 0.2003 - accuracy: 0.9669 - val_loss: 0.4155 - val_accuracy: 0.8846\n","Epoch 434/500\n","10521/10521 [==============================] - 3s 293us/sample - loss: 0.1999 - accuracy: 0.9670 - val_loss: 0.4152 - val_accuracy: 0.8846\n","Epoch 435/500\n","10521/10521 [==============================] - 3s 292us/sample - loss: 0.1995 - accuracy: 0.9670 - val_loss: 0.4148 - val_accuracy: 0.8846\n","Epoch 436/500\n","10521/10521 [==============================] - 3s 298us/sample - loss: 0.1991 - accuracy: 0.9671 - val_loss: 0.4145 - val_accuracy: 0.8846\n","Epoch 437/500\n","10521/10521 [==============================] - 3s 295us/sample - loss: 0.1987 - accuracy: 0.9671 - val_loss: 0.4141 - val_accuracy: 0.8846\n","Epoch 438/500\n","10521/10521 [==============================] - 3s 297us/sample - loss: 0.1982 - accuracy: 0.9671 - val_loss: 0.4138 - val_accuracy: 0.8846\n","Epoch 439/500\n","10521/10521 [==============================] - 3s 298us/sample - loss: 0.1978 - accuracy: 0.9671 - val_loss: 0.4135 - val_accuracy: 0.8846\n","Epoch 440/500\n","10521/10521 [==============================] - 3s 296us/sample - loss: 0.1974 - accuracy: 0.9671 - val_loss: 0.4131 - val_accuracy: 0.8846\n","Epoch 441/500\n","10521/10521 [==============================] - 3s 290us/sample - loss: 0.1970 - accuracy: 0.9672 - val_loss: 0.4128 - val_accuracy: 0.8846\n","Epoch 442/500\n","10521/10521 [==============================] - 3s 289us/sample - loss: 0.1966 - accuracy: 0.9671 - val_loss: 0.4125 - val_accuracy: 0.8855\n","Epoch 443/500\n","10521/10521 [==============================] - 3s 295us/sample - loss: 0.1962 - accuracy: 0.9672 - val_loss: 0.4122 - val_accuracy: 0.8855\n","Epoch 444/500\n","10521/10521 [==============================] - 3s 292us/sample - loss: 0.1958 - accuracy: 0.9671 - val_loss: 0.4118 - val_accuracy: 0.8855\n","Epoch 445/500\n","10521/10521 [==============================] - 3s 293us/sample - loss: 0.1954 - accuracy: 0.9670 - val_loss: 0.4115 - val_accuracy: 0.8855\n","Epoch 446/500\n","10521/10521 [==============================] - 3s 289us/sample - loss: 0.1950 - accuracy: 0.9670 - val_loss: 0.4112 - val_accuracy: 0.8855\n","Epoch 447/500\n","10521/10521 [==============================] - 3s 291us/sample - loss: 0.1946 - accuracy: 0.9671 - val_loss: 0.4108 - val_accuracy: 0.8855\n","Epoch 448/500\n","10521/10521 [==============================] - 3s 320us/sample - loss: 0.1942 - accuracy: 0.9670 - val_loss: 0.4105 - val_accuracy: 0.8855\n","Epoch 449/500\n","10521/10521 [==============================] - 3s 329us/sample - loss: 0.1938 - accuracy: 0.9671 - val_loss: 0.4102 - val_accuracy: 0.8855\n","Epoch 450/500\n","10521/10521 [==============================] - 3s 327us/sample - loss: 0.1934 - accuracy: 0.9671 - val_loss: 0.4099 - val_accuracy: 0.8855\n","Epoch 451/500\n","10521/10521 [==============================] - 3s 299us/sample - loss: 0.1930 - accuracy: 0.9671 - val_loss: 0.4095 - val_accuracy: 0.8846\n","Epoch 452/500\n","10521/10521 [==============================] - 3s 291us/sample - loss: 0.1926 - accuracy: 0.9671 - val_loss: 0.4092 - val_accuracy: 0.8846\n","Epoch 453/500\n","10521/10521 [==============================] - 3s 292us/sample - loss: 0.1922 - accuracy: 0.9671 - val_loss: 0.4089 - val_accuracy: 0.8846\n","Epoch 454/500\n","10521/10521 [==============================] - 3s 298us/sample - loss: 0.1918 - accuracy: 0.9671 - val_loss: 0.4086 - val_accuracy: 0.8855\n","Epoch 455/500\n","10521/10521 [==============================] - 3s 293us/sample - loss: 0.1914 - accuracy: 0.9671 - val_loss: 0.4083 - val_accuracy: 0.8855\n","Epoch 456/500\n","10521/10521 [==============================] - 3s 294us/sample - loss: 0.1910 - accuracy: 0.9670 - val_loss: 0.4079 - val_accuracy: 0.8855\n","Epoch 457/500\n","10521/10521 [==============================] - 3s 293us/sample - loss: 0.1906 - accuracy: 0.9670 - val_loss: 0.4076 - val_accuracy: 0.8855\n","Epoch 458/500\n","10521/10521 [==============================] - 3s 293us/sample - loss: 0.1902 - accuracy: 0.9669 - val_loss: 0.4073 - val_accuracy: 0.8855\n","Epoch 459/500\n","10521/10521 [==============================] - 3s 297us/sample - loss: 0.1898 - accuracy: 0.9668 - val_loss: 0.4070 - val_accuracy: 0.8863\n","Epoch 460/500\n","10521/10521 [==============================] - 3s 293us/sample - loss: 0.1894 - accuracy: 0.9669 - val_loss: 0.4067 - val_accuracy: 0.8863\n","Epoch 461/500\n","10521/10521 [==============================] - 3s 293us/sample - loss: 0.1891 - accuracy: 0.9669 - val_loss: 0.4064 - val_accuracy: 0.8863\n","Epoch 462/500\n","10521/10521 [==============================] - 3s 293us/sample - loss: 0.1887 - accuracy: 0.9669 - val_loss: 0.4061 - val_accuracy: 0.8863\n","Epoch 463/500\n","10521/10521 [==============================] - 3s 293us/sample - loss: 0.1883 - accuracy: 0.9669 - val_loss: 0.4058 - val_accuracy: 0.8863\n","Epoch 464/500\n","10521/10521 [==============================] - 3s 294us/sample - loss: 0.1879 - accuracy: 0.9668 - val_loss: 0.4055 - val_accuracy: 0.8863\n","Epoch 465/500\n","10521/10521 [==============================] - 3s 294us/sample - loss: 0.1876 - accuracy: 0.9668 - val_loss: 0.4051 - val_accuracy: 0.8863\n","Epoch 466/500\n","10521/10521 [==============================] - 3s 293us/sample - loss: 0.1872 - accuracy: 0.9668 - val_loss: 0.4048 - val_accuracy: 0.8863\n","Epoch 467/500\n","10521/10521 [==============================] - 3s 293us/sample - loss: 0.1868 - accuracy: 0.9668 - val_loss: 0.4045 - val_accuracy: 0.8872\n","Epoch 468/500\n","10521/10521 [==============================] - 3s 294us/sample - loss: 0.1864 - accuracy: 0.9668 - val_loss: 0.4042 - val_accuracy: 0.8872\n","Epoch 469/500\n","10521/10521 [==============================] - 3s 294us/sample - loss: 0.1861 - accuracy: 0.9668 - val_loss: 0.4039 - val_accuracy: 0.8872\n","Epoch 470/500\n","10521/10521 [==============================] - 3s 295us/sample - loss: 0.1857 - accuracy: 0.9668 - val_loss: 0.4036 - val_accuracy: 0.8863\n","Epoch 471/500\n","10521/10521 [==============================] - 3s 291us/sample - loss: 0.1853 - accuracy: 0.9669 - val_loss: 0.4033 - val_accuracy: 0.8872\n","Epoch 472/500\n","10521/10521 [==============================] - 3s 299us/sample - loss: 0.1850 - accuracy: 0.9669 - val_loss: 0.4030 - val_accuracy: 0.8872\n","Epoch 473/500\n","10521/10521 [==============================] - 3s 306us/sample - loss: 0.1846 - accuracy: 0.9669 - val_loss: 0.4027 - val_accuracy: 0.8872\n","Epoch 474/500\n","10521/10521 [==============================] - 3s 327us/sample - loss: 0.1842 - accuracy: 0.9669 - val_loss: 0.4025 - val_accuracy: 0.8872\n","Epoch 475/500\n","10521/10521 [==============================] - 3s 313us/sample - loss: 0.1839 - accuracy: 0.9669 - val_loss: 0.4022 - val_accuracy: 0.8872\n","Epoch 476/500\n","10521/10521 [==============================] - 3s 294us/sample - loss: 0.1835 - accuracy: 0.9668 - val_loss: 0.4019 - val_accuracy: 0.8872\n","Epoch 477/500\n","10521/10521 [==============================] - 3s 292us/sample - loss: 0.1831 - accuracy: 0.9669 - val_loss: 0.4016 - val_accuracy: 0.8872\n","Epoch 478/500\n","10521/10521 [==============================] - 3s 299us/sample - loss: 0.1828 - accuracy: 0.9668 - val_loss: 0.4013 - val_accuracy: 0.8872\n","Epoch 479/500\n","10521/10521 [==============================] - 3s 294us/sample - loss: 0.1824 - accuracy: 0.9668 - val_loss: 0.4010 - val_accuracy: 0.8872\n","Epoch 480/500\n","10521/10521 [==============================] - 3s 293us/sample - loss: 0.1821 - accuracy: 0.9668 - val_loss: 0.4007 - val_accuracy: 0.8872\n","Epoch 481/500\n","10521/10521 [==============================] - 3s 295us/sample - loss: 0.1817 - accuracy: 0.9668 - val_loss: 0.4004 - val_accuracy: 0.8872\n","Epoch 482/500\n","10521/10521 [==============================] - 3s 293us/sample - loss: 0.1814 - accuracy: 0.9667 - val_loss: 0.4001 - val_accuracy: 0.8872\n","Epoch 483/500\n","10521/10521 [==============================] - 3s 290us/sample - loss: 0.1810 - accuracy: 0.9668 - val_loss: 0.3998 - val_accuracy: 0.8872\n","Epoch 484/500\n","10521/10521 [==============================] - 3s 292us/sample - loss: 0.1806 - accuracy: 0.9666 - val_loss: 0.3996 - val_accuracy: 0.8863\n","Epoch 485/500\n","10521/10521 [==============================] - 3s 295us/sample - loss: 0.1803 - accuracy: 0.9667 - val_loss: 0.3993 - val_accuracy: 0.8863\n","Epoch 486/500\n","10521/10521 [==============================] - 3s 295us/sample - loss: 0.1799 - accuracy: 0.9667 - val_loss: 0.3990 - val_accuracy: 0.8863\n","Epoch 487/500\n","10521/10521 [==============================] - 3s 296us/sample - loss: 0.1796 - accuracy: 0.9667 - val_loss: 0.3987 - val_accuracy: 0.8872\n","Epoch 488/500\n","10521/10521 [==============================] - 3s 297us/sample - loss: 0.1792 - accuracy: 0.9665 - val_loss: 0.3984 - val_accuracy: 0.8872\n","Epoch 489/500\n","10521/10521 [==============================] - 3s 295us/sample - loss: 0.1789 - accuracy: 0.9667 - val_loss: 0.3981 - val_accuracy: 0.8872\n","Epoch 490/500\n","10521/10521 [==============================] - 3s 297us/sample - loss: 0.1786 - accuracy: 0.9667 - val_loss: 0.3978 - val_accuracy: 0.8880\n","Epoch 491/500\n","10521/10521 [==============================] - 3s 295us/sample - loss: 0.1782 - accuracy: 0.9668 - val_loss: 0.3976 - val_accuracy: 0.8880\n","Epoch 492/500\n","10521/10521 [==============================] - 3s 294us/sample - loss: 0.1779 - accuracy: 0.9667 - val_loss: 0.3973 - val_accuracy: 0.8880\n","Epoch 493/500\n","10521/10521 [==============================] - 3s 295us/sample - loss: 0.1775 - accuracy: 0.9667 - val_loss: 0.3970 - val_accuracy: 0.8880\n","Epoch 494/500\n","10521/10521 [==============================] - 3s 291us/sample - loss: 0.1772 - accuracy: 0.9666 - val_loss: 0.3967 - val_accuracy: 0.8880\n","Epoch 495/500\n","10521/10521 [==============================] - 3s 294us/sample - loss: 0.1768 - accuracy: 0.9666 - val_loss: 0.3964 - val_accuracy: 0.8880\n","Epoch 496/500\n","10521/10521 [==============================] - 3s 293us/sample - loss: 0.1765 - accuracy: 0.9666 - val_loss: 0.3962 - val_accuracy: 0.8880\n","Epoch 497/500\n","10521/10521 [==============================] - 3s 293us/sample - loss: 0.1762 - accuracy: 0.9665 - val_loss: 0.3959 - val_accuracy: 0.8880\n","Epoch 498/500\n","10521/10521 [==============================] - 3s 292us/sample - loss: 0.1758 - accuracy: 0.9666 - val_loss: 0.3956 - val_accuracy: 0.8880\n","Epoch 499/500\n","10521/10521 [==============================] - 3s 299us/sample - loss: 0.1755 - accuracy: 0.9666 - val_loss: 0.3953 - val_accuracy: 0.8880\n","Epoch 500/500\n","10521/10521 [==============================] - 3s 304us/sample - loss: 0.1752 - accuracy: 0.9666 - val_loss: 0.3950 - val_accuracy: 0.8880\n","Weights from best epoch have been loaded into model.\n","                 precision    recall  f1-score   support\n","\n","        general       0.91      0.83      0.86       491\n","twisted_meaning       0.80      0.86      0.83       332\n","   very_twisted       0.95      1.00      0.97       347\n","\n","       accuracy                           0.89      1170\n","      macro avg       0.89      0.90      0.89      1170\n","   weighted avg       0.89      0.89      0.89      1170\n","\n","model /content/drive/My Drive/memotion/wgts/onlysar3classesgeneral_twisted_meaning_very_twisted.h5\n","Model Saved\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tZAOuStNYJnO","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"o7eeVfoX8NtA","colab_type":"code","outputId":"1e295c83-bb7e-4ace-b6ea-495b33c3101a","executionInfo":{"status":"ok","timestamp":1583960781566,"user_tz":-330,"elapsed":755637,"user":{"displayName":"Harsh Kataria","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimH0VsMEaiEzaEeo5nJzMe1WuJRQ2b1DK9fb3S=s64","userId":"07300304294680146354"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import ktrain\n","from ktrain import text\n","\n","\n","\n","modelslist = [['not_motivational','motivational']]\n","data_files = ['memotion_eq_motivation.csv']\n","for columnclass, data_file in zip(modelslist, data_files):\n","\n","    \n","    columns=columnclass      \n","    DATA_PATH = '/content/drive/My Drive/memotion/' + data_file\n","    print(DATA_PATH)\n","\n","    NUM_WORDS = 90000\n","    MAXLEN = 250\n","\n","    h5name = '_'.join(columnclass)\n","    print('model', h5name)\n","\n","    (x_train, y_train), (x_test, y_test), preproc = text.texts_from_csv(DATA_PATH,\n","                          'cleaned_ocr',\n","                          label_columns = columns,\n","                          val_filepath=None, # if None, 10% of data will be used for validation\n","                          max_features=NUM_WORDS, maxlen=MAXLEN,\n","                        ngram_range=5,)\n","\n","\n","    model = text.text_classifier('logreg', (x_train, y_train), preproc=preproc,)\n","\n","    learner = ktrain.get_learner(model, train_data=(x_train, y_train), val_data=(x_test, y_test),batch_size=20)\n","    a='/content/drive/My Drive/Weight_file/'+h5name\n","    learner.autofit(0.0001, 500,early_stopping=300, reduce_on_plateau=5,)\n","\n","    a=learner.validate(val_data=(x_test, y_test), class_names=columns)\n","  \n","    predictor = ktrain.get_predictor(learner.model, preproc)\n","\n","   # score = 'xyz'########################\n","    h5name = \"/content/drive/My Drive/memotion/wgts/motivationalnotmoticlasses\"+h5name +  '.h5'\n","    print('model', h5name)\n","\n","    predictor.save(h5name)\n","    print(\"Model Saved\")\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/memotion/memotion_eq_motivation.csv\n","model not_motivational_motivational\n","detected encoding: utf-8 (if wrong, set manually)\n","language: en\n","Word Counts: 11348\n","Nrows: 7704\n","7704 train sequences\n","train sequence lengths:\n","\tmean : 8\n","\t95percentile : 17\n","\t99percentile : 25\n","Adding 5-gram features\n","max_features changed to 223467 with addition of ngrams\n","Average train sequence length with ngrams: 30\n","train (w/ngrams) sequence lengths:\n","\tmean : 31\n","\t95percentile : 75\n","\t99percentile : 115\n","x_train shape: (7704,250)\n","y_train shape: (7704, 2)\n","857 test sequences\n","test sequence lengths:\n","\tmean : 7\n","\t95percentile : 16\n","\t99percentile : 22\n","Average test sequence length with ngrams: 16\n","test (w/ngrams) sequence lengths:\n","\tmean : 16\n","\t95percentile : 55\n","\t99percentile : 92\n","x_test shape: (857,250)\n","y_test shape: (857, 2)\n","Is Multi-Label? False\n","compiling word ID features...\n","maxlen is 250\n","done.\n","\n","\n","begin training using triangular learning rate policy with max lr of 0.0001...\n","Train on 7704 samples, validate on 857 samples\n","Epoch 1/500\n","7704/7704 [==============================] - 3s 382us/sample - loss: 0.6903 - accuracy: 0.5615 - val_loss: 0.6873 - val_accuracy: 0.6243\n","Epoch 2/500\n","7704/7704 [==============================] - 2s 240us/sample - loss: 0.6691 - accuracy: 0.8545 - val_loss: 0.6804 - val_accuracy: 0.7013\n","Epoch 3/500\n","7704/7704 [==============================] - 2s 242us/sample - loss: 0.6460 - accuracy: 0.9337 - val_loss: 0.6743 - val_accuracy: 0.7223\n","Epoch 4/500\n","7704/7704 [==============================] - 2s 241us/sample - loss: 0.6238 - accuracy: 0.9548 - val_loss: 0.6678 - val_accuracy: 0.7410\n","Epoch 5/500\n","7704/7704 [==============================] - 2s 244us/sample - loss: 0.6026 - accuracy: 0.9640 - val_loss: 0.6614 - val_accuracy: 0.7526\n","Epoch 6/500\n","7704/7704 [==============================] - 2s 238us/sample - loss: 0.5826 - accuracy: 0.9661 - val_loss: 0.6553 - val_accuracy: 0.7480\n","Epoch 7/500\n","7704/7704 [==============================] - 2s 236us/sample - loss: 0.5636 - accuracy: 0.9700 - val_loss: 0.6494 - val_accuracy: 0.7503\n","Epoch 8/500\n","7704/7704 [==============================] - 2s 240us/sample - loss: 0.5456 - accuracy: 0.9709 - val_loss: 0.6439 - val_accuracy: 0.7526\n","Epoch 9/500\n","7704/7704 [==============================] - 2s 241us/sample - loss: 0.5283 - accuracy: 0.9733 - val_loss: 0.6383 - val_accuracy: 0.7573\n","Epoch 10/500\n","7704/7704 [==============================] - 2s 239us/sample - loss: 0.5121 - accuracy: 0.9740 - val_loss: 0.6332 - val_accuracy: 0.7608\n","Epoch 11/500\n","7704/7704 [==============================] - 2s 239us/sample - loss: 0.4967 - accuracy: 0.9752 - val_loss: 0.6286 - val_accuracy: 0.7620\n","Epoch 12/500\n","7704/7704 [==============================] - 2s 239us/sample - loss: 0.4820 - accuracy: 0.9756 - val_loss: 0.6236 - val_accuracy: 0.7643\n","Epoch 13/500\n","7704/7704 [==============================] - 2s 240us/sample - loss: 0.4679 - accuracy: 0.9764 - val_loss: 0.6187 - val_accuracy: 0.7666\n","Epoch 14/500\n","7704/7704 [==============================] - 2s 241us/sample - loss: 0.4545 - accuracy: 0.9769 - val_loss: 0.6143 - val_accuracy: 0.7655\n","Epoch 15/500\n","7704/7704 [==============================] - 2s 245us/sample - loss: 0.4417 - accuracy: 0.9774 - val_loss: 0.6099 - val_accuracy: 0.7690\n","Epoch 16/500\n","7704/7704 [==============================] - 2s 243us/sample - loss: 0.4295 - accuracy: 0.9778 - val_loss: 0.6058 - val_accuracy: 0.7678\n","Epoch 17/500\n","7704/7704 [==============================] - 2s 243us/sample - loss: 0.4177 - accuracy: 0.9782 - val_loss: 0.6018 - val_accuracy: 0.7701\n","Epoch 18/500\n","7704/7704 [==============================] - 2s 239us/sample - loss: 0.4065 - accuracy: 0.9781 - val_loss: 0.5982 - val_accuracy: 0.7701\n","Epoch 19/500\n","7704/7704 [==============================] - 2s 244us/sample - loss: 0.3958 - accuracy: 0.9782 - val_loss: 0.5946 - val_accuracy: 0.7713\n","Epoch 20/500\n","7704/7704 [==============================] - 2s 243us/sample - loss: 0.3854 - accuracy: 0.9791 - val_loss: 0.5909 - val_accuracy: 0.7760\n","Epoch 21/500\n","7704/7704 [==============================] - 2s 245us/sample - loss: 0.3756 - accuracy: 0.9791 - val_loss: 0.5873 - val_accuracy: 0.7736\n","Epoch 22/500\n","7704/7704 [==============================] - 2s 245us/sample - loss: 0.3661 - accuracy: 0.9795 - val_loss: 0.5839 - val_accuracy: 0.7760\n","Epoch 23/500\n","7704/7704 [==============================] - 2s 241us/sample - loss: 0.3570 - accuracy: 0.9799 - val_loss: 0.5807 - val_accuracy: 0.7760\n","Epoch 24/500\n","7704/7704 [==============================] - 2s 239us/sample - loss: 0.3482 - accuracy: 0.9803 - val_loss: 0.5775 - val_accuracy: 0.7771\n","Epoch 25/500\n","7704/7704 [==============================] - 2s 242us/sample - loss: 0.3398 - accuracy: 0.9807 - val_loss: 0.5747 - val_accuracy: 0.7748\n","Epoch 26/500\n","7704/7704 [==============================] - 2s 238us/sample - loss: 0.3317 - accuracy: 0.9810 - val_loss: 0.5718 - val_accuracy: 0.7783\n","Epoch 27/500\n","7704/7704 [==============================] - 2s 239us/sample - loss: 0.3239 - accuracy: 0.9813 - val_loss: 0.5688 - val_accuracy: 0.7806\n","Epoch 28/500\n","7704/7704 [==============================] - 2s 239us/sample - loss: 0.3164 - accuracy: 0.9818 - val_loss: 0.5661 - val_accuracy: 0.7783\n","Epoch 29/500\n","7704/7704 [==============================] - 2s 241us/sample - loss: 0.3092 - accuracy: 0.9820 - val_loss: 0.5633 - val_accuracy: 0.7771\n","Epoch 30/500\n","7704/7704 [==============================] - 2s 240us/sample - loss: 0.3022 - accuracy: 0.9826 - val_loss: 0.5606 - val_accuracy: 0.7760\n","Epoch 31/500\n","7704/7704 [==============================] - 2s 245us/sample - loss: 0.2955 - accuracy: 0.9829 - val_loss: 0.5584 - val_accuracy: 0.7760\n","Epoch 32/500\n","7704/7704 [==============================] - 2s 243us/sample - loss: 0.2890 - accuracy: 0.9829 - val_loss: 0.5561 - val_accuracy: 0.7771\n","Epoch 33/500\n","7704/7704 [==============================] - 2s 240us/sample - loss: 0.2828 - accuracy: 0.9831 - val_loss: 0.5537 - val_accuracy: 0.7771\n","Epoch 34/500\n","7704/7704 [==============================] - 2s 238us/sample - loss: 0.2768 - accuracy: 0.9829 - val_loss: 0.5515 - val_accuracy: 0.7783\n","Epoch 35/500\n","7704/7704 [==============================] - 2s 236us/sample - loss: 0.2710 - accuracy: 0.9836 - val_loss: 0.5492 - val_accuracy: 0.7783\n","Epoch 36/500\n","7704/7704 [==============================] - 2s 239us/sample - loss: 0.2654 - accuracy: 0.9838 - val_loss: 0.5472 - val_accuracy: 0.7783\n","Epoch 37/500\n","7704/7704 [==============================] - 2s 239us/sample - loss: 0.2599 - accuracy: 0.9836 - val_loss: 0.5453 - val_accuracy: 0.7783\n","Epoch 38/500\n","7704/7704 [==============================] - 2s 236us/sample - loss: 0.2547 - accuracy: 0.9839 - val_loss: 0.5435 - val_accuracy: 0.7795\n","Epoch 39/500\n","7704/7704 [==============================] - 2s 241us/sample - loss: 0.2496 - accuracy: 0.9839 - val_loss: 0.5415 - val_accuracy: 0.7783\n","Epoch 40/500\n","7704/7704 [==============================] - 2s 242us/sample - loss: 0.2448 - accuracy: 0.9842 - val_loss: 0.5397 - val_accuracy: 0.7783\n","Epoch 41/500\n","7704/7704 [==============================] - 2s 239us/sample - loss: 0.2400 - accuracy: 0.9843 - val_loss: 0.5377 - val_accuracy: 0.7783\n","Epoch 42/500\n","7704/7704 [==============================] - 2s 241us/sample - loss: 0.2354 - accuracy: 0.9852 - val_loss: 0.5358 - val_accuracy: 0.7795\n","Epoch 43/500\n","7704/7704 [==============================] - 2s 242us/sample - loss: 0.2309 - accuracy: 0.9852 - val_loss: 0.5342 - val_accuracy: 0.7806\n","Epoch 44/500\n","7704/7704 [==============================] - 2s 248us/sample - loss: 0.2266 - accuracy: 0.9852 - val_loss: 0.5329 - val_accuracy: 0.7818\n","Epoch 45/500\n","7704/7704 [==============================] - 2s 248us/sample - loss: 0.2224 - accuracy: 0.9853 - val_loss: 0.5316 - val_accuracy: 0.7795\n","Epoch 46/500\n","7704/7704 [==============================] - 2s 244us/sample - loss: 0.2184 - accuracy: 0.9853 - val_loss: 0.5303 - val_accuracy: 0.7783\n","Epoch 47/500\n","7704/7704 [==============================] - 2s 246us/sample - loss: 0.2145 - accuracy: 0.9856 - val_loss: 0.5291 - val_accuracy: 0.7806\n","Epoch 48/500\n","7704/7704 [==============================] - 2s 246us/sample - loss: 0.2107 - accuracy: 0.9859 - val_loss: 0.5278 - val_accuracy: 0.7818\n","Epoch 49/500\n","7704/7704 [==============================] - 2s 249us/sample - loss: 0.2070 - accuracy: 0.9860 - val_loss: 0.5266 - val_accuracy: 0.7795\n","Epoch 50/500\n","7704/7704 [==============================] - 2s 250us/sample - loss: 0.2035 - accuracy: 0.9861 - val_loss: 0.5257 - val_accuracy: 0.7783\n","Epoch 51/500\n","7704/7704 [==============================] - 2s 248us/sample - loss: 0.2000 - accuracy: 0.9860 - val_loss: 0.5247 - val_accuracy: 0.7783\n","Epoch 52/500\n","7704/7704 [==============================] - 2s 247us/sample - loss: 0.1966 - accuracy: 0.9862 - val_loss: 0.5234 - val_accuracy: 0.7806\n","Epoch 53/500\n","7704/7704 [==============================] - 2s 244us/sample - loss: 0.1933 - accuracy: 0.9866 - val_loss: 0.5223 - val_accuracy: 0.7818\n","Epoch 54/500\n","7704/7704 [==============================] - 2s 240us/sample - loss: 0.1902 - accuracy: 0.9868 - val_loss: 0.5212 - val_accuracy: 0.7783\n","Epoch 55/500\n","7704/7704 [==============================] - 2s 239us/sample - loss: 0.1871 - accuracy: 0.9869 - val_loss: 0.5202 - val_accuracy: 0.7795\n","Epoch 56/500\n","7704/7704 [==============================] - 2s 245us/sample - loss: 0.1841 - accuracy: 0.9868 - val_loss: 0.5194 - val_accuracy: 0.7806\n","Epoch 57/500\n","7704/7704 [==============================] - 2s 244us/sample - loss: 0.1812 - accuracy: 0.9869 - val_loss: 0.5181 - val_accuracy: 0.7795\n","Epoch 58/500\n","7704/7704 [==============================] - 2s 245us/sample - loss: 0.1783 - accuracy: 0.9873 - val_loss: 0.5174 - val_accuracy: 0.7806\n","Epoch 59/500\n","7704/7704 [==============================] - 2s 244us/sample - loss: 0.1756 - accuracy: 0.9874 - val_loss: 0.5164 - val_accuracy: 0.7806\n","Epoch 60/500\n","7704/7704 [==============================] - 2s 247us/sample - loss: 0.1729 - accuracy: 0.9871 - val_loss: 0.5154 - val_accuracy: 0.7806\n","Epoch 61/500\n","7704/7704 [==============================] - 2s 245us/sample - loss: 0.1703 - accuracy: 0.9871 - val_loss: 0.5147 - val_accuracy: 0.7783\n","Epoch 62/500\n","7704/7704 [==============================] - 2s 244us/sample - loss: 0.1677 - accuracy: 0.9874 - val_loss: 0.5137 - val_accuracy: 0.7783\n","Epoch 63/500\n","7704/7704 [==============================] - 2s 241us/sample - loss: 0.1652 - accuracy: 0.9873 - val_loss: 0.5130 - val_accuracy: 0.7771\n","Epoch 64/500\n","7704/7704 [==============================] - 2s 241us/sample - loss: 0.1628 - accuracy: 0.9875 - val_loss: 0.5122 - val_accuracy: 0.7783\n","Epoch 65/500\n","7704/7704 [==============================] - 2s 241us/sample - loss: 0.1605 - accuracy: 0.9879 - val_loss: 0.5116 - val_accuracy: 0.7795\n","Epoch 66/500\n","7704/7704 [==============================] - 2s 236us/sample - loss: 0.1582 - accuracy: 0.9879 - val_loss: 0.5110 - val_accuracy: 0.7806\n","Epoch 67/500\n","7704/7704 [==============================] - 2s 243us/sample - loss: 0.1559 - accuracy: 0.9883 - val_loss: 0.5106 - val_accuracy: 0.7806\n","Epoch 68/500\n","7704/7704 [==============================] - 2s 237us/sample - loss: 0.1538 - accuracy: 0.9882 - val_loss: 0.5100 - val_accuracy: 0.7783\n","Epoch 69/500\n","7704/7704 [==============================] - 2s 238us/sample - loss: 0.1516 - accuracy: 0.9883 - val_loss: 0.5097 - val_accuracy: 0.7783\n","Epoch 70/500\n","7704/7704 [==============================] - 2s 237us/sample - loss: 0.1495 - accuracy: 0.9884 - val_loss: 0.5089 - val_accuracy: 0.7795\n","Epoch 71/500\n","7704/7704 [==============================] - 2s 237us/sample - loss: 0.1475 - accuracy: 0.9884 - val_loss: 0.5086 - val_accuracy: 0.7795\n","Epoch 72/500\n","7704/7704 [==============================] - 2s 240us/sample - loss: 0.1455 - accuracy: 0.9886 - val_loss: 0.5080 - val_accuracy: 0.7806\n","Epoch 73/500\n","7704/7704 [==============================] - 2s 244us/sample - loss: 0.1436 - accuracy: 0.9887 - val_loss: 0.5077 - val_accuracy: 0.7783\n","Epoch 74/500\n","7704/7704 [==============================] - 2s 240us/sample - loss: 0.1417 - accuracy: 0.9887 - val_loss: 0.5074 - val_accuracy: 0.7748\n","Epoch 75/500\n","7704/7704 [==============================] - 2s 236us/sample - loss: 0.1399 - accuracy: 0.9888 - val_loss: 0.5069 - val_accuracy: 0.7760\n","Epoch 76/500\n","7704/7704 [==============================] - 2s 238us/sample - loss: 0.1381 - accuracy: 0.9887 - val_loss: 0.5065 - val_accuracy: 0.7748\n","Epoch 77/500\n","7704/7704 [==============================] - 2s 237us/sample - loss: 0.1363 - accuracy: 0.9887 - val_loss: 0.5062 - val_accuracy: 0.7725\n","Epoch 78/500\n","7704/7704 [==============================] - 2s 234us/sample - loss: 0.1346 - accuracy: 0.9887 - val_loss: 0.5060 - val_accuracy: 0.7725\n","Epoch 79/500\n","7704/7704 [==============================] - 2s 234us/sample - loss: 0.1329 - accuracy: 0.9886 - val_loss: 0.5058 - val_accuracy: 0.7701\n","Epoch 80/500\n","7704/7704 [==============================] - 2s 236us/sample - loss: 0.1312 - accuracy: 0.9888 - val_loss: 0.5052 - val_accuracy: 0.7701\n","Epoch 81/500\n","7704/7704 [==============================] - 2s 233us/sample - loss: 0.1297 - accuracy: 0.9888 - val_loss: 0.5051 - val_accuracy: 0.7701\n","Epoch 82/500\n","7704/7704 [==============================] - 2s 237us/sample - loss: 0.1281 - accuracy: 0.9894 - val_loss: 0.5049 - val_accuracy: 0.7713\n","Epoch 83/500\n","7704/7704 [==============================] - 2s 236us/sample - loss: 0.1265 - accuracy: 0.9892 - val_loss: 0.5046 - val_accuracy: 0.7713\n","Epoch 84/500\n","7704/7704 [==============================] - 2s 239us/sample - loss: 0.1250 - accuracy: 0.9894 - val_loss: 0.5044 - val_accuracy: 0.7725\n","Epoch 85/500\n","7704/7704 [==============================] - 2s 235us/sample - loss: 0.1236 - accuracy: 0.9891 - val_loss: 0.5042 - val_accuracy: 0.7736\n","Epoch 86/500\n","7704/7704 [==============================] - 2s 238us/sample - loss: 0.1221 - accuracy: 0.9892 - val_loss: 0.5038 - val_accuracy: 0.7736\n","Epoch 87/500\n","7704/7704 [==============================] - 2s 237us/sample - loss: 0.1207 - accuracy: 0.9895 - val_loss: 0.5035 - val_accuracy: 0.7736\n","Epoch 88/500\n","7704/7704 [==============================] - 2s 239us/sample - loss: 0.1193 - accuracy: 0.9896 - val_loss: 0.5033 - val_accuracy: 0.7736\n","Epoch 89/500\n","7704/7704 [==============================] - 2s 234us/sample - loss: 0.1179 - accuracy: 0.9895 - val_loss: 0.5032 - val_accuracy: 0.7736\n","Epoch 90/500\n","7704/7704 [==============================] - 2s 238us/sample - loss: 0.1166 - accuracy: 0.9897 - val_loss: 0.5032 - val_accuracy: 0.7725\n","Epoch 91/500\n","7704/7704 [==============================] - 2s 238us/sample - loss: 0.1153 - accuracy: 0.9899 - val_loss: 0.5029 - val_accuracy: 0.7736\n","Epoch 92/500\n","7704/7704 [==============================] - 2s 236us/sample - loss: 0.1140 - accuracy: 0.9899 - val_loss: 0.5027 - val_accuracy: 0.7736\n","Epoch 93/500\n","7704/7704 [==============================] - 2s 238us/sample - loss: 0.1128 - accuracy: 0.9900 - val_loss: 0.5026 - val_accuracy: 0.7725\n","Epoch 94/500\n","7704/7704 [==============================] - 2s 236us/sample - loss: 0.1115 - accuracy: 0.9901 - val_loss: 0.5025 - val_accuracy: 0.7713\n","Epoch 95/500\n","7704/7704 [==============================] - 2s 237us/sample - loss: 0.1104 - accuracy: 0.9900 - val_loss: 0.5025 - val_accuracy: 0.7713\n","Epoch 96/500\n","7704/7704 [==============================] - 2s 242us/sample - loss: 0.1092 - accuracy: 0.9901 - val_loss: 0.5025 - val_accuracy: 0.7725\n","Epoch 97/500\n","7704/7704 [==============================] - 2s 238us/sample - loss: 0.1080 - accuracy: 0.9903 - val_loss: 0.5024 - val_accuracy: 0.7713\n","Epoch 98/500\n","7704/7704 [==============================] - 2s 240us/sample - loss: 0.1069 - accuracy: 0.9901 - val_loss: 0.5022 - val_accuracy: 0.7701\n","Epoch 99/500\n","7704/7704 [==============================] - 2s 239us/sample - loss: 0.1058 - accuracy: 0.9903 - val_loss: 0.5024 - val_accuracy: 0.7701\n","Epoch 100/500\n","7704/7704 [==============================] - 2s 233us/sample - loss: 0.1047 - accuracy: 0.9903 - val_loss: 0.5024 - val_accuracy: 0.7713\n","Epoch 101/500\n","7704/7704 [==============================] - 2s 234us/sample - loss: 0.1036 - accuracy: 0.9904 - val_loss: 0.5023 - val_accuracy: 0.7713\n","Epoch 102/500\n","7704/7704 [==============================] - 2s 237us/sample - loss: 0.1026 - accuracy: 0.9903 - val_loss: 0.5024 - val_accuracy: 0.7713\n","Epoch 103/500\n","7520/7704 [============================>.] - ETA: 0s - loss: 0.1012 - accuracy: 0.9902\n","Epoch 00103: Reducing Max LR on Plateau: new max lr will be 5e-05 (if not early_stopping).\n","7704/7704 [==============================] - 2s 235us/sample - loss: 0.1016 - accuracy: 0.9903 - val_loss: 0.5024 - val_accuracy: 0.7701\n","Epoch 104/500\n","7704/7704 [==============================] - 2s 239us/sample - loss: 0.1005 - accuracy: 0.9904 - val_loss: 0.5024 - val_accuracy: 0.7701\n","Epoch 105/500\n","7704/7704 [==============================] - 2s 269us/sample - loss: 0.1000 - accuracy: 0.9907 - val_loss: 0.5023 - val_accuracy: 0.7690\n","Epoch 106/500\n","7704/7704 [==============================] - 2s 270us/sample - loss: 0.0995 - accuracy: 0.9905 - val_loss: 0.5022 - val_accuracy: 0.7701\n","Epoch 107/500\n","7704/7704 [==============================] - 2s 279us/sample - loss: 0.0990 - accuracy: 0.9907 - val_loss: 0.5023 - val_accuracy: 0.7701\n","Epoch 108/500\n","7540/7704 [============================>.] - ETA: 0s - loss: 0.0985 - accuracy: 0.9906\n","Epoch 00108: Reducing Max LR on Plateau: new max lr will be 2.5e-05 (if not early_stopping).\n","7704/7704 [==============================] - 2s 277us/sample - loss: 0.0985 - accuracy: 0.9907 - val_loss: 0.5023 - val_accuracy: 0.7690\n","Epoch 109/500\n","7704/7704 [==============================] - 2s 271us/sample - loss: 0.0980 - accuracy: 0.9907 - val_loss: 0.5023 - val_accuracy: 0.7690\n","Epoch 110/500\n","7704/7704 [==============================] - 2s 236us/sample - loss: 0.0977 - accuracy: 0.9907 - val_loss: 0.5023 - val_accuracy: 0.7701\n","Epoch 111/500\n","7704/7704 [==============================] - 2s 241us/sample - loss: 0.0974 - accuracy: 0.9908 - val_loss: 0.5024 - val_accuracy: 0.7701\n","Epoch 112/500\n","7704/7704 [==============================] - 2s 239us/sample - loss: 0.0972 - accuracy: 0.9908 - val_loss: 0.5024 - val_accuracy: 0.7701\n","Epoch 113/500\n","7520/7704 [============================>.] - ETA: 0s - loss: 0.0968 - accuracy: 0.9906\n","Epoch 00113: Reducing Max LR on Plateau: new max lr will be 1.25e-05 (if not early_stopping).\n","7704/7704 [==============================] - 2s 242us/sample - loss: 0.0969 - accuracy: 0.9905 - val_loss: 0.5025 - val_accuracy: 0.7701\n","Epoch 114/500\n","7704/7704 [==============================] - 2s 241us/sample - loss: 0.0966 - accuracy: 0.9907 - val_loss: 0.5025 - val_accuracy: 0.7701\n","Epoch 115/500\n","7704/7704 [==============================] - 2s 249us/sample - loss: 0.0965 - accuracy: 0.9907 - val_loss: 0.5025 - val_accuracy: 0.7701\n","Epoch 116/500\n","7704/7704 [==============================] - 2s 244us/sample - loss: 0.0963 - accuracy: 0.9907 - val_loss: 0.5026 - val_accuracy: 0.7701\n","Epoch 117/500\n","7704/7704 [==============================] - 2s 245us/sample - loss: 0.0961 - accuracy: 0.9908 - val_loss: 0.5026 - val_accuracy: 0.7701\n","Epoch 118/500\n","7500/7704 [============================>.] - ETA: 0s - loss: 0.0960 - accuracy: 0.9905\n","Epoch 00118: Reducing Max LR on Plateau: new max lr will be 6.25e-06 (if not early_stopping).\n","7704/7704 [==============================] - 2s 248us/sample - loss: 0.0960 - accuracy: 0.9908 - val_loss: 0.5026 - val_accuracy: 0.7701\n","Epoch 119/500\n","7704/7704 [==============================] - 2s 242us/sample - loss: 0.0958 - accuracy: 0.9908 - val_loss: 0.5026 - val_accuracy: 0.7690\n","Epoch 120/500\n","7704/7704 [==============================] - 2s 241us/sample - loss: 0.0957 - accuracy: 0.9908 - val_loss: 0.5026 - val_accuracy: 0.7690\n","Epoch 121/500\n","7704/7704 [==============================] - 2s 239us/sample - loss: 0.0956 - accuracy: 0.9908 - val_loss: 0.5026 - val_accuracy: 0.7690\n","Epoch 122/500\n","7704/7704 [==============================] - 2s 242us/sample - loss: 0.0955 - accuracy: 0.9908 - val_loss: 0.5026 - val_accuracy: 0.7690\n","Epoch 123/500\n","7680/7704 [============================>.] - ETA: 0s - loss: 0.0955 - accuracy: 0.9908\n","Epoch 00123: Reducing Max LR on Plateau: new max lr will be 3.125e-06 (if not early_stopping).\n","7704/7704 [==============================] - 2s 244us/sample - loss: 0.0954 - accuracy: 0.9908 - val_loss: 0.5027 - val_accuracy: 0.7690\n","Epoch 124/500\n","7704/7704 [==============================] - 2s 253us/sample - loss: 0.0953 - accuracy: 0.9908 - val_loss: 0.5027 - val_accuracy: 0.7690\n","Epoch 125/500\n","7704/7704 [==============================] - 2s 247us/sample - loss: 0.0953 - accuracy: 0.9908 - val_loss: 0.5027 - val_accuracy: 0.7690\n","Epoch 126/500\n","7704/7704 [==============================] - 2s 244us/sample - loss: 0.0952 - accuracy: 0.9908 - val_loss: 0.5027 - val_accuracy: 0.7690\n","Epoch 127/500\n","7704/7704 [==============================] - 2s 244us/sample - loss: 0.0951 - accuracy: 0.9908 - val_loss: 0.5028 - val_accuracy: 0.7690\n","Epoch 128/500\n","7540/7704 [============================>.] - ETA: 0s - loss: 0.0955 - accuracy: 0.9906\n","Epoch 00128: Reducing Max LR on Plateau: new max lr will be 1.5625e-06 (if not early_stopping).\n","7704/7704 [==============================] - 2s 241us/sample - loss: 0.0951 - accuracy: 0.9908 - val_loss: 0.5027 - val_accuracy: 0.7690\n","Epoch 129/500\n","7704/7704 [==============================] - 2s 242us/sample - loss: 0.0950 - accuracy: 0.9908 - val_loss: 0.5028 - val_accuracy: 0.7690\n","Epoch 130/500\n","7704/7704 [==============================] - 2s 240us/sample - loss: 0.0950 - accuracy: 0.9908 - val_loss: 0.5027 - val_accuracy: 0.7690\n","Epoch 131/500\n","7704/7704 [==============================] - 2s 238us/sample - loss: 0.0949 - accuracy: 0.9908 - val_loss: 0.5028 - val_accuracy: 0.7690\n","Epoch 132/500\n","7704/7704 [==============================] - 2s 242us/sample - loss: 0.0948 - accuracy: 0.9908 - val_loss: 0.5028 - val_accuracy: 0.7690\n","Epoch 133/500\n","7520/7704 [============================>.] - ETA: 0s - loss: 0.0947 - accuracy: 0.9906\n","Epoch 00133: Reducing Max LR on Plateau: new max lr will be 7.8125e-07 (if not early_stopping).\n","7704/7704 [==============================] - 2s 240us/sample - loss: 0.0948 - accuracy: 0.9908 - val_loss: 0.5028 - val_accuracy: 0.7690\n","Epoch 134/500\n","7704/7704 [==============================] - 2s 246us/sample - loss: 0.0947 - accuracy: 0.9908 - val_loss: 0.5028 - val_accuracy: 0.7690\n","Epoch 135/500\n","7704/7704 [==============================] - 2s 250us/sample - loss: 0.0947 - accuracy: 0.9908 - val_loss: 0.5028 - val_accuracy: 0.7690\n","Epoch 136/500\n","7704/7704 [==============================] - 2s 279us/sample - loss: 0.0946 - accuracy: 0.9908 - val_loss: 0.5028 - val_accuracy: 0.7690\n","Epoch 137/500\n","7704/7704 [==============================] - 3s 336us/sample - loss: 0.0946 - accuracy: 0.9908 - val_loss: 0.5028 - val_accuracy: 0.7690\n","Epoch 138/500\n","7580/7704 [============================>.] - ETA: 0s - loss: 0.0946 - accuracy: 0.9908\n","Epoch 00138: Reducing Max LR on Plateau: new max lr will be 3.90625e-07 (if not early_stopping).\n","7704/7704 [==============================] - 2s 238us/sample - loss: 0.0945 - accuracy: 0.9908 - val_loss: 0.5028 - val_accuracy: 0.7690\n","Epoch 139/500\n","7704/7704 [==============================] - 2s 244us/sample - loss: 0.0945 - accuracy: 0.9908 - val_loss: 0.5028 - val_accuracy: 0.7690\n","Epoch 140/500\n","7704/7704 [==============================] - 2s 244us/sample - loss: 0.0945 - accuracy: 0.9908 - val_loss: 0.5028 - val_accuracy: 0.7690\n","Epoch 141/500\n","7704/7704 [==============================] - 2s 250us/sample - loss: 0.0944 - accuracy: 0.9908 - val_loss: 0.5028 - val_accuracy: 0.7690\n","Epoch 142/500\n","7704/7704 [==============================] - 2s 277us/sample - loss: 0.0944 - accuracy: 0.9908 - val_loss: 0.5028 - val_accuracy: 0.7690\n","Epoch 143/500\n","7600/7704 [============================>.] - ETA: 0s - loss: 0.0943 - accuracy: 0.9909\n","Epoch 00143: Reducing Max LR on Plateau: new max lr will be 1.953125e-07 (if not early_stopping).\n","7704/7704 [==============================] - 2s 275us/sample - loss: 0.0943 - accuracy: 0.9908 - val_loss: 0.5028 - val_accuracy: 0.7690\n","Epoch 144/500\n","7704/7704 [==============================] - 2s 257us/sample - loss: 0.0943 - accuracy: 0.9908 - val_loss: 0.5028 - val_accuracy: 0.7690\n","Epoch 145/500\n","7704/7704 [==============================] - 2s 244us/sample - loss: 0.0943 - accuracy: 0.9908 - val_loss: 0.5029 - val_accuracy: 0.7690\n","Epoch 146/500\n","7704/7704 [==============================] - 2s 244us/sample - loss: 0.0942 - accuracy: 0.9908 - val_loss: 0.5029 - val_accuracy: 0.7690\n","Epoch 147/500\n","7704/7704 [==============================] - 2s 247us/sample - loss: 0.0942 - accuracy: 0.9908 - val_loss: 0.5029 - val_accuracy: 0.7690\n","Epoch 148/500\n","7660/7704 [============================>.] - ETA: 0s - loss: 0.0939 - accuracy: 0.9909\n","Epoch 00148: Reducing Max LR on Plateau: new max lr will be 9.765625e-08 (if not early_stopping).\n","7704/7704 [==============================] - 2s 243us/sample - loss: 0.0941 - accuracy: 0.9908 - val_loss: 0.5030 - val_accuracy: 0.7690\n","Epoch 149/500\n","7704/7704 [==============================] - 2s 247us/sample - loss: 0.0941 - accuracy: 0.9908 - val_loss: 0.5029 - val_accuracy: 0.7690\n","Epoch 150/500\n","7704/7704 [==============================] - 2s 242us/sample - loss: 0.0941 - accuracy: 0.9908 - val_loss: 0.5030 - val_accuracy: 0.7690\n","Epoch 151/500\n","7704/7704 [==============================] - 2s 247us/sample - loss: 0.0940 - accuracy: 0.9908 - val_loss: 0.5030 - val_accuracy: 0.7690\n","Epoch 152/500\n","7704/7704 [==============================] - 2s 246us/sample - loss: 0.0940 - accuracy: 0.9908 - val_loss: 0.5030 - val_accuracy: 0.7690\n","Epoch 153/500\n","7704/7704 [==============================] - 2s 251us/sample - loss: 0.0939 - accuracy: 0.9908 - val_loss: 0.5030 - val_accuracy: 0.7690\n","Epoch 154/500\n","7704/7704 [==============================] - 2s 253us/sample - loss: 0.0939 - accuracy: 0.9908 - val_loss: 0.5030 - val_accuracy: 0.7690\n","Epoch 155/500\n","7704/7704 [==============================] - 2s 254us/sample - loss: 0.0938 - accuracy: 0.9908 - val_loss: 0.5031 - val_accuracy: 0.7690\n","Epoch 156/500\n","7704/7704 [==============================] - 2s 253us/sample - loss: 0.0938 - accuracy: 0.9908 - val_loss: 0.5031 - val_accuracy: 0.7690\n","Epoch 157/500\n","7704/7704 [==============================] - 2s 249us/sample - loss: 0.0938 - accuracy: 0.9908 - val_loss: 0.5031 - val_accuracy: 0.7690\n","Epoch 158/500\n","7704/7704 [==============================] - 2s 248us/sample - loss: 0.0937 - accuracy: 0.9908 - val_loss: 0.5031 - val_accuracy: 0.7690\n","Epoch 159/500\n","7704/7704 [==============================] - 2s 249us/sample - loss: 0.0937 - accuracy: 0.9908 - val_loss: 0.5031 - val_accuracy: 0.7690\n","Epoch 160/500\n","7704/7704 [==============================] - 2s 257us/sample - loss: 0.0936 - accuracy: 0.9908 - val_loss: 0.5031 - val_accuracy: 0.7690\n","Epoch 161/500\n","7704/7704 [==============================] - 2s 251us/sample - loss: 0.0936 - accuracy: 0.9908 - val_loss: 0.5031 - val_accuracy: 0.7690\n","Epoch 162/500\n","7704/7704 [==============================] - 2s 250us/sample - loss: 0.0935 - accuracy: 0.9908 - val_loss: 0.5031 - val_accuracy: 0.7690\n","Epoch 163/500\n","7704/7704 [==============================] - 2s 246us/sample - loss: 0.0935 - accuracy: 0.9908 - val_loss: 0.5031 - val_accuracy: 0.7690\n","Epoch 164/500\n","7704/7704 [==============================] - 2s 246us/sample - loss: 0.0934 - accuracy: 0.9908 - val_loss: 0.5031 - val_accuracy: 0.7690\n","Epoch 165/500\n","7704/7704 [==============================] - 2s 249us/sample - loss: 0.0934 - accuracy: 0.9908 - val_loss: 0.5032 - val_accuracy: 0.7690\n","Epoch 166/500\n","7704/7704 [==============================] - 2s 251us/sample - loss: 0.0934 - accuracy: 0.9908 - val_loss: 0.5031 - val_accuracy: 0.7690\n","Epoch 167/500\n","7704/7704 [==============================] - 2s 252us/sample - loss: 0.0933 - accuracy: 0.9908 - val_loss: 0.5031 - val_accuracy: 0.7690\n","Epoch 168/500\n","7704/7704 [==============================] - 2s 253us/sample - loss: 0.0933 - accuracy: 0.9908 - val_loss: 0.5031 - val_accuracy: 0.7690\n","Epoch 169/500\n","7704/7704 [==============================] - 2s 245us/sample - loss: 0.0932 - accuracy: 0.9908 - val_loss: 0.5031 - val_accuracy: 0.7690\n","Epoch 170/500\n","7704/7704 [==============================] - 2s 242us/sample - loss: 0.0932 - accuracy: 0.9908 - val_loss: 0.5031 - val_accuracy: 0.7690\n","Epoch 171/500\n","7704/7704 [==============================] - 2s 240us/sample - loss: 0.0931 - accuracy: 0.9908 - val_loss: 0.5031 - val_accuracy: 0.7690\n","Epoch 172/500\n","7704/7704 [==============================] - 2s 245us/sample - loss: 0.0931 - accuracy: 0.9908 - val_loss: 0.5031 - val_accuracy: 0.7690\n","Epoch 173/500\n","7704/7704 [==============================] - 2s 247us/sample - loss: 0.0931 - accuracy: 0.9908 - val_loss: 0.5031 - val_accuracy: 0.7690\n","Epoch 174/500\n","7704/7704 [==============================] - 2s 244us/sample - loss: 0.0930 - accuracy: 0.9908 - val_loss: 0.5031 - val_accuracy: 0.7690\n","Epoch 175/500\n","7704/7704 [==============================] - 2s 244us/sample - loss: 0.0930 - accuracy: 0.9908 - val_loss: 0.5031 - val_accuracy: 0.7690\n","Epoch 176/500\n","7704/7704 [==============================] - 2s 243us/sample - loss: 0.0929 - accuracy: 0.9908 - val_loss: 0.5031 - val_accuracy: 0.7690\n","Epoch 177/500\n","7704/7704 [==============================] - 2s 249us/sample - loss: 0.0929 - accuracy: 0.9908 - val_loss: 0.5031 - val_accuracy: 0.7690\n","Epoch 178/500\n","7704/7704 [==============================] - 2s 248us/sample - loss: 0.0928 - accuracy: 0.9908 - val_loss: 0.5031 - val_accuracy: 0.7690\n","Epoch 179/500\n","7704/7704 [==============================] - 2s 245us/sample - loss: 0.0928 - accuracy: 0.9908 - val_loss: 0.5031 - val_accuracy: 0.7690\n","Epoch 180/500\n","7704/7704 [==============================] - 2s 243us/sample - loss: 0.0928 - accuracy: 0.9908 - val_loss: 0.5031 - val_accuracy: 0.7690\n","Epoch 181/500\n","7704/7704 [==============================] - 2s 245us/sample - loss: 0.0927 - accuracy: 0.9908 - val_loss: 0.5031 - val_accuracy: 0.7690\n","Epoch 182/500\n","7704/7704 [==============================] - 2s 243us/sample - loss: 0.0927 - accuracy: 0.9908 - val_loss: 0.5031 - val_accuracy: 0.7690\n","Epoch 183/500\n","7704/7704 [==============================] - 2s 248us/sample - loss: 0.0926 - accuracy: 0.9908 - val_loss: 0.5031 - val_accuracy: 0.7690\n","Epoch 184/500\n","7704/7704 [==============================] - 2s 248us/sample - loss: 0.0926 - accuracy: 0.9908 - val_loss: 0.5031 - val_accuracy: 0.7690\n","Epoch 185/500\n","7704/7704 [==============================] - 2s 241us/sample - loss: 0.0925 - accuracy: 0.9908 - val_loss: 0.5031 - val_accuracy: 0.7690\n","Epoch 186/500\n","7704/7704 [==============================] - 2s 244us/sample - loss: 0.0925 - accuracy: 0.9908 - val_loss: 0.5031 - val_accuracy: 0.7690\n","Epoch 187/500\n","7704/7704 [==============================] - 2s 246us/sample - loss: 0.0925 - accuracy: 0.9908 - val_loss: 0.5031 - val_accuracy: 0.7690\n","Epoch 188/500\n","7704/7704 [==============================] - 2s 245us/sample - loss: 0.0924 - accuracy: 0.9908 - val_loss: 0.5031 - val_accuracy: 0.7690\n","Epoch 189/500\n","7704/7704 [==============================] - 2s 250us/sample - loss: 0.0924 - accuracy: 0.9908 - val_loss: 0.5031 - val_accuracy: 0.7690\n","Epoch 190/500\n","7704/7704 [==============================] - 2s 247us/sample - loss: 0.0923 - accuracy: 0.9908 - val_loss: 0.5031 - val_accuracy: 0.7690\n","Epoch 191/500\n","7704/7704 [==============================] - 2s 247us/sample - loss: 0.0923 - accuracy: 0.9908 - val_loss: 0.5031 - val_accuracy: 0.7690\n","Epoch 192/500\n","7704/7704 [==============================] - 2s 245us/sample - loss: 0.0922 - accuracy: 0.9908 - val_loss: 0.5031 - val_accuracy: 0.7690\n","Epoch 193/500\n","7704/7704 [==============================] - 2s 246us/sample - loss: 0.0922 - accuracy: 0.9908 - val_loss: 0.5031 - val_accuracy: 0.7690\n","Epoch 194/500\n","7704/7704 [==============================] - 2s 246us/sample - loss: 0.0922 - accuracy: 0.9908 - val_loss: 0.5031 - val_accuracy: 0.7690\n","Epoch 195/500\n","7704/7704 [==============================] - 2s 242us/sample - loss: 0.0921 - accuracy: 0.9908 - val_loss: 0.5032 - val_accuracy: 0.7690\n","Epoch 196/500\n","7704/7704 [==============================] - 2s 242us/sample - loss: 0.0921 - accuracy: 0.9908 - val_loss: 0.5032 - val_accuracy: 0.7690\n","Epoch 197/500\n","7704/7704 [==============================] - 2s 242us/sample - loss: 0.0920 - accuracy: 0.9908 - val_loss: 0.5032 - val_accuracy: 0.7690\n","Epoch 198/500\n","7704/7704 [==============================] - 2s 245us/sample - loss: 0.0920 - accuracy: 0.9908 - val_loss: 0.5032 - val_accuracy: 0.7690\n","Epoch 199/500\n","7704/7704 [==============================] - 2s 253us/sample - loss: 0.0919 - accuracy: 0.9908 - val_loss: 0.5032 - val_accuracy: 0.7690\n","Epoch 200/500\n","7704/7704 [==============================] - 2s 248us/sample - loss: 0.0919 - accuracy: 0.9908 - val_loss: 0.5032 - val_accuracy: 0.7690\n","Epoch 201/500\n","7704/7704 [==============================] - 2s 245us/sample - loss: 0.0919 - accuracy: 0.9908 - val_loss: 0.5032 - val_accuracy: 0.7690\n","Epoch 202/500\n","7704/7704 [==============================] - 2s 247us/sample - loss: 0.0918 - accuracy: 0.9907 - val_loss: 0.5032 - val_accuracy: 0.7690\n","Epoch 203/500\n","7704/7704 [==============================] - 2s 245us/sample - loss: 0.0918 - accuracy: 0.9908 - val_loss: 0.5032 - val_accuracy: 0.7690\n","Epoch 204/500\n","7704/7704 [==============================] - 2s 245us/sample - loss: 0.0917 - accuracy: 0.9908 - val_loss: 0.5032 - val_accuracy: 0.7690\n","Epoch 205/500\n","7704/7704 [==============================] - 2s 241us/sample - loss: 0.0917 - accuracy: 0.9908 - val_loss: 0.5032 - val_accuracy: 0.7690\n","Epoch 206/500\n","7704/7704 [==============================] - 2s 242us/sample - loss: 0.0917 - accuracy: 0.9908 - val_loss: 0.5032 - val_accuracy: 0.7690\n","Epoch 207/500\n","7704/7704 [==============================] - 2s 243us/sample - loss: 0.0916 - accuracy: 0.9908 - val_loss: 0.5033 - val_accuracy: 0.7690\n","Epoch 208/500\n","7704/7704 [==============================] - 2s 242us/sample - loss: 0.0916 - accuracy: 0.9907 - val_loss: 0.5033 - val_accuracy: 0.7690\n","Epoch 209/500\n","7704/7704 [==============================] - 2s 244us/sample - loss: 0.0915 - accuracy: 0.9908 - val_loss: 0.5033 - val_accuracy: 0.7690\n","Epoch 210/500\n","7704/7704 [==============================] - 2s 249us/sample - loss: 0.0915 - accuracy: 0.9908 - val_loss: 0.5033 - val_accuracy: 0.7690\n","Epoch 211/500\n","7704/7704 [==============================] - 2s 244us/sample - loss: 0.0914 - accuracy: 0.9908 - val_loss: 0.5033 - val_accuracy: 0.7690\n","Epoch 212/500\n","7704/7704 [==============================] - 2s 240us/sample - loss: 0.0914 - accuracy: 0.9908 - val_loss: 0.5033 - val_accuracy: 0.7690\n","Epoch 213/500\n","7704/7704 [==============================] - 2s 244us/sample - loss: 0.0914 - accuracy: 0.9908 - val_loss: 0.5033 - val_accuracy: 0.7690\n","Epoch 214/500\n","7704/7704 [==============================] - 2s 241us/sample - loss: 0.0913 - accuracy: 0.9908 - val_loss: 0.5033 - val_accuracy: 0.7690\n","Epoch 215/500\n","7704/7704 [==============================] - 2s 241us/sample - loss: 0.0913 - accuracy: 0.9908 - val_loss: 0.5033 - val_accuracy: 0.7690\n","Epoch 216/500\n","7704/7704 [==============================] - 2s 248us/sample - loss: 0.0912 - accuracy: 0.9908 - val_loss: 0.5033 - val_accuracy: 0.7690\n","Epoch 217/500\n","7704/7704 [==============================] - 2s 247us/sample - loss: 0.0912 - accuracy: 0.9908 - val_loss: 0.5033 - val_accuracy: 0.7690\n","Epoch 218/500\n","7704/7704 [==============================] - 2s 249us/sample - loss: 0.0912 - accuracy: 0.9908 - val_loss: 0.5033 - val_accuracy: 0.7690\n","Epoch 219/500\n","7704/7704 [==============================] - 2s 250us/sample - loss: 0.0911 - accuracy: 0.9908 - val_loss: 0.5034 - val_accuracy: 0.7690\n","Epoch 220/500\n","7704/7704 [==============================] - 2s 251us/sample - loss: 0.0911 - accuracy: 0.9908 - val_loss: 0.5034 - val_accuracy: 0.7690\n","Epoch 221/500\n","7704/7704 [==============================] - 2s 245us/sample - loss: 0.0910 - accuracy: 0.9908 - val_loss: 0.5034 - val_accuracy: 0.7690\n","Epoch 222/500\n","7704/7704 [==============================] - 2s 249us/sample - loss: 0.0910 - accuracy: 0.9908 - val_loss: 0.5034 - val_accuracy: 0.7690\n","Epoch 223/500\n","7704/7704 [==============================] - 2s 242us/sample - loss: 0.0909 - accuracy: 0.9908 - val_loss: 0.5034 - val_accuracy: 0.7690\n","Epoch 224/500\n","7704/7704 [==============================] - 2s 245us/sample - loss: 0.0909 - accuracy: 0.9908 - val_loss: 0.5035 - val_accuracy: 0.7690\n","Epoch 225/500\n","7704/7704 [==============================] - 2s 248us/sample - loss: 0.0909 - accuracy: 0.9908 - val_loss: 0.5034 - val_accuracy: 0.7690\n","Epoch 226/500\n","7704/7704 [==============================] - 2s 256us/sample - loss: 0.0908 - accuracy: 0.9908 - val_loss: 0.5035 - val_accuracy: 0.7690\n","Epoch 227/500\n","7704/7704 [==============================] - 2s 245us/sample - loss: 0.0908 - accuracy: 0.9909 - val_loss: 0.5035 - val_accuracy: 0.7690\n","Epoch 228/500\n","7704/7704 [==============================] - 2s 250us/sample - loss: 0.0907 - accuracy: 0.9909 - val_loss: 0.5035 - val_accuracy: 0.7690\n","Epoch 229/500\n","7704/7704 [==============================] - 2s 252us/sample - loss: 0.0907 - accuracy: 0.9909 - val_loss: 0.5035 - val_accuracy: 0.7690\n","Epoch 230/500\n","7704/7704 [==============================] - 2s 258us/sample - loss: 0.0907 - accuracy: 0.9909 - val_loss: 0.5035 - val_accuracy: 0.7690\n","Epoch 231/500\n","7704/7704 [==============================] - 2s 259us/sample - loss: 0.0906 - accuracy: 0.9909 - val_loss: 0.5035 - val_accuracy: 0.7690\n","Epoch 232/500\n","7704/7704 [==============================] - 2s 255us/sample - loss: 0.0906 - accuracy: 0.9909 - val_loss: 0.5035 - val_accuracy: 0.7690\n","Epoch 233/500\n","7704/7704 [==============================] - 2s 257us/sample - loss: 0.0905 - accuracy: 0.9909 - val_loss: 0.5035 - val_accuracy: 0.7690\n","Epoch 234/500\n","7704/7704 [==============================] - 2s 252us/sample - loss: 0.0905 - accuracy: 0.9909 - val_loss: 0.5035 - val_accuracy: 0.7690\n","Epoch 235/500\n","7704/7704 [==============================] - 2s 248us/sample - loss: 0.0905 - accuracy: 0.9909 - val_loss: 0.5035 - val_accuracy: 0.7690\n","Epoch 236/500\n","7704/7704 [==============================] - 2s 248us/sample - loss: 0.0904 - accuracy: 0.9908 - val_loss: 0.5035 - val_accuracy: 0.7690\n","Epoch 237/500\n","7704/7704 [==============================] - 2s 250us/sample - loss: 0.0904 - accuracy: 0.9909 - val_loss: 0.5035 - val_accuracy: 0.7690\n","Epoch 238/500\n","7704/7704 [==============================] - 2s 247us/sample - loss: 0.0903 - accuracy: 0.9909 - val_loss: 0.5036 - val_accuracy: 0.7690\n","Epoch 239/500\n","7704/7704 [==============================] - 2s 249us/sample - loss: 0.0903 - accuracy: 0.9909 - val_loss: 0.5036 - val_accuracy: 0.7690\n","Epoch 240/500\n","7704/7704 [==============================] - 2s 249us/sample - loss: 0.0903 - accuracy: 0.9909 - val_loss: 0.5036 - val_accuracy: 0.7690\n","Epoch 241/500\n","7704/7704 [==============================] - 2s 247us/sample - loss: 0.0902 - accuracy: 0.9909 - val_loss: 0.5036 - val_accuracy: 0.7690\n","Epoch 242/500\n","7704/7704 [==============================] - 2s 249us/sample - loss: 0.0902 - accuracy: 0.9909 - val_loss: 0.5036 - val_accuracy: 0.7690\n","Epoch 243/500\n","7704/7704 [==============================] - 2s 249us/sample - loss: 0.0901 - accuracy: 0.9909 - val_loss: 0.5036 - val_accuracy: 0.7690\n","Epoch 244/500\n","7704/7704 [==============================] - 2s 250us/sample - loss: 0.0901 - accuracy: 0.9909 - val_loss: 0.5036 - val_accuracy: 0.7690\n","Epoch 245/500\n","7704/7704 [==============================] - 2s 249us/sample - loss: 0.0901 - accuracy: 0.9908 - val_loss: 0.5036 - val_accuracy: 0.7678\n","Epoch 246/500\n","7704/7704 [==============================] - 2s 246us/sample - loss: 0.0900 - accuracy: 0.9909 - val_loss: 0.5036 - val_accuracy: 0.7678\n","Epoch 247/500\n","7704/7704 [==============================] - 2s 248us/sample - loss: 0.0900 - accuracy: 0.9908 - val_loss: 0.5036 - val_accuracy: 0.7678\n","Epoch 248/500\n","7704/7704 [==============================] - 2s 241us/sample - loss: 0.0899 - accuracy: 0.9908 - val_loss: 0.5037 - val_accuracy: 0.7678\n","Epoch 249/500\n","7704/7704 [==============================] - 2s 240us/sample - loss: 0.0899 - accuracy: 0.9909 - val_loss: 0.5036 - val_accuracy: 0.7678\n","Epoch 250/500\n","7704/7704 [==============================] - 2s 244us/sample - loss: 0.0899 - accuracy: 0.9908 - val_loss: 0.5037 - val_accuracy: 0.7678\n","Epoch 251/500\n","7704/7704 [==============================] - 2s 246us/sample - loss: 0.0898 - accuracy: 0.9908 - val_loss: 0.5037 - val_accuracy: 0.7678\n","Epoch 252/500\n","7704/7704 [==============================] - 2s 247us/sample - loss: 0.0898 - accuracy: 0.9908 - val_loss: 0.5037 - val_accuracy: 0.7678\n","Epoch 253/500\n","7704/7704 [==============================] - 2s 242us/sample - loss: 0.0897 - accuracy: 0.9908 - val_loss: 0.5037 - val_accuracy: 0.7678\n","Epoch 254/500\n","7704/7704 [==============================] - 2s 243us/sample - loss: 0.0897 - accuracy: 0.9909 - val_loss: 0.5037 - val_accuracy: 0.7678\n","Epoch 255/500\n","7704/7704 [==============================] - 2s 246us/sample - loss: 0.0897 - accuracy: 0.9909 - val_loss: 0.5037 - val_accuracy: 0.7666\n","Epoch 256/500\n","7704/7704 [==============================] - 2s 247us/sample - loss: 0.0896 - accuracy: 0.9908 - val_loss: 0.5037 - val_accuracy: 0.7678\n","Epoch 257/500\n","7704/7704 [==============================] - 2s 249us/sample - loss: 0.0896 - accuracy: 0.9908 - val_loss: 0.5038 - val_accuracy: 0.7666\n","Epoch 258/500\n","7704/7704 [==============================] - 2s 246us/sample - loss: 0.0895 - accuracy: 0.9908 - val_loss: 0.5038 - val_accuracy: 0.7666\n","Epoch 259/500\n","7704/7704 [==============================] - 2s 242us/sample - loss: 0.0895 - accuracy: 0.9908 - val_loss: 0.5038 - val_accuracy: 0.7666\n","Epoch 260/500\n","7704/7704 [==============================] - 2s 244us/sample - loss: 0.0895 - accuracy: 0.9908 - val_loss: 0.5038 - val_accuracy: 0.7666\n","Epoch 261/500\n","7704/7704 [==============================] - 2s 240us/sample - loss: 0.0894 - accuracy: 0.9908 - val_loss: 0.5038 - val_accuracy: 0.7666\n","Epoch 262/500\n","7704/7704 [==============================] - 2s 251us/sample - loss: 0.0894 - accuracy: 0.9908 - val_loss: 0.5038 - val_accuracy: 0.7666\n","Epoch 263/500\n","7704/7704 [==============================] - 2s 246us/sample - loss: 0.0893 - accuracy: 0.9908 - val_loss: 0.5038 - val_accuracy: 0.7666\n","Epoch 264/500\n","7704/7704 [==============================] - 2s 245us/sample - loss: 0.0893 - accuracy: 0.9908 - val_loss: 0.5038 - val_accuracy: 0.7666\n","Epoch 265/500\n","7704/7704 [==============================] - 2s 245us/sample - loss: 0.0893 - accuracy: 0.9908 - val_loss: 0.5038 - val_accuracy: 0.7666\n","Epoch 266/500\n","7704/7704 [==============================] - 2s 241us/sample - loss: 0.0892 - accuracy: 0.9908 - val_loss: 0.5038 - val_accuracy: 0.7666\n","Epoch 267/500\n","7704/7704 [==============================] - 2s 247us/sample - loss: 0.0892 - accuracy: 0.9908 - val_loss: 0.5038 - val_accuracy: 0.7666\n","Epoch 268/500\n","7704/7704 [==============================] - 2s 272us/sample - loss: 0.0891 - accuracy: 0.9908 - val_loss: 0.5038 - val_accuracy: 0.7666\n","Epoch 269/500\n","7704/7704 [==============================] - 2s 269us/sample - loss: 0.0891 - accuracy: 0.9908 - val_loss: 0.5038 - val_accuracy: 0.7666\n","Epoch 270/500\n","7704/7704 [==============================] - 2s 272us/sample - loss: 0.0891 - accuracy: 0.9908 - val_loss: 0.5038 - val_accuracy: 0.7666\n","Epoch 271/500\n","7704/7704 [==============================] - 2s 273us/sample - loss: 0.0890 - accuracy: 0.9908 - val_loss: 0.5038 - val_accuracy: 0.7666\n","Epoch 272/500\n","7704/7704 [==============================] - 2s 269us/sample - loss: 0.0890 - accuracy: 0.9908 - val_loss: 0.5038 - val_accuracy: 0.7666\n","Epoch 273/500\n","7704/7704 [==============================] - 2s 249us/sample - loss: 0.0889 - accuracy: 0.9908 - val_loss: 0.5039 - val_accuracy: 0.7666\n","Epoch 274/500\n","7704/7704 [==============================] - 2s 249us/sample - loss: 0.0889 - accuracy: 0.9908 - val_loss: 0.5038 - val_accuracy: 0.7666\n","Epoch 275/500\n","7704/7704 [==============================] - 2s 245us/sample - loss: 0.0889 - accuracy: 0.9908 - val_loss: 0.5038 - val_accuracy: 0.7666\n","Epoch 276/500\n","7704/7704 [==============================] - 2s 245us/sample - loss: 0.0888 - accuracy: 0.9908 - val_loss: 0.5039 - val_accuracy: 0.7666\n","Epoch 277/500\n","7704/7704 [==============================] - 2s 245us/sample - loss: 0.0888 - accuracy: 0.9908 - val_loss: 0.5039 - val_accuracy: 0.7666\n","Epoch 278/500\n","7704/7704 [==============================] - 2s 247us/sample - loss: 0.0887 - accuracy: 0.9909 - val_loss: 0.5039 - val_accuracy: 0.7666\n","Epoch 279/500\n","7704/7704 [==============================] - 2s 246us/sample - loss: 0.0887 - accuracy: 0.9909 - val_loss: 0.5039 - val_accuracy: 0.7666\n","Epoch 280/500\n","7704/7704 [==============================] - 2s 243us/sample - loss: 0.0887 - accuracy: 0.9908 - val_loss: 0.5039 - val_accuracy: 0.7666\n","Epoch 281/500\n","7704/7704 [==============================] - 2s 249us/sample - loss: 0.0886 - accuracy: 0.9908 - val_loss: 0.5039 - val_accuracy: 0.7666\n","Epoch 282/500\n","7704/7704 [==============================] - 2s 243us/sample - loss: 0.0886 - accuracy: 0.9909 - val_loss: 0.5039 - val_accuracy: 0.7666\n","Epoch 283/500\n","7704/7704 [==============================] - 2s 244us/sample - loss: 0.0885 - accuracy: 0.9909 - val_loss: 0.5040 - val_accuracy: 0.7666\n","Epoch 284/500\n","7704/7704 [==============================] - 2s 249us/sample - loss: 0.0885 - accuracy: 0.9909 - val_loss: 0.5040 - val_accuracy: 0.7666\n","Epoch 285/500\n","7704/7704 [==============================] - 2s 243us/sample - loss: 0.0885 - accuracy: 0.9909 - val_loss: 0.5040 - val_accuracy: 0.7666\n","Epoch 286/500\n","7704/7704 [==============================] - 2s 243us/sample - loss: 0.0884 - accuracy: 0.9909 - val_loss: 0.5040 - val_accuracy: 0.7666\n","Epoch 287/500\n","7704/7704 [==============================] - 2s 245us/sample - loss: 0.0884 - accuracy: 0.9909 - val_loss: 0.5040 - val_accuracy: 0.7666\n","Epoch 288/500\n","7704/7704 [==============================] - 2s 245us/sample - loss: 0.0883 - accuracy: 0.9909 - val_loss: 0.5040 - val_accuracy: 0.7666\n","Epoch 289/500\n","7704/7704 [==============================] - 2s 249us/sample - loss: 0.0883 - accuracy: 0.9909 - val_loss: 0.5040 - val_accuracy: 0.7666\n","Epoch 290/500\n","7704/7704 [==============================] - 2s 250us/sample - loss: 0.0883 - accuracy: 0.9909 - val_loss: 0.5040 - val_accuracy: 0.7666\n","Epoch 291/500\n","7704/7704 [==============================] - 2s 248us/sample - loss: 0.0882 - accuracy: 0.9909 - val_loss: 0.5041 - val_accuracy: 0.7666\n","Epoch 292/500\n","7704/7704 [==============================] - 2s 244us/sample - loss: 0.0882 - accuracy: 0.9909 - val_loss: 0.5041 - val_accuracy: 0.7666\n","Epoch 293/500\n","7704/7704 [==============================] - 2s 248us/sample - loss: 0.0882 - accuracy: 0.9909 - val_loss: 0.5041 - val_accuracy: 0.7666\n","Epoch 294/500\n","7704/7704 [==============================] - 2s 248us/sample - loss: 0.0881 - accuracy: 0.9909 - val_loss: 0.5041 - val_accuracy: 0.7666\n","Epoch 295/500\n","7704/7704 [==============================] - 2s 244us/sample - loss: 0.0881 - accuracy: 0.9909 - val_loss: 0.5041 - val_accuracy: 0.7666\n","Epoch 296/500\n","7704/7704 [==============================] - 2s 241us/sample - loss: 0.0880 - accuracy: 0.9909 - val_loss: 0.5041 - val_accuracy: 0.7666\n","Epoch 297/500\n","7704/7704 [==============================] - 2s 244us/sample - loss: 0.0880 - accuracy: 0.9909 - val_loss: 0.5041 - val_accuracy: 0.7666\n","Epoch 298/500\n","7704/7704 [==============================] - 2s 242us/sample - loss: 0.0880 - accuracy: 0.9909 - val_loss: 0.5041 - val_accuracy: 0.7666\n","Epoch 299/500\n","7704/7704 [==============================] - 2s 242us/sample - loss: 0.0879 - accuracy: 0.9909 - val_loss: 0.5041 - val_accuracy: 0.7666\n","Epoch 300/500\n","7704/7704 [==============================] - 2s 241us/sample - loss: 0.0879 - accuracy: 0.9909 - val_loss: 0.5041 - val_accuracy: 0.7666\n","Epoch 301/500\n","7704/7704 [==============================] - 2s 243us/sample - loss: 0.0878 - accuracy: 0.9909 - val_loss: 0.5041 - val_accuracy: 0.7666\n","Epoch 302/500\n","7704/7704 [==============================] - 2s 266us/sample - loss: 0.0878 - accuracy: 0.9909 - val_loss: 0.5041 - val_accuracy: 0.7666\n","Epoch 303/500\n","7704/7704 [==============================] - 2s 276us/sample - loss: 0.0878 - accuracy: 0.9909 - val_loss: 0.5041 - val_accuracy: 0.7666\n","Epoch 304/500\n","7704/7704 [==============================] - 2s 274us/sample - loss: 0.0877 - accuracy: 0.9909 - val_loss: 0.5041 - val_accuracy: 0.7666\n","Epoch 305/500\n","7704/7704 [==============================] - 2s 246us/sample - loss: 0.0877 - accuracy: 0.9909 - val_loss: 0.5041 - val_accuracy: 0.7666\n","Epoch 306/500\n","7704/7704 [==============================] - 2s 239us/sample - loss: 0.0877 - accuracy: 0.9909 - val_loss: 0.5041 - val_accuracy: 0.7666\n","Epoch 307/500\n","7704/7704 [==============================] - 2s 240us/sample - loss: 0.0876 - accuracy: 0.9908 - val_loss: 0.5041 - val_accuracy: 0.7666\n","Epoch 308/500\n","7704/7704 [==============================] - 2s 242us/sample - loss: 0.0876 - accuracy: 0.9908 - val_loss: 0.5042 - val_accuracy: 0.7666\n","Epoch 309/500\n","7704/7704 [==============================] - 2s 244us/sample - loss: 0.0875 - accuracy: 0.9908 - val_loss: 0.5042 - val_accuracy: 0.7666\n","Epoch 310/500\n","7704/7704 [==============================] - 2s 246us/sample - loss: 0.0875 - accuracy: 0.9909 - val_loss: 0.5042 - val_accuracy: 0.7666\n","Epoch 311/500\n","7704/7704 [==============================] - 2s 246us/sample - loss: 0.0875 - accuracy: 0.9909 - val_loss: 0.5041 - val_accuracy: 0.7666\n","Epoch 312/500\n","7704/7704 [==============================] - 2s 246us/sample - loss: 0.0874 - accuracy: 0.9909 - val_loss: 0.5042 - val_accuracy: 0.7666\n","Epoch 313/500\n","7704/7704 [==============================] - 2s 241us/sample - loss: 0.0874 - accuracy: 0.9909 - val_loss: 0.5042 - val_accuracy: 0.7666\n","Epoch 314/500\n","7704/7704 [==============================] - 2s 243us/sample - loss: 0.0873 - accuracy: 0.9909 - val_loss: 0.5042 - val_accuracy: 0.7666\n","Epoch 315/500\n","7704/7704 [==============================] - 2s 239us/sample - loss: 0.0873 - accuracy: 0.9909 - val_loss: 0.5042 - val_accuracy: 0.7666\n","Epoch 316/500\n","7704/7704 [==============================] - 2s 243us/sample - loss: 0.0873 - accuracy: 0.9909 - val_loss: 0.5042 - val_accuracy: 0.7666\n","Epoch 317/500\n","7704/7704 [==============================] - 2s 242us/sample - loss: 0.0872 - accuracy: 0.9909 - val_loss: 0.5042 - val_accuracy: 0.7666\n","Epoch 318/500\n","7704/7704 [==============================] - 2s 240us/sample - loss: 0.0872 - accuracy: 0.9909 - val_loss: 0.5042 - val_accuracy: 0.7666\n","Epoch 319/500\n","7704/7704 [==============================] - 2s 244us/sample - loss: 0.0872 - accuracy: 0.9909 - val_loss: 0.5042 - val_accuracy: 0.7678\n","Epoch 320/500\n","7704/7704 [==============================] - 2s 241us/sample - loss: 0.0871 - accuracy: 0.9909 - val_loss: 0.5042 - val_accuracy: 0.7678\n","Epoch 321/500\n","7704/7704 [==============================] - 2s 243us/sample - loss: 0.0871 - accuracy: 0.9909 - val_loss: 0.5043 - val_accuracy: 0.7678\n","Epoch 322/500\n","7704/7704 [==============================] - 2s 245us/sample - loss: 0.0870 - accuracy: 0.9909 - val_loss: 0.5043 - val_accuracy: 0.7678\n","Epoch 323/500\n","7704/7704 [==============================] - 2s 245us/sample - loss: 0.0870 - accuracy: 0.9909 - val_loss: 0.5043 - val_accuracy: 0.7678\n","Epoch 324/500\n","7704/7704 [==============================] - 2s 245us/sample - loss: 0.0870 - accuracy: 0.9908 - val_loss: 0.5043 - val_accuracy: 0.7678\n","Epoch 325/500\n","7704/7704 [==============================] - 2s 250us/sample - loss: 0.0869 - accuracy: 0.9909 - val_loss: 0.5043 - val_accuracy: 0.7678\n","Epoch 326/500\n","7704/7704 [==============================] - 2s 246us/sample - loss: 0.0869 - accuracy: 0.9909 - val_loss: 0.5043 - val_accuracy: 0.7678\n","Epoch 327/500\n","7704/7704 [==============================] - 2s 248us/sample - loss: 0.0869 - accuracy: 0.9909 - val_loss: 0.5043 - val_accuracy: 0.7678\n","Epoch 328/500\n","7704/7704 [==============================] - 2s 245us/sample - loss: 0.0868 - accuracy: 0.9909 - val_loss: 0.5043 - val_accuracy: 0.7678\n","Epoch 329/500\n","7704/7704 [==============================] - 2s 246us/sample - loss: 0.0868 - accuracy: 0.9908 - val_loss: 0.5043 - val_accuracy: 0.7678\n","Epoch 330/500\n","7704/7704 [==============================] - 2s 244us/sample - loss: 0.0867 - accuracy: 0.9909 - val_loss: 0.5043 - val_accuracy: 0.7678\n","Epoch 331/500\n","7704/7704 [==============================] - 2s 244us/sample - loss: 0.0867 - accuracy: 0.9908 - val_loss: 0.5043 - val_accuracy: 0.7678\n","Epoch 332/500\n","7704/7704 [==============================] - 2s 238us/sample - loss: 0.0867 - accuracy: 0.9910 - val_loss: 0.5044 - val_accuracy: 0.7678\n","Epoch 333/500\n","7704/7704 [==============================] - 2s 244us/sample - loss: 0.0866 - accuracy: 0.9908 - val_loss: 0.5044 - val_accuracy: 0.7678\n","Epoch 334/500\n","7704/7704 [==============================] - 2s 242us/sample - loss: 0.0866 - accuracy: 0.9909 - val_loss: 0.5044 - val_accuracy: 0.7678\n","Epoch 335/500\n","7704/7704 [==============================] - 2s 243us/sample - loss: 0.0866 - accuracy: 0.9909 - val_loss: 0.5045 - val_accuracy: 0.7678\n","Epoch 336/500\n","7704/7704 [==============================] - 2s 245us/sample - loss: 0.0865 - accuracy: 0.9910 - val_loss: 0.5045 - val_accuracy: 0.7678\n","Epoch 337/500\n","7704/7704 [==============================] - 2s 245us/sample - loss: 0.0865 - accuracy: 0.9909 - val_loss: 0.5045 - val_accuracy: 0.7678\n","Epoch 338/500\n","7704/7704 [==============================] - 2s 245us/sample - loss: 0.0864 - accuracy: 0.9910 - val_loss: 0.5045 - val_accuracy: 0.7678\n","Epoch 339/500\n","7704/7704 [==============================] - 2s 243us/sample - loss: 0.0864 - accuracy: 0.9910 - val_loss: 0.5045 - val_accuracy: 0.7678\n","Epoch 340/500\n","7704/7704 [==============================] - 2s 244us/sample - loss: 0.0864 - accuracy: 0.9910 - val_loss: 0.5045 - val_accuracy: 0.7678\n","Epoch 341/500\n","7704/7704 [==============================] - 2s 242us/sample - loss: 0.0863 - accuracy: 0.9910 - val_loss: 0.5045 - val_accuracy: 0.7678\n","Epoch 342/500\n","7704/7704 [==============================] - 2s 243us/sample - loss: 0.0863 - accuracy: 0.9910 - val_loss: 0.5045 - val_accuracy: 0.7678\n","Epoch 343/500\n","7704/7704 [==============================] - 2s 237us/sample - loss: 0.0863 - accuracy: 0.9910 - val_loss: 0.5045 - val_accuracy: 0.7678\n","Epoch 344/500\n","7704/7704 [==============================] - 2s 244us/sample - loss: 0.0862 - accuracy: 0.9910 - val_loss: 0.5045 - val_accuracy: 0.7678\n","Epoch 345/500\n","7704/7704 [==============================] - 2s 241us/sample - loss: 0.0862 - accuracy: 0.9910 - val_loss: 0.5045 - val_accuracy: 0.7678\n","Epoch 346/500\n","7704/7704 [==============================] - 2s 242us/sample - loss: 0.0861 - accuracy: 0.9910 - val_loss: 0.5046 - val_accuracy: 0.7678\n","Epoch 347/500\n","7704/7704 [==============================] - 2s 242us/sample - loss: 0.0861 - accuracy: 0.9910 - val_loss: 0.5046 - val_accuracy: 0.7678\n","Epoch 348/500\n","7704/7704 [==============================] - 2s 241us/sample - loss: 0.0861 - accuracy: 0.9910 - val_loss: 0.5046 - val_accuracy: 0.7678\n","Epoch 349/500\n","7704/7704 [==============================] - 2s 245us/sample - loss: 0.0860 - accuracy: 0.9910 - val_loss: 0.5046 - val_accuracy: 0.7678\n","Epoch 350/500\n","7704/7704 [==============================] - 2s 239us/sample - loss: 0.0860 - accuracy: 0.9910 - val_loss: 0.5046 - val_accuracy: 0.7678\n","Epoch 351/500\n","7704/7704 [==============================] - 2s 238us/sample - loss: 0.0860 - accuracy: 0.9910 - val_loss: 0.5047 - val_accuracy: 0.7678\n","Epoch 352/500\n","7704/7704 [==============================] - 2s 240us/sample - loss: 0.0859 - accuracy: 0.9910 - val_loss: 0.5046 - val_accuracy: 0.7678\n","Epoch 353/500\n","7704/7704 [==============================] - 2s 244us/sample - loss: 0.0859 - accuracy: 0.9909 - val_loss: 0.5047 - val_accuracy: 0.7666\n","Epoch 354/500\n","7704/7704 [==============================] - 2s 240us/sample - loss: 0.0859 - accuracy: 0.9910 - val_loss: 0.5047 - val_accuracy: 0.7666\n","Epoch 355/500\n","7704/7704 [==============================] - 2s 241us/sample - loss: 0.0858 - accuracy: 0.9910 - val_loss: 0.5047 - val_accuracy: 0.7666\n","Epoch 356/500\n","7704/7704 [==============================] - 2s 238us/sample - loss: 0.0858 - accuracy: 0.9910 - val_loss: 0.5047 - val_accuracy: 0.7666\n","Epoch 357/500\n","7704/7704 [==============================] - 2s 247us/sample - loss: 0.0857 - accuracy: 0.9910 - val_loss: 0.5047 - val_accuracy: 0.7666\n","Epoch 358/500\n","7704/7704 [==============================] - 2s 245us/sample - loss: 0.0857 - accuracy: 0.9910 - val_loss: 0.5047 - val_accuracy: 0.7666\n","Epoch 359/500\n","7704/7704 [==============================] - 2s 244us/sample - loss: 0.0857 - accuracy: 0.9910 - val_loss: 0.5047 - val_accuracy: 0.7666\n","Epoch 360/500\n","7704/7704 [==============================] - 2s 246us/sample - loss: 0.0856 - accuracy: 0.9910 - val_loss: 0.5047 - val_accuracy: 0.7666\n","Epoch 361/500\n","7704/7704 [==============================] - 2s 243us/sample - loss: 0.0856 - accuracy: 0.9910 - val_loss: 0.5047 - val_accuracy: 0.7666\n","Epoch 362/500\n","7704/7704 [==============================] - 2s 241us/sample - loss: 0.0856 - accuracy: 0.9910 - val_loss: 0.5048 - val_accuracy: 0.7666\n","Epoch 363/500\n","7704/7704 [==============================] - 2s 243us/sample - loss: 0.0855 - accuracy: 0.9910 - val_loss: 0.5048 - val_accuracy: 0.7655\n","Epoch 364/500\n","7704/7704 [==============================] - 2s 240us/sample - loss: 0.0855 - accuracy: 0.9910 - val_loss: 0.5048 - val_accuracy: 0.7655\n","Epoch 365/500\n","7704/7704 [==============================] - 2s 245us/sample - loss: 0.0854 - accuracy: 0.9910 - val_loss: 0.5048 - val_accuracy: 0.7655\n","Epoch 366/500\n","7704/7704 [==============================] - 2s 241us/sample - loss: 0.0854 - accuracy: 0.9910 - val_loss: 0.5048 - val_accuracy: 0.7655\n","Epoch 367/500\n","7704/7704 [==============================] - 2s 239us/sample - loss: 0.0854 - accuracy: 0.9910 - val_loss: 0.5048 - val_accuracy: 0.7655\n","Epoch 368/500\n","7704/7704 [==============================] - 2s 238us/sample - loss: 0.0853 - accuracy: 0.9910 - val_loss: 0.5048 - val_accuracy: 0.7655\n","Epoch 369/500\n","7704/7704 [==============================] - 2s 239us/sample - loss: 0.0853 - accuracy: 0.9910 - val_loss: 0.5048 - val_accuracy: 0.7655\n","Epoch 370/500\n","7704/7704 [==============================] - 2s 240us/sample - loss: 0.0853 - accuracy: 0.9910 - val_loss: 0.5048 - val_accuracy: 0.7655\n","Epoch 371/500\n","7704/7704 [==============================] - 2s 244us/sample - loss: 0.0852 - accuracy: 0.9910 - val_loss: 0.5048 - val_accuracy: 0.7655\n","Epoch 372/500\n","7704/7704 [==============================] - 2s 243us/sample - loss: 0.0852 - accuracy: 0.9910 - val_loss: 0.5048 - val_accuracy: 0.7655\n","Epoch 373/500\n","7704/7704 [==============================] - 2s 238us/sample - loss: 0.0852 - accuracy: 0.9910 - val_loss: 0.5048 - val_accuracy: 0.7655\n","Epoch 374/500\n","7704/7704 [==============================] - 2s 245us/sample - loss: 0.0851 - accuracy: 0.9910 - val_loss: 0.5049 - val_accuracy: 0.7655\n","Epoch 375/500\n","7704/7704 [==============================] - 2s 243us/sample - loss: 0.0851 - accuracy: 0.9910 - val_loss: 0.5049 - val_accuracy: 0.7655\n","Epoch 376/500\n","7704/7704 [==============================] - 2s 237us/sample - loss: 0.0850 - accuracy: 0.9910 - val_loss: 0.5049 - val_accuracy: 0.7655\n","Epoch 377/500\n","7704/7704 [==============================] - 2s 242us/sample - loss: 0.0850 - accuracy: 0.9910 - val_loss: 0.5049 - val_accuracy: 0.7655\n","Epoch 378/500\n","7704/7704 [==============================] - 2s 242us/sample - loss: 0.0850 - accuracy: 0.9910 - val_loss: 0.5049 - val_accuracy: 0.7655\n","Epoch 379/500\n","7704/7704 [==============================] - 2s 238us/sample - loss: 0.0849 - accuracy: 0.9910 - val_loss: 0.5050 - val_accuracy: 0.7655\n","Epoch 380/500\n","7704/7704 [==============================] - 2s 243us/sample - loss: 0.0849 - accuracy: 0.9910 - val_loss: 0.5050 - val_accuracy: 0.7655\n","Epoch 381/500\n","7704/7704 [==============================] - 2s 246us/sample - loss: 0.0849 - accuracy: 0.9910 - val_loss: 0.5050 - val_accuracy: 0.7655\n","Epoch 382/500\n","7704/7704 [==============================] - 2s 245us/sample - loss: 0.0848 - accuracy: 0.9910 - val_loss: 0.5050 - val_accuracy: 0.7655\n","Epoch 383/500\n","7704/7704 [==============================] - 2s 245us/sample - loss: 0.0848 - accuracy: 0.9910 - val_loss: 0.5050 - val_accuracy: 0.7655\n","Epoch 384/500\n","7704/7704 [==============================] - 2s 245us/sample - loss: 0.0848 - accuracy: 0.9910 - val_loss: 0.5050 - val_accuracy: 0.7655\n","Epoch 385/500\n","7704/7704 [==============================] - 2s 247us/sample - loss: 0.0847 - accuracy: 0.9910 - val_loss: 0.5050 - val_accuracy: 0.7655\n","Epoch 386/500\n","7704/7704 [==============================] - 2s 245us/sample - loss: 0.0847 - accuracy: 0.9910 - val_loss: 0.5050 - val_accuracy: 0.7655\n","Epoch 387/500\n","7704/7704 [==============================] - 2s 243us/sample - loss: 0.0847 - accuracy: 0.9910 - val_loss: 0.5050 - val_accuracy: 0.7655\n","Epoch 388/500\n","7704/7704 [==============================] - 2s 240us/sample - loss: 0.0846 - accuracy: 0.9910 - val_loss: 0.5050 - val_accuracy: 0.7655\n","Epoch 389/500\n","7704/7704 [==============================] - 2s 243us/sample - loss: 0.0846 - accuracy: 0.9910 - val_loss: 0.5050 - val_accuracy: 0.7655\n","Epoch 390/500\n","7704/7704 [==============================] - 2s 247us/sample - loss: 0.0845 - accuracy: 0.9910 - val_loss: 0.5050 - val_accuracy: 0.7655\n","Epoch 391/500\n","7704/7704 [==============================] - 2s 245us/sample - loss: 0.0845 - accuracy: 0.9910 - val_loss: 0.5050 - val_accuracy: 0.7655\n","Epoch 392/500\n","7704/7704 [==============================] - 2s 246us/sample - loss: 0.0845 - accuracy: 0.9910 - val_loss: 0.5050 - val_accuracy: 0.7655\n","Epoch 393/500\n","7704/7704 [==============================] - 2s 244us/sample - loss: 0.0844 - accuracy: 0.9910 - val_loss: 0.5051 - val_accuracy: 0.7655\n","Epoch 394/500\n","7704/7704 [==============================] - 2s 240us/sample - loss: 0.0844 - accuracy: 0.9910 - val_loss: 0.5051 - val_accuracy: 0.7655\n","Epoch 395/500\n","7704/7704 [==============================] - 2s 238us/sample - loss: 0.0844 - accuracy: 0.9910 - val_loss: 0.5051 - val_accuracy: 0.7655\n","Epoch 396/500\n","7704/7704 [==============================] - 2s 242us/sample - loss: 0.0843 - accuracy: 0.9910 - val_loss: 0.5051 - val_accuracy: 0.7655\n","Epoch 397/500\n","7704/7704 [==============================] - 2s 238us/sample - loss: 0.0843 - accuracy: 0.9910 - val_loss: 0.5051 - val_accuracy: 0.7655\n","Epoch 398/500\n","7480/7704 [============================>.] - ETA: 0s - loss: 0.0842 - accuracy: 0.9913Restoring model weights from the end of the best epoch.\n","7704/7704 [==============================] - 2s 242us/sample - loss: 0.0843 - accuracy: 0.9910 - val_loss: 0.5051 - val_accuracy: 0.7655\n","Epoch 00398: early stopping\n","Weights from best epoch have been loaded into model.\n","                  precision    recall  f1-score   support\n","\n","not_motivational       0.79      0.78      0.79       464\n","    motivational       0.75      0.76      0.75       393\n","\n","        accuracy                           0.77       857\n","       macro avg       0.77      0.77      0.77       857\n","    weighted avg       0.77      0.77      0.77       857\n","\n","model /content/drive/My Drive/memotion/wgts/motivationalnotmoticlassesnot_motivational_motivational.h5\n","Model Saved\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OdzK8mFWSNCJ","colab_type":"code","outputId":"e4d4ebf8-d3ed-4917-aaa0-495d6f4994b0","executionInfo":{"status":"ok","timestamp":1583963106026,"user_tz":-330,"elapsed":3066092,"user":{"displayName":"Harsh Kataria","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimH0VsMEaiEzaEeo5nJzMe1WuJRQ2b1DK9fb3S=s64","userId":"07300304294680146354"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import ktrain\n","from ktrain import text\n","\n","\n","\n","modelslist = [['not_funny','fun']]\n","data_files = ['memotion_eq_humour.csv']\n","for columnclass, data_file in zip(modelslist, data_files):\n","\n","    \n","    columns=columnclass      \n","    DATA_PATH = '/content/drive/My Drive/memotion/' + data_file\n","    print(DATA_PATH)\n","\n","    NUM_WORDS = 90000\n","    MAXLEN = 250\n","\n","    h5name = '_'.join(columnclass)\n","    print('model', h5name)\n","\n","    (x_train, y_train), (x_test, y_test), preproc = text.texts_from_csv(DATA_PATH,\n","                          'cleaned_ocr',\n","                          label_columns = columns,\n","                          val_filepath=None, # if None, 10% of data will be used for validation\n","                          max_features=NUM_WORDS, maxlen=MAXLEN,\n","                        ngram_range=5,)\n","\n","\n","    model = text.text_classifier('logreg', (x_train, y_train), preproc=preproc,)\n","\n","    learner = ktrain.get_learner(model, train_data=(x_train, y_train), val_data=(x_test, y_test),batch_size=20)\n","    a='/content/drive/My Drive/Weight_file/'+h5name\n","    learner.autofit(0.0001, 1000,early_stopping=800, reduce_on_plateau=10,)\n","\n","    a=learner.validate(val_data=(x_test, y_test), class_names=columns)\n","  \n","    predictor = ktrain.get_predictor(learner.model, preproc)\n","\n","   # score = 'xyz'########################\n","    h5name = \"/content/drive/My Drive/memotion/wgts/funnotfunclasses\"+h5name +  '.h5'\n","    print('model', h5name)\n","\n","    predictor.save(h5name)\n","    print(\"Model Saved\")\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/memotion/memotion_eq_humour.csv\n","model not_funny_fun\n","detected encoding: utf-8 (if wrong, set manually)\n","language: en\n","Word Counts: 11475\n","Nrows: 9264\n","9264 train sequences\n","train sequence lengths:\n","\tmean : 8\n","\t95percentile : 18\n","\t99percentile : 26\n","Adding 5-gram features\n","max_features changed to 224839 with addition of ngrams\n","Average train sequence length with ngrams: 31\n","train (w/ngrams) sequence lengths:\n","\tmean : 31\n","\t95percentile : 80\n","\t99percentile : 120\n","x_train shape: (9264,250)\n","y_train shape: (9264, 2)\n","1030 test sequences\n","test sequence lengths:\n","\tmean : 8\n","\t95percentile : 17\n","\t99percentile : 23\n","Average test sequence length with ngrams: 19\n","test (w/ngrams) sequence lengths:\n","\tmean : 20\n","\t95percentile : 65\n","\t99percentile : 104\n","x_test shape: (1030,250)\n","y_test shape: (1030, 2)\n","Is Multi-Label? False\n","compiling word ID features...\n","maxlen is 250\n","done.\n","\n","\n","begin training using triangular learning rate policy with max lr of 0.0001...\n","Train on 9264 samples, validate on 1030 samples\n","Epoch 1/1000\n","9264/9264 [==============================] - 3s 295us/sample - loss: 0.6859 - accuracy: 0.6110 - val_loss: 0.6797 - val_accuracy: 0.7223\n","Epoch 2/1000\n","9264/9264 [==============================] - 2s 243us/sample - loss: 0.6566 - accuracy: 0.8960 - val_loss: 0.6649 - val_accuracy: 0.7825\n","Epoch 3/1000\n","9264/9264 [==============================] - 2s 233us/sample - loss: 0.6267 - accuracy: 0.9533 - val_loss: 0.6506 - val_accuracy: 0.8126\n","Epoch 4/1000\n","9264/9264 [==============================] - 2s 235us/sample - loss: 0.5988 - accuracy: 0.9656 - val_loss: 0.6368 - val_accuracy: 0.8272\n","Epoch 5/1000\n","9264/9264 [==============================] - 2s 238us/sample - loss: 0.5728 - accuracy: 0.9714 - val_loss: 0.6241 - val_accuracy: 0.8379\n","Epoch 6/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.5485 - accuracy: 0.9745 - val_loss: 0.6121 - val_accuracy: 0.8515\n","Epoch 7/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.5260 - accuracy: 0.9772 - val_loss: 0.6008 - val_accuracy: 0.8583\n","Epoch 8/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.5048 - accuracy: 0.9783 - val_loss: 0.5898 - val_accuracy: 0.8670\n","Epoch 9/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.4850 - accuracy: 0.9793 - val_loss: 0.5793 - val_accuracy: 0.8689\n","Epoch 10/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.4664 - accuracy: 0.9796 - val_loss: 0.5692 - val_accuracy: 0.8660\n","Epoch 11/1000\n","9264/9264 [==============================] - 2s 255us/sample - loss: 0.4489 - accuracy: 0.9804 - val_loss: 0.5597 - val_accuracy: 0.8718\n","Epoch 12/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.4324 - accuracy: 0.9802 - val_loss: 0.5506 - val_accuracy: 0.8748\n","Epoch 13/1000\n","9264/9264 [==============================] - 2s 260us/sample - loss: 0.4168 - accuracy: 0.9805 - val_loss: 0.5414 - val_accuracy: 0.8845\n","Epoch 14/1000\n","9264/9264 [==============================] - 2s 264us/sample - loss: 0.4020 - accuracy: 0.9807 - val_loss: 0.5329 - val_accuracy: 0.8825\n","Epoch 15/1000\n","9264/9264 [==============================] - 2s 260us/sample - loss: 0.3881 - accuracy: 0.9809 - val_loss: 0.5249 - val_accuracy: 0.8854\n","Epoch 16/1000\n","9264/9264 [==============================] - 2s 256us/sample - loss: 0.3750 - accuracy: 0.9815 - val_loss: 0.5171 - val_accuracy: 0.8932\n","Epoch 17/1000\n","9264/9264 [==============================] - 2s 264us/sample - loss: 0.3625 - accuracy: 0.9825 - val_loss: 0.5093 - val_accuracy: 0.8913\n","Epoch 18/1000\n","9264/9264 [==============================] - 2s 267us/sample - loss: 0.3506 - accuracy: 0.9823 - val_loss: 0.5020 - val_accuracy: 0.8951\n","Epoch 19/1000\n","9264/9264 [==============================] - 2s 265us/sample - loss: 0.3394 - accuracy: 0.9832 - val_loss: 0.4952 - val_accuracy: 0.8961\n","Epoch 20/1000\n","9264/9264 [==============================] - 2s 253us/sample - loss: 0.3286 - accuracy: 0.9831 - val_loss: 0.4883 - val_accuracy: 0.8971\n","Epoch 21/1000\n","9264/9264 [==============================] - 2s 264us/sample - loss: 0.3185 - accuracy: 0.9833 - val_loss: 0.4820 - val_accuracy: 0.8971\n","Epoch 22/1000\n","9264/9264 [==============================] - 2s 252us/sample - loss: 0.3088 - accuracy: 0.9832 - val_loss: 0.4757 - val_accuracy: 0.9019\n","Epoch 23/1000\n","9264/9264 [==============================] - 2s 257us/sample - loss: 0.2996 - accuracy: 0.9834 - val_loss: 0.4694 - val_accuracy: 0.9029\n","Epoch 24/1000\n","9264/9264 [==============================] - 2s 256us/sample - loss: 0.2908 - accuracy: 0.9835 - val_loss: 0.4636 - val_accuracy: 0.9029\n","Epoch 25/1000\n","9264/9264 [==============================] - 2s 259us/sample - loss: 0.2824 - accuracy: 0.9836 - val_loss: 0.4580 - val_accuracy: 0.9049\n","Epoch 26/1000\n","9264/9264 [==============================] - 2s 254us/sample - loss: 0.2744 - accuracy: 0.9840 - val_loss: 0.4527 - val_accuracy: 0.9049\n","Epoch 27/1000\n","9264/9264 [==============================] - 3s 282us/sample - loss: 0.2668 - accuracy: 0.9845 - val_loss: 0.4475 - val_accuracy: 0.9058\n","Epoch 28/1000\n","9264/9264 [==============================] - 3s 284us/sample - loss: 0.2595 - accuracy: 0.9846 - val_loss: 0.4424 - val_accuracy: 0.9068\n","Epoch 29/1000\n","9264/9264 [==============================] - 3s 287us/sample - loss: 0.2526 - accuracy: 0.9851 - val_loss: 0.4377 - val_accuracy: 0.9068\n","Epoch 30/1000\n","9264/9264 [==============================] - 3s 278us/sample - loss: 0.2459 - accuracy: 0.9853 - val_loss: 0.4326 - val_accuracy: 0.9068\n","Epoch 31/1000\n","9264/9264 [==============================] - 2s 254us/sample - loss: 0.2395 - accuracy: 0.9855 - val_loss: 0.4280 - val_accuracy: 0.9078\n","Epoch 32/1000\n","9264/9264 [==============================] - 2s 252us/sample - loss: 0.2333 - accuracy: 0.9856 - val_loss: 0.4236 - val_accuracy: 0.9097\n","Epoch 33/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.2274 - accuracy: 0.9859 - val_loss: 0.4192 - val_accuracy: 0.9107\n","Epoch 34/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.2218 - accuracy: 0.9862 - val_loss: 0.4151 - val_accuracy: 0.9107\n","Epoch 35/1000\n","9264/9264 [==============================] - 2s 251us/sample - loss: 0.2164 - accuracy: 0.9864 - val_loss: 0.4111 - val_accuracy: 0.9117\n","Epoch 36/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.2113 - accuracy: 0.9863 - val_loss: 0.4069 - val_accuracy: 0.9107\n","Epoch 37/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.2063 - accuracy: 0.9863 - val_loss: 0.4031 - val_accuracy: 0.9117\n","Epoch 38/1000\n","9264/9264 [==============================] - 2s 251us/sample - loss: 0.2015 - accuracy: 0.9865 - val_loss: 0.3994 - val_accuracy: 0.9117\n","Epoch 39/1000\n","9264/9264 [==============================] - 2s 253us/sample - loss: 0.1969 - accuracy: 0.9864 - val_loss: 0.3957 - val_accuracy: 0.9117\n","Epoch 40/1000\n","9264/9264 [==============================] - 2s 258us/sample - loss: 0.1925 - accuracy: 0.9866 - val_loss: 0.3923 - val_accuracy: 0.9146\n","Epoch 41/1000\n","9264/9264 [==============================] - 2s 254us/sample - loss: 0.1882 - accuracy: 0.9868 - val_loss: 0.3888 - val_accuracy: 0.9155\n","Epoch 42/1000\n","9264/9264 [==============================] - 2s 252us/sample - loss: 0.1841 - accuracy: 0.9870 - val_loss: 0.3854 - val_accuracy: 0.9165\n","Epoch 43/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.1802 - accuracy: 0.9872 - val_loss: 0.3824 - val_accuracy: 0.9165\n","Epoch 44/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.1763 - accuracy: 0.9876 - val_loss: 0.3793 - val_accuracy: 0.9175\n","Epoch 45/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.1727 - accuracy: 0.9877 - val_loss: 0.3764 - val_accuracy: 0.9175\n","Epoch 46/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.1691 - accuracy: 0.9876 - val_loss: 0.3735 - val_accuracy: 0.9184\n","Epoch 47/1000\n","9264/9264 [==============================] - 2s 252us/sample - loss: 0.1657 - accuracy: 0.9876 - val_loss: 0.3707 - val_accuracy: 0.9194\n","Epoch 48/1000\n","9264/9264 [==============================] - 2s 251us/sample - loss: 0.1625 - accuracy: 0.9878 - val_loss: 0.3681 - val_accuracy: 0.9204\n","Epoch 49/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.1593 - accuracy: 0.9881 - val_loss: 0.3655 - val_accuracy: 0.9204\n","Epoch 50/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.1562 - accuracy: 0.9884 - val_loss: 0.3628 - val_accuracy: 0.9204\n","Epoch 51/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.1532 - accuracy: 0.9884 - val_loss: 0.3603 - val_accuracy: 0.9214\n","Epoch 52/1000\n","9264/9264 [==============================] - 2s 251us/sample - loss: 0.1504 - accuracy: 0.9884 - val_loss: 0.3579 - val_accuracy: 0.9204\n","Epoch 53/1000\n","9264/9264 [==============================] - 2s 269us/sample - loss: 0.1476 - accuracy: 0.9886 - val_loss: 0.3556 - val_accuracy: 0.9194\n","Epoch 54/1000\n","9264/9264 [==============================] - 3s 274us/sample - loss: 0.1449 - accuracy: 0.9882 - val_loss: 0.3533 - val_accuracy: 0.9175\n","Epoch 55/1000\n","9264/9264 [==============================] - 2s 251us/sample - loss: 0.1422 - accuracy: 0.9881 - val_loss: 0.3512 - val_accuracy: 0.9175\n","Epoch 56/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.1397 - accuracy: 0.9887 - val_loss: 0.3490 - val_accuracy: 0.9175\n","Epoch 57/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.1373 - accuracy: 0.9887 - val_loss: 0.3470 - val_accuracy: 0.9175\n","Epoch 58/1000\n","9264/9264 [==============================] - 2s 238us/sample - loss: 0.1349 - accuracy: 0.9887 - val_loss: 0.3450 - val_accuracy: 0.9184\n","Epoch 59/1000\n","9264/9264 [==============================] - 2s 240us/sample - loss: 0.1326 - accuracy: 0.9887 - val_loss: 0.3429 - val_accuracy: 0.9184\n","Epoch 60/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.1304 - accuracy: 0.9887 - val_loss: 0.3407 - val_accuracy: 0.9184\n","Epoch 61/1000\n","9264/9264 [==============================] - 2s 240us/sample - loss: 0.1282 - accuracy: 0.9886 - val_loss: 0.3389 - val_accuracy: 0.9184\n","Epoch 62/1000\n","9264/9264 [==============================] - 2s 241us/sample - loss: 0.1261 - accuracy: 0.9886 - val_loss: 0.3368 - val_accuracy: 0.9184\n","Epoch 63/1000\n","9264/9264 [==============================] - 2s 238us/sample - loss: 0.1240 - accuracy: 0.9891 - val_loss: 0.3349 - val_accuracy: 0.9175\n","Epoch 64/1000\n","9264/9264 [==============================] - 2s 243us/sample - loss: 0.1220 - accuracy: 0.9890 - val_loss: 0.3332 - val_accuracy: 0.9165\n","Epoch 65/1000\n","9264/9264 [==============================] - 2s 242us/sample - loss: 0.1201 - accuracy: 0.9892 - val_loss: 0.3314 - val_accuracy: 0.9175\n","Epoch 66/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.1182 - accuracy: 0.9892 - val_loss: 0.3299 - val_accuracy: 0.9175\n","Epoch 67/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.1164 - accuracy: 0.9895 - val_loss: 0.3284 - val_accuracy: 0.9175\n","Epoch 68/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.1146 - accuracy: 0.9897 - val_loss: 0.3269 - val_accuracy: 0.9175\n","Epoch 69/1000\n","9264/9264 [==============================] - 2s 242us/sample - loss: 0.1129 - accuracy: 0.9897 - val_loss: 0.3254 - val_accuracy: 0.9175\n","Epoch 70/1000\n","9264/9264 [==============================] - 2s 237us/sample - loss: 0.1112 - accuracy: 0.9899 - val_loss: 0.3241 - val_accuracy: 0.9175\n","Epoch 71/1000\n","9264/9264 [==============================] - 2s 235us/sample - loss: 0.1096 - accuracy: 0.9904 - val_loss: 0.3226 - val_accuracy: 0.9175\n","Epoch 72/1000\n","9264/9264 [==============================] - 2s 237us/sample - loss: 0.1080 - accuracy: 0.9904 - val_loss: 0.3210 - val_accuracy: 0.9175\n","Epoch 73/1000\n","9264/9264 [==============================] - 2s 243us/sample - loss: 0.1064 - accuracy: 0.9905 - val_loss: 0.3195 - val_accuracy: 0.9175\n","Epoch 74/1000\n","9264/9264 [==============================] - 2s 239us/sample - loss: 0.1049 - accuracy: 0.9907 - val_loss: 0.3181 - val_accuracy: 0.9175\n","Epoch 75/1000\n","9264/9264 [==============================] - 2s 239us/sample - loss: 0.1034 - accuracy: 0.9907 - val_loss: 0.3166 - val_accuracy: 0.9175\n","Epoch 76/1000\n","9264/9264 [==============================] - 2s 239us/sample - loss: 0.1020 - accuracy: 0.9908 - val_loss: 0.3156 - val_accuracy: 0.9175\n","Epoch 77/1000\n","9264/9264 [==============================] - 2s 238us/sample - loss: 0.1006 - accuracy: 0.9908 - val_loss: 0.3141 - val_accuracy: 0.9184\n","Epoch 78/1000\n","9264/9264 [==============================] - 2s 241us/sample - loss: 0.0992 - accuracy: 0.9913 - val_loss: 0.3128 - val_accuracy: 0.9184\n","Epoch 79/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0979 - accuracy: 0.9911 - val_loss: 0.3114 - val_accuracy: 0.9175\n","Epoch 80/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0966 - accuracy: 0.9911 - val_loss: 0.3102 - val_accuracy: 0.9184\n","Epoch 81/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0953 - accuracy: 0.9910 - val_loss: 0.3087 - val_accuracy: 0.9194\n","Epoch 82/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.0940 - accuracy: 0.9913 - val_loss: 0.3074 - val_accuracy: 0.9204\n","Epoch 83/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.0928 - accuracy: 0.9913 - val_loss: 0.3063 - val_accuracy: 0.9194\n","Epoch 84/1000\n","9264/9264 [==============================] - 2s 242us/sample - loss: 0.0917 - accuracy: 0.9909 - val_loss: 0.3051 - val_accuracy: 0.9194\n","Epoch 85/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0905 - accuracy: 0.9909 - val_loss: 0.3040 - val_accuracy: 0.9184\n","Epoch 86/1000\n","9264/9264 [==============================] - 2s 237us/sample - loss: 0.0893 - accuracy: 0.9911 - val_loss: 0.3030 - val_accuracy: 0.9184\n","Epoch 87/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0882 - accuracy: 0.9911 - val_loss: 0.3020 - val_accuracy: 0.9175\n","Epoch 88/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0872 - accuracy: 0.9911 - val_loss: 0.3009 - val_accuracy: 0.9184\n","Epoch 89/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0861 - accuracy: 0.9911 - val_loss: 0.2999 - val_accuracy: 0.9175\n","Epoch 90/1000\n","9264/9264 [==============================] - 2s 241us/sample - loss: 0.0851 - accuracy: 0.9911 - val_loss: 0.2989 - val_accuracy: 0.9194\n","Epoch 91/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0841 - accuracy: 0.9911 - val_loss: 0.2979 - val_accuracy: 0.9194\n","Epoch 92/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.0831 - accuracy: 0.9914 - val_loss: 0.2969 - val_accuracy: 0.9214\n","Epoch 93/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.0821 - accuracy: 0.9917 - val_loss: 0.2959 - val_accuracy: 0.9214\n","Epoch 94/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0811 - accuracy: 0.9918 - val_loss: 0.2950 - val_accuracy: 0.9214\n","Epoch 95/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.0802 - accuracy: 0.9919 - val_loss: 0.2942 - val_accuracy: 0.9204\n","Epoch 96/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0793 - accuracy: 0.9919 - val_loss: 0.2934 - val_accuracy: 0.9204\n","Epoch 97/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0784 - accuracy: 0.9918 - val_loss: 0.2924 - val_accuracy: 0.9233\n","Epoch 98/1000\n","9264/9264 [==============================] - 2s 239us/sample - loss: 0.0776 - accuracy: 0.9918 - val_loss: 0.2915 - val_accuracy: 0.9233\n","Epoch 99/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0767 - accuracy: 0.9918 - val_loss: 0.2907 - val_accuracy: 0.9233\n","Epoch 100/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0759 - accuracy: 0.9919 - val_loss: 0.2898 - val_accuracy: 0.9223\n","Epoch 101/1000\n","9264/9264 [==============================] - 2s 243us/sample - loss: 0.0750 - accuracy: 0.9920 - val_loss: 0.2890 - val_accuracy: 0.9223\n","Epoch 102/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0742 - accuracy: 0.9919 - val_loss: 0.2883 - val_accuracy: 0.9223\n","Epoch 103/1000\n","9264/9264 [==============================] - 2s 237us/sample - loss: 0.0735 - accuracy: 0.9920 - val_loss: 0.2875 - val_accuracy: 0.9223\n","Epoch 104/1000\n","9264/9264 [==============================] - 2s 243us/sample - loss: 0.0727 - accuracy: 0.9920 - val_loss: 0.2867 - val_accuracy: 0.9233\n","Epoch 105/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0720 - accuracy: 0.9920 - val_loss: 0.2860 - val_accuracy: 0.9233\n","Epoch 106/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0712 - accuracy: 0.9920 - val_loss: 0.2851 - val_accuracy: 0.9233\n","Epoch 107/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.0705 - accuracy: 0.9921 - val_loss: 0.2845 - val_accuracy: 0.9233\n","Epoch 108/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0698 - accuracy: 0.9921 - val_loss: 0.2838 - val_accuracy: 0.9233\n","Epoch 109/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0691 - accuracy: 0.9922 - val_loss: 0.2830 - val_accuracy: 0.9233\n","Epoch 110/1000\n","9264/9264 [==============================] - 2s 251us/sample - loss: 0.0684 - accuracy: 0.9921 - val_loss: 0.2824 - val_accuracy: 0.9233\n","Epoch 111/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0677 - accuracy: 0.9922 - val_loss: 0.2816 - val_accuracy: 0.9233\n","Epoch 112/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0670 - accuracy: 0.9922 - val_loss: 0.2809 - val_accuracy: 0.9233\n","Epoch 113/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0663 - accuracy: 0.9924 - val_loss: 0.2804 - val_accuracy: 0.9223\n","Epoch 114/1000\n","9264/9264 [==============================] - 2s 240us/sample - loss: 0.0657 - accuracy: 0.9927 - val_loss: 0.2796 - val_accuracy: 0.9233\n","Epoch 115/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0650 - accuracy: 0.9928 - val_loss: 0.2790 - val_accuracy: 0.9233\n","Epoch 116/1000\n","9264/9264 [==============================] - 2s 239us/sample - loss: 0.0644 - accuracy: 0.9929 - val_loss: 0.2782 - val_accuracy: 0.9223\n","Epoch 117/1000\n","9264/9264 [==============================] - 2s 243us/sample - loss: 0.0638 - accuracy: 0.9928 - val_loss: 0.2776 - val_accuracy: 0.9223\n","Epoch 118/1000\n","9264/9264 [==============================] - 2s 242us/sample - loss: 0.0632 - accuracy: 0.9926 - val_loss: 0.2768 - val_accuracy: 0.9223\n","Epoch 119/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0626 - accuracy: 0.9929 - val_loss: 0.2764 - val_accuracy: 0.9223\n","Epoch 120/1000\n","9264/9264 [==============================] - 2s 241us/sample - loss: 0.0621 - accuracy: 0.9930 - val_loss: 0.2758 - val_accuracy: 0.9233\n","Epoch 121/1000\n","9264/9264 [==============================] - 2s 239us/sample - loss: 0.0615 - accuracy: 0.9930 - val_loss: 0.2753 - val_accuracy: 0.9233\n","Epoch 122/1000\n","9264/9264 [==============================] - 2s 252us/sample - loss: 0.0609 - accuracy: 0.9929 - val_loss: 0.2746 - val_accuracy: 0.9233\n","Epoch 123/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0604 - accuracy: 0.9930 - val_loss: 0.2741 - val_accuracy: 0.9243\n","Epoch 124/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0598 - accuracy: 0.9930 - val_loss: 0.2735 - val_accuracy: 0.9233\n","Epoch 125/1000\n","9264/9264 [==============================] - 2s 236us/sample - loss: 0.0593 - accuracy: 0.9930 - val_loss: 0.2731 - val_accuracy: 0.9233\n","Epoch 126/1000\n","9264/9264 [==============================] - 2s 243us/sample - loss: 0.0588 - accuracy: 0.9932 - val_loss: 0.2726 - val_accuracy: 0.9243\n","Epoch 127/1000\n","9264/9264 [==============================] - 2s 241us/sample - loss: 0.0583 - accuracy: 0.9935 - val_loss: 0.2722 - val_accuracy: 0.9233\n","Epoch 128/1000\n","9264/9264 [==============================] - 2s 242us/sample - loss: 0.0578 - accuracy: 0.9935 - val_loss: 0.2715 - val_accuracy: 0.9233\n","Epoch 129/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0573 - accuracy: 0.9935 - val_loss: 0.2708 - val_accuracy: 0.9233\n","Epoch 130/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0568 - accuracy: 0.9935 - val_loss: 0.2703 - val_accuracy: 0.9233\n","Epoch 131/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0563 - accuracy: 0.9935 - val_loss: 0.2701 - val_accuracy: 0.9233\n","Epoch 132/1000\n","9264/9264 [==============================] - 2s 259us/sample - loss: 0.0558 - accuracy: 0.9934 - val_loss: 0.2696 - val_accuracy: 0.9214\n","Epoch 133/1000\n","9264/9264 [==============================] - 2s 251us/sample - loss: 0.0554 - accuracy: 0.9936 - val_loss: 0.2690 - val_accuracy: 0.9233\n","Epoch 134/1000\n","9264/9264 [==============================] - 2s 254us/sample - loss: 0.0549 - accuracy: 0.9935 - val_loss: 0.2686 - val_accuracy: 0.9233\n","Epoch 135/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0545 - accuracy: 0.9935 - val_loss: 0.2680 - val_accuracy: 0.9233\n","Epoch 136/1000\n","9264/9264 [==============================] - 2s 253us/sample - loss: 0.0540 - accuracy: 0.9935 - val_loss: 0.2677 - val_accuracy: 0.9233\n","Epoch 137/1000\n","9264/9264 [==============================] - 2s 264us/sample - loss: 0.0536 - accuracy: 0.9935 - val_loss: 0.2672 - val_accuracy: 0.9233\n","Epoch 138/1000\n","9264/9264 [==============================] - 2s 257us/sample - loss: 0.0531 - accuracy: 0.9935 - val_loss: 0.2669 - val_accuracy: 0.9214\n","Epoch 139/1000\n","9264/9264 [==============================] - 2s 252us/sample - loss: 0.0527 - accuracy: 0.9934 - val_loss: 0.2665 - val_accuracy: 0.9214\n","Epoch 140/1000\n","9264/9264 [==============================] - 2s 252us/sample - loss: 0.0523 - accuracy: 0.9936 - val_loss: 0.2661 - val_accuracy: 0.9214\n","Epoch 141/1000\n","9264/9264 [==============================] - 2s 243us/sample - loss: 0.0519 - accuracy: 0.9936 - val_loss: 0.2658 - val_accuracy: 0.9214\n","Epoch 142/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0515 - accuracy: 0.9936 - val_loss: 0.2655 - val_accuracy: 0.9204\n","Epoch 143/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0511 - accuracy: 0.9936 - val_loss: 0.2649 - val_accuracy: 0.9204\n","Epoch 144/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0507 - accuracy: 0.9935 - val_loss: 0.2645 - val_accuracy: 0.9204\n","Epoch 145/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0503 - accuracy: 0.9934 - val_loss: 0.2640 - val_accuracy: 0.9233\n","Epoch 146/1000\n","9264/9264 [==============================] - 2s 251us/sample - loss: 0.0499 - accuracy: 0.9935 - val_loss: 0.2636 - val_accuracy: 0.9204\n","Epoch 147/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0495 - accuracy: 0.9934 - val_loss: 0.2632 - val_accuracy: 0.9223\n","Epoch 148/1000\n","9264/9264 [==============================] - 2s 257us/sample - loss: 0.0491 - accuracy: 0.9936 - val_loss: 0.2629 - val_accuracy: 0.9243\n","Epoch 149/1000\n","9264/9264 [==============================] - 2s 269us/sample - loss: 0.0488 - accuracy: 0.9938 - val_loss: 0.2626 - val_accuracy: 0.9214\n","Epoch 150/1000\n","9264/9264 [==============================] - 2s 254us/sample - loss: 0.0484 - accuracy: 0.9938 - val_loss: 0.2622 - val_accuracy: 0.9233\n","Epoch 151/1000\n","9264/9264 [==============================] - 2s 252us/sample - loss: 0.0481 - accuracy: 0.9938 - val_loss: 0.2620 - val_accuracy: 0.9214\n","Epoch 152/1000\n","9264/9264 [==============================] - 2s 254us/sample - loss: 0.0477 - accuracy: 0.9938 - val_loss: 0.2616 - val_accuracy: 0.9223\n","Epoch 153/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0474 - accuracy: 0.9940 - val_loss: 0.2611 - val_accuracy: 0.9223\n","Epoch 154/1000\n","9264/9264 [==============================] - 2s 258us/sample - loss: 0.0470 - accuracy: 0.9938 - val_loss: 0.2608 - val_accuracy: 0.9223\n","Epoch 155/1000\n","9264/9264 [==============================] - 2s 262us/sample - loss: 0.0467 - accuracy: 0.9941 - val_loss: 0.2604 - val_accuracy: 0.9223\n","Epoch 156/1000\n","9264/9264 [==============================] - 2s 253us/sample - loss: 0.0463 - accuracy: 0.9941 - val_loss: 0.2601 - val_accuracy: 0.9223\n","Epoch 157/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0460 - accuracy: 0.9941 - val_loss: 0.2598 - val_accuracy: 0.9214\n","Epoch 158/1000\n","9264/9264 [==============================] - 2s 267us/sample - loss: 0.0457 - accuracy: 0.9940 - val_loss: 0.2595 - val_accuracy: 0.9214\n","Epoch 159/1000\n","9264/9264 [==============================] - 3s 290us/sample - loss: 0.0453 - accuracy: 0.9941 - val_loss: 0.2590 - val_accuracy: 0.9214\n","Epoch 160/1000\n","9264/9264 [==============================] - 3s 298us/sample - loss: 0.0450 - accuracy: 0.9941 - val_loss: 0.2587 - val_accuracy: 0.9214\n","Epoch 161/1000\n","9264/9264 [==============================] - 3s 302us/sample - loss: 0.0447 - accuracy: 0.9940 - val_loss: 0.2584 - val_accuracy: 0.9214\n","Epoch 162/1000\n","9264/9264 [==============================] - 3s 349us/sample - loss: 0.0444 - accuracy: 0.9940 - val_loss: 0.2581 - val_accuracy: 0.9214\n","Epoch 163/1000\n","9264/9264 [==============================] - 3s 345us/sample - loss: 0.0441 - accuracy: 0.9941 - val_loss: 0.2577 - val_accuracy: 0.9204\n","Epoch 164/1000\n","9264/9264 [==============================] - 3s 362us/sample - loss: 0.0438 - accuracy: 0.9941 - val_loss: 0.2573 - val_accuracy: 0.9194\n","Epoch 165/1000\n","9264/9264 [==============================] - 3s 304us/sample - loss: 0.0435 - accuracy: 0.9941 - val_loss: 0.2570 - val_accuracy: 0.9204\n","Epoch 166/1000\n","9264/9264 [==============================] - 3s 296us/sample - loss: 0.0432 - accuracy: 0.9940 - val_loss: 0.2567 - val_accuracy: 0.9194\n","Epoch 167/1000\n","9264/9264 [==============================] - 3s 295us/sample - loss: 0.0429 - accuracy: 0.9941 - val_loss: 0.2565 - val_accuracy: 0.9204\n","Epoch 168/1000\n","9264/9264 [==============================] - 3s 303us/sample - loss: 0.0427 - accuracy: 0.9940 - val_loss: 0.2561 - val_accuracy: 0.9204\n","Epoch 169/1000\n","9264/9264 [==============================] - 3s 314us/sample - loss: 0.0424 - accuracy: 0.9940 - val_loss: 0.2561 - val_accuracy: 0.9194\n","Epoch 170/1000\n","9264/9264 [==============================] - 3s 306us/sample - loss: 0.0421 - accuracy: 0.9940 - val_loss: 0.2557 - val_accuracy: 0.9194\n","Epoch 171/1000\n","9264/9264 [==============================] - 3s 307us/sample - loss: 0.0418 - accuracy: 0.9938 - val_loss: 0.2554 - val_accuracy: 0.9204\n","Epoch 172/1000\n","9264/9264 [==============================] - 3s 296us/sample - loss: 0.0415 - accuracy: 0.9940 - val_loss: 0.2552 - val_accuracy: 0.9204\n","Epoch 173/1000\n","9264/9264 [==============================] - 3s 302us/sample - loss: 0.0413 - accuracy: 0.9942 - val_loss: 0.2549 - val_accuracy: 0.9204\n","Epoch 174/1000\n","9264/9264 [==============================] - 3s 308us/sample - loss: 0.0410 - accuracy: 0.9942 - val_loss: 0.2545 - val_accuracy: 0.9204\n","Epoch 175/1000\n","9264/9264 [==============================] - 3s 323us/sample - loss: 0.0407 - accuracy: 0.9941 - val_loss: 0.2541 - val_accuracy: 0.9204\n","Epoch 176/1000\n","9264/9264 [==============================] - 4s 409us/sample - loss: 0.0405 - accuracy: 0.9942 - val_loss: 0.2539 - val_accuracy: 0.9214\n","Epoch 177/1000\n","9264/9264 [==============================] - 3s 284us/sample - loss: 0.0402 - accuracy: 0.9942 - val_loss: 0.2536 - val_accuracy: 0.9223\n","Epoch 178/1000\n","9264/9264 [==============================] - 2s 258us/sample - loss: 0.0400 - accuracy: 0.9943 - val_loss: 0.2534 - val_accuracy: 0.9252\n","Epoch 179/1000\n","9264/9264 [==============================] - 2s 264us/sample - loss: 0.0397 - accuracy: 0.9944 - val_loss: 0.2531 - val_accuracy: 0.9233\n","Epoch 180/1000\n","9264/9264 [==============================] - 2s 251us/sample - loss: 0.0395 - accuracy: 0.9943 - val_loss: 0.2529 - val_accuracy: 0.9233\n","Epoch 181/1000\n","9264/9264 [==============================] - 3s 272us/sample - loss: 0.0392 - accuracy: 0.9945 - val_loss: 0.2527 - val_accuracy: 0.9233\n","Epoch 182/1000\n","9264/9264 [==============================] - 3s 293us/sample - loss: 0.0390 - accuracy: 0.9945 - val_loss: 0.2526 - val_accuracy: 0.9233\n","Epoch 183/1000\n","9264/9264 [==============================] - 3s 284us/sample - loss: 0.0387 - accuracy: 0.9945 - val_loss: 0.2522 - val_accuracy: 0.9233\n","Epoch 184/1000\n","9264/9264 [==============================] - 2s 251us/sample - loss: 0.0385 - accuracy: 0.9947 - val_loss: 0.2519 - val_accuracy: 0.9252\n","Epoch 185/1000\n","9264/9264 [==============================] - 2s 255us/sample - loss: 0.0383 - accuracy: 0.9948 - val_loss: 0.2517 - val_accuracy: 0.9252\n","Epoch 186/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0381 - accuracy: 0.9948 - val_loss: 0.2515 - val_accuracy: 0.9252\n","Epoch 187/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0378 - accuracy: 0.9949 - val_loss: 0.2515 - val_accuracy: 0.9233\n","Epoch 188/1000\n","9264/9264 [==============================] - 2s 242us/sample - loss: 0.0376 - accuracy: 0.9949 - val_loss: 0.2513 - val_accuracy: 0.9233\n","Epoch 189/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0374 - accuracy: 0.9950 - val_loss: 0.2509 - val_accuracy: 0.9233\n","Epoch 190/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0372 - accuracy: 0.9950 - val_loss: 0.2506 - val_accuracy: 0.9252\n","Epoch 191/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0370 - accuracy: 0.9950 - val_loss: 0.2506 - val_accuracy: 0.9243\n","Epoch 192/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.0367 - accuracy: 0.9950 - val_loss: 0.2505 - val_accuracy: 0.9243\n","Epoch 193/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0365 - accuracy: 0.9953 - val_loss: 0.2503 - val_accuracy: 0.9243\n","Epoch 194/1000\n","9264/9264 [==============================] - 2s 255us/sample - loss: 0.0363 - accuracy: 0.9951 - val_loss: 0.2500 - val_accuracy: 0.9243\n","Epoch 195/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0361 - accuracy: 0.9951 - val_loss: 0.2499 - val_accuracy: 0.9243\n","Epoch 196/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0359 - accuracy: 0.9951 - val_loss: 0.2497 - val_accuracy: 0.9243\n","Epoch 197/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0357 - accuracy: 0.9951 - val_loss: 0.2494 - val_accuracy: 0.9243\n","Epoch 198/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0355 - accuracy: 0.9950 - val_loss: 0.2494 - val_accuracy: 0.9233\n","Epoch 199/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0353 - accuracy: 0.9951 - val_loss: 0.2493 - val_accuracy: 0.9243\n","Epoch 200/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0351 - accuracy: 0.9953 - val_loss: 0.2490 - val_accuracy: 0.9243\n","Epoch 201/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0349 - accuracy: 0.9953 - val_loss: 0.2488 - val_accuracy: 0.9243\n","Epoch 202/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0347 - accuracy: 0.9953 - val_loss: 0.2487 - val_accuracy: 0.9243\n","Epoch 203/1000\n","9264/9264 [==============================] - 2s 252us/sample - loss: 0.0345 - accuracy: 0.9953 - val_loss: 0.2484 - val_accuracy: 0.9243\n","Epoch 204/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0344 - accuracy: 0.9953 - val_loss: 0.2483 - val_accuracy: 0.9243\n","Epoch 205/1000\n","9264/9264 [==============================] - 2s 252us/sample - loss: 0.0342 - accuracy: 0.9953 - val_loss: 0.2481 - val_accuracy: 0.9243\n","Epoch 206/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.0340 - accuracy: 0.9953 - val_loss: 0.2478 - val_accuracy: 0.9243\n","Epoch 207/1000\n","9264/9264 [==============================] - 2s 241us/sample - loss: 0.0338 - accuracy: 0.9953 - val_loss: 0.2476 - val_accuracy: 0.9243\n","Epoch 208/1000\n","9264/9264 [==============================] - 2s 242us/sample - loss: 0.0336 - accuracy: 0.9951 - val_loss: 0.2473 - val_accuracy: 0.9233\n","Epoch 209/1000\n","9264/9264 [==============================] - 2s 243us/sample - loss: 0.0334 - accuracy: 0.9950 - val_loss: 0.2470 - val_accuracy: 0.9252\n","Epoch 210/1000\n","9264/9264 [==============================] - 2s 240us/sample - loss: 0.0333 - accuracy: 0.9950 - val_loss: 0.2468 - val_accuracy: 0.9243\n","Epoch 211/1000\n","9264/9264 [==============================] - 2s 240us/sample - loss: 0.0331 - accuracy: 0.9953 - val_loss: 0.2467 - val_accuracy: 0.9252\n","Epoch 212/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0329 - accuracy: 0.9951 - val_loss: 0.2465 - val_accuracy: 0.9252\n","Epoch 213/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0328 - accuracy: 0.9950 - val_loss: 0.2464 - val_accuracy: 0.9233\n","Epoch 214/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0326 - accuracy: 0.9949 - val_loss: 0.2463 - val_accuracy: 0.9252\n","Epoch 215/1000\n","9264/9264 [==============================] - 2s 242us/sample - loss: 0.0324 - accuracy: 0.9951 - val_loss: 0.2461 - val_accuracy: 0.9252\n","Epoch 216/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0322 - accuracy: 0.9950 - val_loss: 0.2460 - val_accuracy: 0.9252\n","Epoch 217/1000\n","9264/9264 [==============================] - 2s 239us/sample - loss: 0.0321 - accuracy: 0.9951 - val_loss: 0.2458 - val_accuracy: 0.9252\n","Epoch 218/1000\n","9264/9264 [==============================] - 2s 241us/sample - loss: 0.0319 - accuracy: 0.9953 - val_loss: 0.2458 - val_accuracy: 0.9243\n","Epoch 219/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.0318 - accuracy: 0.9953 - val_loss: 0.2456 - val_accuracy: 0.9243\n","Epoch 220/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0316 - accuracy: 0.9951 - val_loss: 0.2455 - val_accuracy: 0.9252\n","Epoch 221/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0315 - accuracy: 0.9953 - val_loss: 0.2453 - val_accuracy: 0.9252\n","Epoch 222/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0313 - accuracy: 0.9951 - val_loss: 0.2450 - val_accuracy: 0.9262\n","Epoch 223/1000\n","9264/9264 [==============================] - 2s 241us/sample - loss: 0.0311 - accuracy: 0.9951 - val_loss: 0.2449 - val_accuracy: 0.9243\n","Epoch 224/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0310 - accuracy: 0.9951 - val_loss: 0.2447 - val_accuracy: 0.9262\n","Epoch 225/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0308 - accuracy: 0.9953 - val_loss: 0.2446 - val_accuracy: 0.9262\n","Epoch 226/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0307 - accuracy: 0.9951 - val_loss: 0.2445 - val_accuracy: 0.9262\n","Epoch 227/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0305 - accuracy: 0.9951 - val_loss: 0.2443 - val_accuracy: 0.9262\n","Epoch 228/1000\n","9264/9264 [==============================] - 2s 241us/sample - loss: 0.0304 - accuracy: 0.9953 - val_loss: 0.2442 - val_accuracy: 0.9272\n","Epoch 229/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0302 - accuracy: 0.9954 - val_loss: 0.2441 - val_accuracy: 0.9272\n","Epoch 230/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0301 - accuracy: 0.9954 - val_loss: 0.2440 - val_accuracy: 0.9272\n","Epoch 231/1000\n","9264/9264 [==============================] - 2s 236us/sample - loss: 0.0300 - accuracy: 0.9954 - val_loss: 0.2440 - val_accuracy: 0.9272\n","Epoch 232/1000\n","9264/9264 [==============================] - 2s 237us/sample - loss: 0.0298 - accuracy: 0.9955 - val_loss: 0.2438 - val_accuracy: 0.9262\n","Epoch 233/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0297 - accuracy: 0.9955 - val_loss: 0.2436 - val_accuracy: 0.9252\n","Epoch 234/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.0295 - accuracy: 0.9955 - val_loss: 0.2435 - val_accuracy: 0.9252\n","Epoch 235/1000\n","9264/9264 [==============================] - 2s 257us/sample - loss: 0.0294 - accuracy: 0.9955 - val_loss: 0.2434 - val_accuracy: 0.9252\n","Epoch 236/1000\n","9264/9264 [==============================] - 2s 255us/sample - loss: 0.0293 - accuracy: 0.9955 - val_loss: 0.2433 - val_accuracy: 0.9243\n","Epoch 237/1000\n","9264/9264 [==============================] - 2s 252us/sample - loss: 0.0291 - accuracy: 0.9955 - val_loss: 0.2434 - val_accuracy: 0.9243\n","Epoch 238/1000\n","9264/9264 [==============================] - 2s 251us/sample - loss: 0.0290 - accuracy: 0.9954 - val_loss: 0.2432 - val_accuracy: 0.9243\n","Epoch 239/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0289 - accuracy: 0.9955 - val_loss: 0.2432 - val_accuracy: 0.9243\n","Epoch 240/1000\n","9264/9264 [==============================] - 2s 252us/sample - loss: 0.0287 - accuracy: 0.9956 - val_loss: 0.2432 - val_accuracy: 0.9223\n","Epoch 241/1000\n","9264/9264 [==============================] - 2s 240us/sample - loss: 0.0286 - accuracy: 0.9957 - val_loss: 0.2431 - val_accuracy: 0.9223\n","Epoch 242/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.0285 - accuracy: 0.9958 - val_loss: 0.2429 - val_accuracy: 0.9223\n","Epoch 243/1000\n","9264/9264 [==============================] - 2s 252us/sample - loss: 0.0284 - accuracy: 0.9958 - val_loss: 0.2428 - val_accuracy: 0.9223\n","Epoch 244/1000\n","9264/9264 [==============================] - 2s 254us/sample - loss: 0.0282 - accuracy: 0.9958 - val_loss: 0.2427 - val_accuracy: 0.9223\n","Epoch 245/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0281 - accuracy: 0.9957 - val_loss: 0.2426 - val_accuracy: 0.9223\n","Epoch 246/1000\n","9264/9264 [==============================] - 2s 253us/sample - loss: 0.0280 - accuracy: 0.9957 - val_loss: 0.2424 - val_accuracy: 0.9243\n","Epoch 247/1000\n","9264/9264 [==============================] - 2s 255us/sample - loss: 0.0278 - accuracy: 0.9958 - val_loss: 0.2424 - val_accuracy: 0.9223\n","Epoch 248/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0277 - accuracy: 0.9958 - val_loss: 0.2423 - val_accuracy: 0.9223\n","Epoch 249/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0276 - accuracy: 0.9958 - val_loss: 0.2423 - val_accuracy: 0.9223\n","Epoch 250/1000\n","9264/9264 [==============================] - 2s 253us/sample - loss: 0.0275 - accuracy: 0.9958 - val_loss: 0.2422 - val_accuracy: 0.9223\n","Epoch 251/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0274 - accuracy: 0.9957 - val_loss: 0.2420 - val_accuracy: 0.9223\n","Epoch 252/1000\n","9264/9264 [==============================] - 2s 252us/sample - loss: 0.0272 - accuracy: 0.9958 - val_loss: 0.2417 - val_accuracy: 0.9252\n","Epoch 253/1000\n","9264/9264 [==============================] - 2s 240us/sample - loss: 0.0271 - accuracy: 0.9956 - val_loss: 0.2417 - val_accuracy: 0.9243\n","Epoch 254/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0270 - accuracy: 0.9956 - val_loss: 0.2415 - val_accuracy: 0.9223\n","Epoch 255/1000\n","9264/9264 [==============================] - 2s 239us/sample - loss: 0.0269 - accuracy: 0.9957 - val_loss: 0.2415 - val_accuracy: 0.9233\n","Epoch 256/1000\n","9264/9264 [==============================] - 2s 241us/sample - loss: 0.0268 - accuracy: 0.9958 - val_loss: 0.2415 - val_accuracy: 0.9233\n","Epoch 257/1000\n","9264/9264 [==============================] - 2s 238us/sample - loss: 0.0267 - accuracy: 0.9958 - val_loss: 0.2415 - val_accuracy: 0.9223\n","Epoch 258/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0265 - accuracy: 0.9958 - val_loss: 0.2415 - val_accuracy: 0.9233\n","Epoch 259/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0264 - accuracy: 0.9958 - val_loss: 0.2412 - val_accuracy: 0.9233\n","Epoch 260/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0263 - accuracy: 0.9957 - val_loss: 0.2411 - val_accuracy: 0.9233\n","Epoch 261/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0262 - accuracy: 0.9958 - val_loss: 0.2409 - val_accuracy: 0.9233\n","Epoch 262/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0261 - accuracy: 0.9958 - val_loss: 0.2407 - val_accuracy: 0.9233\n","Epoch 263/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.0260 - accuracy: 0.9958 - val_loss: 0.2407 - val_accuracy: 0.9214\n","Epoch 264/1000\n","9264/9264 [==============================] - 2s 259us/sample - loss: 0.0259 - accuracy: 0.9957 - val_loss: 0.2405 - val_accuracy: 0.9233\n","Epoch 265/1000\n","9264/9264 [==============================] - 2s 254us/sample - loss: 0.0258 - accuracy: 0.9956 - val_loss: 0.2403 - val_accuracy: 0.9233\n","Epoch 266/1000\n","9264/9264 [==============================] - 2s 252us/sample - loss: 0.0257 - accuracy: 0.9956 - val_loss: 0.2402 - val_accuracy: 0.9233\n","Epoch 267/1000\n","9264/9264 [==============================] - 2s 251us/sample - loss: 0.0256 - accuracy: 0.9957 - val_loss: 0.2401 - val_accuracy: 0.9223\n","Epoch 268/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0255 - accuracy: 0.9957 - val_loss: 0.2400 - val_accuracy: 0.9223\n","Epoch 269/1000\n","9264/9264 [==============================] - 2s 253us/sample - loss: 0.0254 - accuracy: 0.9956 - val_loss: 0.2400 - val_accuracy: 0.9233\n","Epoch 270/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0253 - accuracy: 0.9958 - val_loss: 0.2400 - val_accuracy: 0.9204\n","Epoch 271/1000\n","9264/9264 [==============================] - 2s 254us/sample - loss: 0.0252 - accuracy: 0.9955 - val_loss: 0.2398 - val_accuracy: 0.9233\n","Epoch 272/1000\n","9264/9264 [==============================] - 2s 257us/sample - loss: 0.0251 - accuracy: 0.9957 - val_loss: 0.2397 - val_accuracy: 0.9233\n","Epoch 273/1000\n","9264/9264 [==============================] - 2s 253us/sample - loss: 0.0250 - accuracy: 0.9957 - val_loss: 0.2396 - val_accuracy: 0.9223\n","Epoch 274/1000\n","9264/9264 [==============================] - 2s 255us/sample - loss: 0.0249 - accuracy: 0.9957 - val_loss: 0.2396 - val_accuracy: 0.9214\n","Epoch 275/1000\n","9264/9264 [==============================] - 2s 256us/sample - loss: 0.0248 - accuracy: 0.9957 - val_loss: 0.2395 - val_accuracy: 0.9223\n","Epoch 276/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.0247 - accuracy: 0.9957 - val_loss: 0.2393 - val_accuracy: 0.9223\n","Epoch 277/1000\n","9264/9264 [==============================] - 2s 258us/sample - loss: 0.0246 - accuracy: 0.9958 - val_loss: 0.2392 - val_accuracy: 0.9223\n","Epoch 278/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0245 - accuracy: 0.9958 - val_loss: 0.2390 - val_accuracy: 0.9223\n","Epoch 279/1000\n","9264/9264 [==============================] - 2s 252us/sample - loss: 0.0244 - accuracy: 0.9958 - val_loss: 0.2391 - val_accuracy: 0.9223\n","Epoch 280/1000\n","9264/9264 [==============================] - 2s 241us/sample - loss: 0.0243 - accuracy: 0.9958 - val_loss: 0.2391 - val_accuracy: 0.9223\n","Epoch 281/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0242 - accuracy: 0.9957 - val_loss: 0.2390 - val_accuracy: 0.9223\n","Epoch 282/1000\n","9264/9264 [==============================] - 2s 255us/sample - loss: 0.0241 - accuracy: 0.9957 - val_loss: 0.2389 - val_accuracy: 0.9223\n","Epoch 283/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0240 - accuracy: 0.9958 - val_loss: 0.2389 - val_accuracy: 0.9223\n","Epoch 284/1000\n","9264/9264 [==============================] - 2s 251us/sample - loss: 0.0239 - accuracy: 0.9957 - val_loss: 0.2387 - val_accuracy: 0.9233\n","Epoch 285/1000\n","9264/9264 [==============================] - 2s 252us/sample - loss: 0.0239 - accuracy: 0.9957 - val_loss: 0.2385 - val_accuracy: 0.9233\n","Epoch 286/1000\n","9264/9264 [==============================] - 2s 255us/sample - loss: 0.0238 - accuracy: 0.9957 - val_loss: 0.2384 - val_accuracy: 0.9233\n","Epoch 287/1000\n","9264/9264 [==============================] - 2s 251us/sample - loss: 0.0237 - accuracy: 0.9958 - val_loss: 0.2382 - val_accuracy: 0.9233\n","Epoch 288/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0236 - accuracy: 0.9958 - val_loss: 0.2382 - val_accuracy: 0.9233\n","Epoch 289/1000\n","9264/9264 [==============================] - 2s 253us/sample - loss: 0.0235 - accuracy: 0.9958 - val_loss: 0.2382 - val_accuracy: 0.9233\n","Epoch 290/1000\n","9264/9264 [==============================] - 2s 254us/sample - loss: 0.0234 - accuracy: 0.9958 - val_loss: 0.2381 - val_accuracy: 0.9233\n","Epoch 291/1000\n","9264/9264 [==============================] - 2s 253us/sample - loss: 0.0233 - accuracy: 0.9958 - val_loss: 0.2380 - val_accuracy: 0.9233\n","Epoch 292/1000\n","9264/9264 [==============================] - 3s 285us/sample - loss: 0.0233 - accuracy: 0.9958 - val_loss: 0.2380 - val_accuracy: 0.9233\n","Epoch 293/1000\n","9264/9264 [==============================] - 3s 289us/sample - loss: 0.0232 - accuracy: 0.9958 - val_loss: 0.2379 - val_accuracy: 0.9233\n","Epoch 294/1000\n","9264/9264 [==============================] - 3s 279us/sample - loss: 0.0231 - accuracy: 0.9958 - val_loss: 0.2379 - val_accuracy: 0.9233\n","Epoch 295/1000\n","9264/9264 [==============================] - 3s 290us/sample - loss: 0.0230 - accuracy: 0.9958 - val_loss: 0.2378 - val_accuracy: 0.9233\n","Epoch 296/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0229 - accuracy: 0.9958 - val_loss: 0.2379 - val_accuracy: 0.9233\n","Epoch 297/1000\n","9264/9264 [==============================] - 2s 253us/sample - loss: 0.0228 - accuracy: 0.9957 - val_loss: 0.2379 - val_accuracy: 0.9214\n","Epoch 298/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0228 - accuracy: 0.9957 - val_loss: 0.2378 - val_accuracy: 0.9233\n","Epoch 299/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0227 - accuracy: 0.9958 - val_loss: 0.2376 - val_accuracy: 0.9223\n","Epoch 300/1000\n","9264/9264 [==============================] - 2s 251us/sample - loss: 0.0226 - accuracy: 0.9958 - val_loss: 0.2377 - val_accuracy: 0.9204\n","Epoch 301/1000\n","9264/9264 [==============================] - 2s 256us/sample - loss: 0.0225 - accuracy: 0.9957 - val_loss: 0.2377 - val_accuracy: 0.9204\n","Epoch 302/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0224 - accuracy: 0.9957 - val_loss: 0.2375 - val_accuracy: 0.9223\n","Epoch 303/1000\n","9264/9264 [==============================] - 2s 253us/sample - loss: 0.0224 - accuracy: 0.9958 - val_loss: 0.2374 - val_accuracy: 0.9223\n","Epoch 304/1000\n","9264/9264 [==============================] - 2s 258us/sample - loss: 0.0223 - accuracy: 0.9958 - val_loss: 0.2374 - val_accuracy: 0.9223\n","Epoch 305/1000\n","9264/9264 [==============================] - 2s 256us/sample - loss: 0.0222 - accuracy: 0.9958 - val_loss: 0.2374 - val_accuracy: 0.9233\n","Epoch 306/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.0221 - accuracy: 0.9958 - val_loss: 0.2374 - val_accuracy: 0.9233\n","Epoch 307/1000\n","9264/9264 [==============================] - 2s 253us/sample - loss: 0.0220 - accuracy: 0.9958 - val_loss: 0.2374 - val_accuracy: 0.9233\n","Epoch 308/1000\n","9264/9264 [==============================] - 2s 251us/sample - loss: 0.0220 - accuracy: 0.9957 - val_loss: 0.2374 - val_accuracy: 0.9233\n","Epoch 309/1000\n","9264/9264 [==============================] - 2s 259us/sample - loss: 0.0219 - accuracy: 0.9958 - val_loss: 0.2373 - val_accuracy: 0.9233\n","Epoch 310/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0218 - accuracy: 0.9958 - val_loss: 0.2373 - val_accuracy: 0.9233\n","Epoch 311/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0218 - accuracy: 0.9958 - val_loss: 0.2373 - val_accuracy: 0.9233\n","Epoch 312/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0217 - accuracy: 0.9958 - val_loss: 0.2372 - val_accuracy: 0.9233\n","Epoch 313/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0216 - accuracy: 0.9957 - val_loss: 0.2372 - val_accuracy: 0.9233\n","Epoch 314/1000\n","9264/9264 [==============================] - 3s 281us/sample - loss: 0.0216 - accuracy: 0.9958 - val_loss: 0.2372 - val_accuracy: 0.9233\n","Epoch 315/1000\n","9264/9264 [==============================] - 3s 277us/sample - loss: 0.0215 - accuracy: 0.9958 - val_loss: 0.2372 - val_accuracy: 0.9233\n","Epoch 316/1000\n","9264/9264 [==============================] - 2s 251us/sample - loss: 0.0214 - accuracy: 0.9957 - val_loss: 0.2372 - val_accuracy: 0.9233\n","Epoch 317/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.0213 - accuracy: 0.9958 - val_loss: 0.2372 - val_accuracy: 0.9233\n","Epoch 318/1000\n","9264/9264 [==============================] - 2s 242us/sample - loss: 0.0213 - accuracy: 0.9958 - val_loss: 0.2370 - val_accuracy: 0.9233\n","Epoch 319/1000\n","9264/9264 [==============================] - 2s 238us/sample - loss: 0.0212 - accuracy: 0.9958 - val_loss: 0.2370 - val_accuracy: 0.9252\n","Epoch 320/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.0211 - accuracy: 0.9958 - val_loss: 0.2369 - val_accuracy: 0.9252\n","Epoch 321/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0211 - accuracy: 0.9955 - val_loss: 0.2369 - val_accuracy: 0.9252\n","Epoch 322/1000\n","9264/9264 [==============================] - 2s 241us/sample - loss: 0.0210 - accuracy: 0.9957 - val_loss: 0.2369 - val_accuracy: 0.9252\n","Epoch 323/1000\n","9264/9264 [==============================] - 2s 241us/sample - loss: 0.0209 - accuracy: 0.9957 - val_loss: 0.2370 - val_accuracy: 0.9233\n","Epoch 324/1000\n","9264/9264 [==============================] - 2s 239us/sample - loss: 0.0209 - accuracy: 0.9958 - val_loss: 0.2368 - val_accuracy: 0.9233\n","Epoch 325/1000\n","9264/9264 [==============================] - 2s 238us/sample - loss: 0.0208 - accuracy: 0.9957 - val_loss: 0.2367 - val_accuracy: 0.9252\n","Epoch 326/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0207 - accuracy: 0.9957 - val_loss: 0.2366 - val_accuracy: 0.9252\n","Epoch 327/1000\n","9264/9264 [==============================] - 2s 252us/sample - loss: 0.0207 - accuracy: 0.9958 - val_loss: 0.2365 - val_accuracy: 0.9252\n","Epoch 328/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0206 - accuracy: 0.9958 - val_loss: 0.2365 - val_accuracy: 0.9252\n","Epoch 329/1000\n","9264/9264 [==============================] - 2s 242us/sample - loss: 0.0205 - accuracy: 0.9958 - val_loss: 0.2365 - val_accuracy: 0.9252\n","Epoch 330/1000\n","9264/9264 [==============================] - 2s 240us/sample - loss: 0.0205 - accuracy: 0.9958 - val_loss: 0.2364 - val_accuracy: 0.9252\n","Epoch 331/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0204 - accuracy: 0.9958 - val_loss: 0.2364 - val_accuracy: 0.9252\n","Epoch 332/1000\n","9264/9264 [==============================] - 2s 243us/sample - loss: 0.0203 - accuracy: 0.9958 - val_loss: 0.2365 - val_accuracy: 0.9252\n","Epoch 333/1000\n","9264/9264 [==============================] - 2s 242us/sample - loss: 0.0203 - accuracy: 0.9958 - val_loss: 0.2366 - val_accuracy: 0.9252\n","Epoch 334/1000\n","9264/9264 [==============================] - 2s 242us/sample - loss: 0.0202 - accuracy: 0.9958 - val_loss: 0.2365 - val_accuracy: 0.9252\n","Epoch 335/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.0202 - accuracy: 0.9957 - val_loss: 0.2365 - val_accuracy: 0.9233\n","Epoch 336/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0201 - accuracy: 0.9958 - val_loss: 0.2366 - val_accuracy: 0.9233\n","Epoch 337/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.0200 - accuracy: 0.9957 - val_loss: 0.2366 - val_accuracy: 0.9233\n","Epoch 338/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0200 - accuracy: 0.9958 - val_loss: 0.2365 - val_accuracy: 0.9233\n","Epoch 339/1000\n","9264/9264 [==============================] - 2s 243us/sample - loss: 0.0199 - accuracy: 0.9958 - val_loss: 0.2364 - val_accuracy: 0.9233\n","Epoch 340/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0198 - accuracy: 0.9957 - val_loss: 0.2365 - val_accuracy: 0.9233\n","Epoch 341/1000\n","9180/9264 [============================>.] - ETA: 0s - loss: 0.0198 - accuracy: 0.9958\n","Epoch 00341: Reducing Max LR on Plateau: new max lr will be 5e-05 (if not early_stopping).\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0198 - accuracy: 0.9958 - val_loss: 0.2366 - val_accuracy: 0.9233\n","Epoch 342/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0197 - accuracy: 0.9958 - val_loss: 0.2366 - val_accuracy: 0.9233\n","Epoch 343/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0197 - accuracy: 0.9957 - val_loss: 0.2366 - val_accuracy: 0.9233\n","Epoch 344/1000\n","9264/9264 [==============================] - 2s 242us/sample - loss: 0.0196 - accuracy: 0.9958 - val_loss: 0.2366 - val_accuracy: 0.9233\n","Epoch 345/1000\n","9264/9264 [==============================] - 2s 238us/sample - loss: 0.0196 - accuracy: 0.9958 - val_loss: 0.2366 - val_accuracy: 0.9223\n","Epoch 346/1000\n","9264/9264 [==============================] - 2s 232us/sample - loss: 0.0196 - accuracy: 0.9958 - val_loss: 0.2366 - val_accuracy: 0.9223\n","Epoch 347/1000\n","9264/9264 [==============================] - 2s 243us/sample - loss: 0.0196 - accuracy: 0.9958 - val_loss: 0.2366 - val_accuracy: 0.9223\n","Epoch 348/1000\n","9264/9264 [==============================] - 2s 243us/sample - loss: 0.0195 - accuracy: 0.9958 - val_loss: 0.2366 - val_accuracy: 0.9214\n","Epoch 349/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0195 - accuracy: 0.9958 - val_loss: 0.2366 - val_accuracy: 0.9214\n","Epoch 350/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0195 - accuracy: 0.9958 - val_loss: 0.2366 - val_accuracy: 0.9214\n","Epoch 351/1000\n","9140/9264 [============================>.] - ETA: 0s - loss: 0.0195 - accuracy: 0.9958\n","Epoch 00351: Reducing Max LR on Plateau: new max lr will be 2.5e-05 (if not early_stopping).\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0194 - accuracy: 0.9958 - val_loss: 0.2365 - val_accuracy: 0.9214\n","Epoch 352/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0194 - accuracy: 0.9958 - val_loss: 0.2365 - val_accuracy: 0.9214\n","Epoch 353/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.0194 - accuracy: 0.9958 - val_loss: 0.2364 - val_accuracy: 0.9214\n","Epoch 354/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0194 - accuracy: 0.9958 - val_loss: 0.2364 - val_accuracy: 0.9214\n","Epoch 355/1000\n","9264/9264 [==============================] - 2s 238us/sample - loss: 0.0193 - accuracy: 0.9958 - val_loss: 0.2364 - val_accuracy: 0.9214\n","Epoch 356/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0193 - accuracy: 0.9958 - val_loss: 0.2364 - val_accuracy: 0.9214\n","Epoch 357/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0193 - accuracy: 0.9958 - val_loss: 0.2364 - val_accuracy: 0.9214\n","Epoch 358/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0193 - accuracy: 0.9958 - val_loss: 0.2364 - val_accuracy: 0.9214\n","Epoch 359/1000\n","9264/9264 [==============================] - 2s 243us/sample - loss: 0.0193 - accuracy: 0.9958 - val_loss: 0.2364 - val_accuracy: 0.9214\n","Epoch 360/1000\n","9264/9264 [==============================] - 2s 242us/sample - loss: 0.0193 - accuracy: 0.9958 - val_loss: 0.2364 - val_accuracy: 0.9214\n","Epoch 361/1000\n","9264/9264 [==============================] - 2s 241us/sample - loss: 0.0192 - accuracy: 0.9958 - val_loss: 0.2363 - val_accuracy: 0.9214\n","Epoch 362/1000\n","9264/9264 [==============================] - 2s 239us/sample - loss: 0.0192 - accuracy: 0.9958 - val_loss: 0.2363 - val_accuracy: 0.9214\n","Epoch 363/1000\n","9264/9264 [==============================] - 2s 239us/sample - loss: 0.0192 - accuracy: 0.9958 - val_loss: 0.2363 - val_accuracy: 0.9214\n","Epoch 364/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0192 - accuracy: 0.9958 - val_loss: 0.2363 - val_accuracy: 0.9214\n","Epoch 365/1000\n","9264/9264 [==============================] - 2s 242us/sample - loss: 0.0192 - accuracy: 0.9958 - val_loss: 0.2363 - val_accuracy: 0.9214\n","Epoch 366/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0192 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 367/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0191 - accuracy: 0.9958 - val_loss: 0.2363 - val_accuracy: 0.9214\n","Epoch 368/1000\n","9264/9264 [==============================] - 2s 251us/sample - loss: 0.0191 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 369/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0191 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 370/1000\n","9264/9264 [==============================] - 2s 240us/sample - loss: 0.0191 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 371/1000\n","9264/9264 [==============================] - 2s 239us/sample - loss: 0.0191 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 372/1000\n","9264/9264 [==============================] - 2s 241us/sample - loss: 0.0191 - accuracy: 0.9957 - val_loss: 0.2362 - val_accuracy: 0.9233\n","Epoch 373/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0190 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 374/1000\n","9264/9264 [==============================] - 2s 242us/sample - loss: 0.0190 - accuracy: 0.9957 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 375/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0190 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 376/1000\n","9264/9264 [==============================] - 2s 241us/sample - loss: 0.0190 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 377/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0190 - accuracy: 0.9958 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 378/1000\n","9264/9264 [==============================] - 2s 241us/sample - loss: 0.0190 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 379/1000\n","9264/9264 [==============================] - 2s 241us/sample - loss: 0.0190 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 380/1000\n","9264/9264 [==============================] - 2s 238us/sample - loss: 0.0189 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 381/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0189 - accuracy: 0.9958 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 382/1000\n","9264/9264 [==============================] - 2s 238us/sample - loss: 0.0189 - accuracy: 0.9958 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 383/1000\n","9264/9264 [==============================] - 2s 235us/sample - loss: 0.0189 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 384/1000\n","9264/9264 [==============================] - 2s 241us/sample - loss: 0.0189 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 385/1000\n","9264/9264 [==============================] - 2s 239us/sample - loss: 0.0189 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 386/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0188 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 387/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0188 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 388/1000\n","9264/9264 [==============================] - 2s 237us/sample - loss: 0.0188 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 389/1000\n","9264/9264 [==============================] - 2s 241us/sample - loss: 0.0188 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 390/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0188 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 391/1000\n","9260/9264 [============================>.] - ETA: 0s - loss: 0.0188 - accuracy: 0.9958\n","Epoch 00391: Reducing Max LR on Plateau: new max lr will be 1.25e-05 (if not early_stopping).\n","9264/9264 [==============================] - 2s 242us/sample - loss: 0.0188 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 392/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.0187 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 393/1000\n","9264/9264 [==============================] - 2s 241us/sample - loss: 0.0187 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 394/1000\n","9264/9264 [==============================] - 2s 243us/sample - loss: 0.0187 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 395/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0187 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 396/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0187 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 397/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0187 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 398/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.0187 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 399/1000\n","9264/9264 [==============================] - 2s 251us/sample - loss: 0.0187 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 400/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0187 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 401/1000\n","9180/9264 [============================>.] - ETA: 0s - loss: 0.0186 - accuracy: 0.9959\n","Epoch 00401: Reducing Max LR on Plateau: new max lr will be 6.25e-06 (if not early_stopping).\n","9264/9264 [==============================] - 2s 253us/sample - loss: 0.0187 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 402/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0187 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 403/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0187 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 404/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0186 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 405/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0186 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 406/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0186 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 407/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0186 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 408/1000\n","9264/9264 [==============================] - 2s 252us/sample - loss: 0.0186 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 409/1000\n","9264/9264 [==============================] - 2s 253us/sample - loss: 0.0186 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 410/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0186 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 411/1000\n","9120/9264 [============================>.] - ETA: 0s - loss: 0.0187 - accuracy: 0.9957\n","Epoch 00411: Reducing Max LR on Plateau: new max lr will be 3.125e-06 (if not early_stopping).\n","9264/9264 [==============================] - 2s 253us/sample - loss: 0.0186 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 412/1000\n","9264/9264 [==============================] - 2s 253us/sample - loss: 0.0186 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 413/1000\n","9264/9264 [==============================] - 2s 251us/sample - loss: 0.0186 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 414/1000\n","9264/9264 [==============================] - 2s 238us/sample - loss: 0.0186 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 415/1000\n","9264/9264 [==============================] - 2s 240us/sample - loss: 0.0186 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 416/1000\n","9264/9264 [==============================] - 2s 241us/sample - loss: 0.0186 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 417/1000\n","9264/9264 [==============================] - 2s 243us/sample - loss: 0.0186 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 418/1000\n","9264/9264 [==============================] - 2s 241us/sample - loss: 0.0186 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 419/1000\n","9264/9264 [==============================] - 2s 243us/sample - loss: 0.0186 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 420/1000\n","9264/9264 [==============================] - 2s 240us/sample - loss: 0.0186 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 421/1000\n","9220/9264 [============================>.] - ETA: 0s - loss: 0.0186 - accuracy: 0.9958\n","Epoch 00421: Reducing Max LR on Plateau: new max lr will be 1.5625e-06 (if not early_stopping).\n","9264/9264 [==============================] - 2s 237us/sample - loss: 0.0186 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 422/1000\n","9264/9264 [==============================] - 2s 242us/sample - loss: 0.0186 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 423/1000\n","9264/9264 [==============================] - 2s 239us/sample - loss: 0.0186 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 424/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0186 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 425/1000\n","9264/9264 [==============================] - 2s 240us/sample - loss: 0.0186 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 426/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0185 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 427/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0185 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 428/1000\n","9264/9264 [==============================] - 2s 267us/sample - loss: 0.0185 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 429/1000\n","9264/9264 [==============================] - 3s 281us/sample - loss: 0.0185 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 430/1000\n","9264/9264 [==============================] - 3s 280us/sample - loss: 0.0185 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 431/1000\n","9080/9264 [============================>.] - ETA: 0s - loss: 0.0184 - accuracy: 0.9958\n","Epoch 00431: Reducing Max LR on Plateau: new max lr will be 7.8125e-07 (if not early_stopping).\n","9264/9264 [==============================] - 3s 271us/sample - loss: 0.0185 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 432/1000\n","9264/9264 [==============================] - 2s 257us/sample - loss: 0.0185 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 433/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.0185 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 434/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0185 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 435/1000\n","9264/9264 [==============================] - 2s 251us/sample - loss: 0.0185 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 436/1000\n","9264/9264 [==============================] - 2s 252us/sample - loss: 0.0185 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 437/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.0185 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 438/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0185 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 439/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0185 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 440/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0185 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 441/1000\n","9100/9264 [============================>.] - ETA: 0s - loss: 0.0186 - accuracy: 0.9957\n","Epoch 00441: Reducing Max LR on Plateau: new max lr will be 3.90625e-07 (if not early_stopping).\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0185 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 442/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0185 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 443/1000\n","9264/9264 [==============================] - 2s 251us/sample - loss: 0.0185 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 444/1000\n","9264/9264 [==============================] - 2s 254us/sample - loss: 0.0185 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 445/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0185 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 446/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0185 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 447/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0185 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 448/1000\n","9264/9264 [==============================] - 2s 268us/sample - loss: 0.0185 - accuracy: 0.9957 - val_loss: 0.2362 - val_accuracy: 0.9204\n","Epoch 449/1000\n","9264/9264 [==============================] - 3s 282us/sample - loss: 0.0185 - accuracy: 0.9957 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 450/1000\n","9264/9264 [==============================] - 3s 271us/sample - loss: 0.0185 - accuracy: 0.9957 - val_loss: 0.2362 - val_accuracy: 0.9204\n","Epoch 451/1000\n","9080/9264 [============================>.] - ETA: 0s - loss: 0.0185 - accuracy: 0.9958\n","Epoch 00451: Reducing Max LR on Plateau: new max lr will be 1.953125e-07 (if not early_stopping).\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0185 - accuracy: 0.9957 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 452/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.0185 - accuracy: 0.9957 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 453/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.0185 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 454/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.0185 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 455/1000\n","9264/9264 [==============================] - 2s 255us/sample - loss: 0.0185 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 456/1000\n","9264/9264 [==============================] - 2s 258us/sample - loss: 0.0185 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 457/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0185 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 458/1000\n","9264/9264 [==============================] - 2s 255us/sample - loss: 0.0185 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 459/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0185 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 460/1000\n","9264/9264 [==============================] - 2s 254us/sample - loss: 0.0185 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 461/1000\n","9200/9264 [============================>.] - ETA: 0s - loss: 0.0183 - accuracy: 0.9959\n","Epoch 00461: Reducing Max LR on Plateau: new max lr will be 9.765625e-08 (if not early_stopping).\n","9264/9264 [==============================] - 2s 252us/sample - loss: 0.0185 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 462/1000\n","9264/9264 [==============================] - 2s 260us/sample - loss: 0.0185 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 463/1000\n","9264/9264 [==============================] - 2s 259us/sample - loss: 0.0185 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 464/1000\n","9264/9264 [==============================] - 2s 255us/sample - loss: 0.0184 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 465/1000\n","9264/9264 [==============================] - 2s 255us/sample - loss: 0.0184 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 466/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0184 - accuracy: 0.9958 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 467/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0184 - accuracy: 0.9958 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 468/1000\n","9264/9264 [==============================] - 2s 255us/sample - loss: 0.0184 - accuracy: 0.9958 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 469/1000\n","9264/9264 [==============================] - 2s 252us/sample - loss: 0.0184 - accuracy: 0.9958 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 470/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0184 - accuracy: 0.9958 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 471/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0184 - accuracy: 0.9958 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 472/1000\n","9264/9264 [==============================] - 2s 251us/sample - loss: 0.0184 - accuracy: 0.9957 - val_loss: 0.2361 - val_accuracy: 0.9204\n","Epoch 473/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0184 - accuracy: 0.9958 - val_loss: 0.2361 - val_accuracy: 0.9204\n","Epoch 474/1000\n","9264/9264 [==============================] - 2s 252us/sample - loss: 0.0184 - accuracy: 0.9958 - val_loss: 0.2361 - val_accuracy: 0.9204\n","Epoch 475/1000\n","9264/9264 [==============================] - 2s 252us/sample - loss: 0.0184 - accuracy: 0.9958 - val_loss: 0.2361 - val_accuracy: 0.9204\n","Epoch 476/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0184 - accuracy: 0.9958 - val_loss: 0.2361 - val_accuracy: 0.9204\n","Epoch 477/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0184 - accuracy: 0.9958 - val_loss: 0.2361 - val_accuracy: 0.9204\n","Epoch 478/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0184 - accuracy: 0.9958 - val_loss: 0.2361 - val_accuracy: 0.9204\n","Epoch 479/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0184 - accuracy: 0.9958 - val_loss: 0.2361 - val_accuracy: 0.9204\n","Epoch 480/1000\n","9264/9264 [==============================] - 2s 252us/sample - loss: 0.0184 - accuracy: 0.9958 - val_loss: 0.2361 - val_accuracy: 0.9204\n","Epoch 481/1000\n","9264/9264 [==============================] - 2s 251us/sample - loss: 0.0184 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9204\n","Epoch 482/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.0184 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9204\n","Epoch 483/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0184 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9204\n","Epoch 484/1000\n","9264/9264 [==============================] - 2s 242us/sample - loss: 0.0184 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9204\n","Epoch 485/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0184 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9204\n","Epoch 486/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0184 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9204\n","Epoch 487/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.0184 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9204\n","Epoch 488/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0184 - accuracy: 0.9957 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 489/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0184 - accuracy: 0.9956 - val_loss: 0.2362 - val_accuracy: 0.9204\n","Epoch 490/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0184 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9204\n","Epoch 491/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0184 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9204\n","Epoch 492/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.0184 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9204\n","Epoch 493/1000\n","9264/9264 [==============================] - 2s 252us/sample - loss: 0.0184 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9204\n","Epoch 494/1000\n","9264/9264 [==============================] - 2s 253us/sample - loss: 0.0184 - accuracy: 0.9957 - val_loss: 0.2362 - val_accuracy: 0.9204\n","Epoch 495/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.0184 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9204\n","Epoch 496/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0184 - accuracy: 0.9957 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 497/1000\n","9264/9264 [==============================] - 2s 254us/sample - loss: 0.0184 - accuracy: 0.9956 - val_loss: 0.2362 - val_accuracy: 0.9204\n","Epoch 498/1000\n","9264/9264 [==============================] - 2s 253us/sample - loss: 0.0184 - accuracy: 0.9957 - val_loss: 0.2362 - val_accuracy: 0.9204\n","Epoch 499/1000\n","9264/9264 [==============================] - 2s 251us/sample - loss: 0.0184 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9204\n","Epoch 500/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0184 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9204\n","Epoch 501/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0184 - accuracy: 0.9957 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 502/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0184 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 503/1000\n","9264/9264 [==============================] - 2s 253us/sample - loss: 0.0184 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 504/1000\n","9264/9264 [==============================] - 2s 251us/sample - loss: 0.0184 - accuracy: 0.9957 - val_loss: 0.2362 - val_accuracy: 0.9204\n","Epoch 505/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.0184 - accuracy: 0.9957 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 506/1000\n","9264/9264 [==============================] - 2s 241us/sample - loss: 0.0183 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9204\n","Epoch 507/1000\n","9264/9264 [==============================] - 2s 253us/sample - loss: 0.0183 - accuracy: 0.9957 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 508/1000\n","9264/9264 [==============================] - 2s 252us/sample - loss: 0.0183 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9204\n","Epoch 509/1000\n","9264/9264 [==============================] - 2s 240us/sample - loss: 0.0183 - accuracy: 0.9957 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 510/1000\n","9264/9264 [==============================] - 2s 241us/sample - loss: 0.0183 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 511/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0183 - accuracy: 0.9957 - val_loss: 0.2362 - val_accuracy: 0.9204\n","Epoch 512/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0183 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 513/1000\n","9264/9264 [==============================] - 2s 243us/sample - loss: 0.0183 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 514/1000\n","9264/9264 [==============================] - 2s 242us/sample - loss: 0.0183 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 515/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0183 - accuracy: 0.9958 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 516/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0183 - accuracy: 0.9958 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 517/1000\n","9264/9264 [==============================] - 2s 239us/sample - loss: 0.0183 - accuracy: 0.9958 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 518/1000\n","9264/9264 [==============================] - 2s 239us/sample - loss: 0.0183 - accuracy: 0.9958 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 519/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0183 - accuracy: 0.9958 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 520/1000\n","9264/9264 [==============================] - 2s 241us/sample - loss: 0.0183 - accuracy: 0.9958 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 521/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0183 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 522/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0183 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 523/1000\n","9264/9264 [==============================] - 2s 252us/sample - loss: 0.0183 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 524/1000\n","9264/9264 [==============================] - 2s 252us/sample - loss: 0.0183 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 525/1000\n","9264/9264 [==============================] - 2s 253us/sample - loss: 0.0183 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 526/1000\n","9264/9264 [==============================] - 2s 254us/sample - loss: 0.0183 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 527/1000\n","9264/9264 [==============================] - 2s 251us/sample - loss: 0.0183 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 528/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0183 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 529/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0183 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 530/1000\n","9264/9264 [==============================] - 2s 241us/sample - loss: 0.0183 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 531/1000\n","9264/9264 [==============================] - 2s 254us/sample - loss: 0.0183 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 532/1000\n","9264/9264 [==============================] - 2s 256us/sample - loss: 0.0183 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 533/1000\n","9264/9264 [==============================] - 2s 259us/sample - loss: 0.0183 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 534/1000\n","9264/9264 [==============================] - 2s 263us/sample - loss: 0.0183 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 535/1000\n","9264/9264 [==============================] - 2s 252us/sample - loss: 0.0183 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 536/1000\n","9264/9264 [==============================] - 2s 251us/sample - loss: 0.0183 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 537/1000\n","9264/9264 [==============================] - 2s 252us/sample - loss: 0.0183 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 538/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0183 - accuracy: 0.9957 - val_loss: 0.2362 - val_accuracy: 0.9204\n","Epoch 539/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0183 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9204\n","Epoch 540/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0183 - accuracy: 0.9957 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 541/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0183 - accuracy: 0.9957 - val_loss: 0.2362 - val_accuracy: 0.9204\n","Epoch 542/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0183 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9204\n","Epoch 543/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0183 - accuracy: 0.9957 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 544/1000\n","9264/9264 [==============================] - 2s 238us/sample - loss: 0.0183 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 545/1000\n","9264/9264 [==============================] - 2s 241us/sample - loss: 0.0183 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 546/1000\n","9264/9264 [==============================] - 2s 239us/sample - loss: 0.0183 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 547/1000\n","9264/9264 [==============================] - 2s 241us/sample - loss: 0.0183 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 548/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0183 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 549/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0182 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 550/1000\n","9264/9264 [==============================] - 2s 240us/sample - loss: 0.0182 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 551/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0182 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 552/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0182 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 553/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.0182 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 554/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0182 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 555/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0182 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 556/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0182 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 557/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0182 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 558/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0182 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 559/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0182 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 560/1000\n","9264/9264 [==============================] - 2s 255us/sample - loss: 0.0182 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 561/1000\n","9264/9264 [==============================] - 2s 254us/sample - loss: 0.0182 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 562/1000\n","9264/9264 [==============================] - 2s 265us/sample - loss: 0.0182 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 563/1000\n","9264/9264 [==============================] - 3s 283us/sample - loss: 0.0182 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 564/1000\n","9264/9264 [==============================] - 3s 282us/sample - loss: 0.0182 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 565/1000\n","9264/9264 [==============================] - 3s 275us/sample - loss: 0.0182 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 566/1000\n","9264/9264 [==============================] - 2s 266us/sample - loss: 0.0182 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 567/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0182 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 568/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0182 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 569/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0182 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 570/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0182 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 571/1000\n","9264/9264 [==============================] - 2s 255us/sample - loss: 0.0182 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 572/1000\n","9264/9264 [==============================] - 4s 382us/sample - loss: 0.0182 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 573/1000\n","9264/9264 [==============================] - 3s 284us/sample - loss: 0.0182 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 574/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.0182 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 575/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0182 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 576/1000\n","9264/9264 [==============================] - 2s 251us/sample - loss: 0.0182 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 577/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0182 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 578/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0182 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 579/1000\n","9264/9264 [==============================] - 2s 255us/sample - loss: 0.0182 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 580/1000\n","9264/9264 [==============================] - 3s 285us/sample - loss: 0.0182 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 581/1000\n","9264/9264 [==============================] - 3s 280us/sample - loss: 0.0182 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 582/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0182 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 583/1000\n","9264/9264 [==============================] - 3s 294us/sample - loss: 0.0182 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 584/1000\n","9264/9264 [==============================] - 2s 256us/sample - loss: 0.0182 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 585/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0182 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 586/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0182 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 587/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0182 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 588/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0182 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 589/1000\n","9264/9264 [==============================] - 2s 241us/sample - loss: 0.0182 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 590/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0182 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 591/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0181 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 592/1000\n","9264/9264 [==============================] - 2s 239us/sample - loss: 0.0181 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 593/1000\n","9264/9264 [==============================] - 2s 240us/sample - loss: 0.0181 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 594/1000\n","9264/9264 [==============================] - 2s 252us/sample - loss: 0.0181 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 595/1000\n","9264/9264 [==============================] - 2s 251us/sample - loss: 0.0181 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 596/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0181 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 597/1000\n","9264/9264 [==============================] - 2s 241us/sample - loss: 0.0181 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 598/1000\n","9264/9264 [==============================] - 2s 243us/sample - loss: 0.0181 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 599/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0181 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 600/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0181 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 601/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0181 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 602/1000\n","9264/9264 [==============================] - 2s 243us/sample - loss: 0.0181 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 603/1000\n","9264/9264 [==============================] - 2s 240us/sample - loss: 0.0181 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 604/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0181 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 605/1000\n","9264/9264 [==============================] - 2s 239us/sample - loss: 0.0181 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 606/1000\n","9264/9264 [==============================] - 2s 241us/sample - loss: 0.0181 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 607/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0181 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 608/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0181 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 609/1000\n","9264/9264 [==============================] - 2s 242us/sample - loss: 0.0181 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 610/1000\n","9264/9264 [==============================] - 2s 242us/sample - loss: 0.0181 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 611/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0181 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 612/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0181 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 613/1000\n","9264/9264 [==============================] - 2s 253us/sample - loss: 0.0181 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 614/1000\n","9264/9264 [==============================] - 2s 260us/sample - loss: 0.0181 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 615/1000\n","9264/9264 [==============================] - 2s 255us/sample - loss: 0.0181 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 616/1000\n","9264/9264 [==============================] - 2s 253us/sample - loss: 0.0181 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 617/1000\n","9264/9264 [==============================] - 2s 254us/sample - loss: 0.0181 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 618/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0181 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 619/1000\n","9264/9264 [==============================] - 2s 253us/sample - loss: 0.0181 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 620/1000\n","9264/9264 [==============================] - 2s 243us/sample - loss: 0.0181 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 621/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0181 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 622/1000\n","9264/9264 [==============================] - 2s 251us/sample - loss: 0.0181 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 623/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0181 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 624/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0181 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 625/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0181 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 626/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.0181 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 627/1000\n","9264/9264 [==============================] - 2s 238us/sample - loss: 0.0181 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 628/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0181 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 629/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0181 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 630/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0181 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 631/1000\n","9264/9264 [==============================] - 2s 243us/sample - loss: 0.0181 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 632/1000\n","9264/9264 [==============================] - 2s 242us/sample - loss: 0.0181 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 633/1000\n","9264/9264 [==============================] - 2s 242us/sample - loss: 0.0181 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 634/1000\n","9264/9264 [==============================] - 2s 243us/sample - loss: 0.0180 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 635/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0180 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 636/1000\n","9264/9264 [==============================] - 2s 254us/sample - loss: 0.0180 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 637/1000\n","9264/9264 [==============================] - 2s 254us/sample - loss: 0.0180 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 638/1000\n","9264/9264 [==============================] - 2s 243us/sample - loss: 0.0180 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 639/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0180 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 640/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0180 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 641/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0180 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 642/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.0180 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 643/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0180 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 644/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.0180 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 645/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0180 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 646/1000\n","9264/9264 [==============================] - 2s 251us/sample - loss: 0.0180 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 647/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.0180 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 648/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0180 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 649/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0180 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 650/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0180 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 651/1000\n","9264/9264 [==============================] - 2s 241us/sample - loss: 0.0180 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 652/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0180 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 653/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0180 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 654/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0180 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 655/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0180 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 656/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0180 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 657/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0180 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 658/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0180 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 659/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0180 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 660/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0180 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 661/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0180 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 662/1000\n","9264/9264 [==============================] - 2s 253us/sample - loss: 0.0180 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 663/1000\n","9264/9264 [==============================] - 2s 252us/sample - loss: 0.0180 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 664/1000\n","9264/9264 [==============================] - 2s 243us/sample - loss: 0.0180 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 665/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0180 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 666/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0180 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 667/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0180 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 668/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0180 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 669/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0180 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 670/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0180 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 671/1000\n","9264/9264 [==============================] - 2s 243us/sample - loss: 0.0180 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 672/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0180 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 673/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0180 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 674/1000\n","9264/9264 [==============================] - 2s 241us/sample - loss: 0.0180 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 675/1000\n","9264/9264 [==============================] - 2s 241us/sample - loss: 0.0180 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 676/1000\n","9264/9264 [==============================] - 2s 243us/sample - loss: 0.0180 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 677/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0180 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 678/1000\n","9264/9264 [==============================] - 2s 240us/sample - loss: 0.0180 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 679/1000\n","9264/9264 [==============================] - 2s 242us/sample - loss: 0.0179 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 680/1000\n","9264/9264 [==============================] - 2s 240us/sample - loss: 0.0179 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 681/1000\n","9264/9264 [==============================] - 2s 240us/sample - loss: 0.0179 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 682/1000\n","9264/9264 [==============================] - 2s 241us/sample - loss: 0.0179 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 683/1000\n","9264/9264 [==============================] - 2s 236us/sample - loss: 0.0179 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 684/1000\n","9264/9264 [==============================] - 2s 242us/sample - loss: 0.0179 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 685/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0179 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 686/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0179 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 687/1000\n","9264/9264 [==============================] - 2s 242us/sample - loss: 0.0179 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 688/1000\n","9264/9264 [==============================] - 2s 243us/sample - loss: 0.0179 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 689/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0179 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 690/1000\n","9264/9264 [==============================] - 2s 242us/sample - loss: 0.0179 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 691/1000\n","9264/9264 [==============================] - 2s 242us/sample - loss: 0.0179 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 692/1000\n","9264/9264 [==============================] - 2s 242us/sample - loss: 0.0179 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 693/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.0179 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 694/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0179 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 695/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0179 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 696/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0179 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 697/1000\n","9264/9264 [==============================] - 3s 272us/sample - loss: 0.0179 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 698/1000\n","9264/9264 [==============================] - 3s 285us/sample - loss: 0.0179 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 699/1000\n","9264/9264 [==============================] - 3s 283us/sample - loss: 0.0179 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 700/1000\n","9264/9264 [==============================] - 3s 280us/sample - loss: 0.0179 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 701/1000\n","9264/9264 [==============================] - 2s 257us/sample - loss: 0.0179 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 702/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0179 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 703/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0179 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 704/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0179 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 705/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.0179 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 706/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0179 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 707/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0179 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 708/1000\n","9264/9264 [==============================] - 2s 251us/sample - loss: 0.0179 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 709/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0179 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 710/1000\n","9264/9264 [==============================] - 2s 239us/sample - loss: 0.0179 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 711/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0179 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 712/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0179 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 713/1000\n","9264/9264 [==============================] - 2s 257us/sample - loss: 0.0179 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 714/1000\n","9264/9264 [==============================] - 3s 273us/sample - loss: 0.0179 - accuracy: 0.9957 - val_loss: 0.2361 - val_accuracy: 0.9214\n","Epoch 715/1000\n","9264/9264 [==============================] - 3s 271us/sample - loss: 0.0179 - accuracy: 0.9958 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 716/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0179 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 717/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0179 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 718/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0179 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 719/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0179 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 720/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.0179 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 721/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.0179 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 722/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0179 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 723/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0178 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 724/1000\n","9264/9264 [==============================] - 2s 239us/sample - loss: 0.0178 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 725/1000\n","9264/9264 [==============================] - 2s 241us/sample - loss: 0.0178 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 726/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0178 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 727/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0178 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 728/1000\n","9264/9264 [==============================] - 2s 239us/sample - loss: 0.0178 - accuracy: 0.9959 - val_loss: 0.2361 - val_accuracy: 0.9223\n","Epoch 729/1000\n","9264/9264 [==============================] - 2s 236us/sample - loss: 0.0178 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 730/1000\n","9264/9264 [==============================] - 2s 233us/sample - loss: 0.0178 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 731/1000\n","9264/9264 [==============================] - 2s 236us/sample - loss: 0.0178 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 732/1000\n","9264/9264 [==============================] - 2s 234us/sample - loss: 0.0178 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 733/1000\n","9264/9264 [==============================] - 2s 238us/sample - loss: 0.0178 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 734/1000\n","9264/9264 [==============================] - 2s 236us/sample - loss: 0.0178 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 735/1000\n","9264/9264 [==============================] - 2s 236us/sample - loss: 0.0178 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 736/1000\n","9264/9264 [==============================] - 2s 239us/sample - loss: 0.0178 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 737/1000\n","9264/9264 [==============================] - 2s 238us/sample - loss: 0.0178 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 738/1000\n","9264/9264 [==============================] - 2s 236us/sample - loss: 0.0178 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 739/1000\n","9264/9264 [==============================] - 2s 234us/sample - loss: 0.0178 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 740/1000\n","9264/9264 [==============================] - 2s 234us/sample - loss: 0.0178 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 741/1000\n","9264/9264 [==============================] - 2s 238us/sample - loss: 0.0178 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 742/1000\n","9264/9264 [==============================] - 2s 239us/sample - loss: 0.0178 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 743/1000\n","9264/9264 [==============================] - 2s 240us/sample - loss: 0.0178 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 744/1000\n","9264/9264 [==============================] - 2s 241us/sample - loss: 0.0178 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 745/1000\n","9264/9264 [==============================] - 2s 237us/sample - loss: 0.0178 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 746/1000\n","9264/9264 [==============================] - 2s 236us/sample - loss: 0.0178 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 747/1000\n","9264/9264 [==============================] - 2s 239us/sample - loss: 0.0178 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 748/1000\n","9264/9264 [==============================] - 2s 236us/sample - loss: 0.0178 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 749/1000\n","9264/9264 [==============================] - 2s 242us/sample - loss: 0.0178 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 750/1000\n","9264/9264 [==============================] - 2s 241us/sample - loss: 0.0178 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 751/1000\n","9264/9264 [==============================] - 2s 237us/sample - loss: 0.0178 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 752/1000\n","9264/9264 [==============================] - 2s 242us/sample - loss: 0.0178 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 753/1000\n","9264/9264 [==============================] - 2s 243us/sample - loss: 0.0178 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 754/1000\n","9264/9264 [==============================] - 2s 239us/sample - loss: 0.0178 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 755/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0178 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 756/1000\n","9264/9264 [==============================] - 2s 240us/sample - loss: 0.0178 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 757/1000\n","9264/9264 [==============================] - 2s 243us/sample - loss: 0.0178 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 758/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0178 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 759/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0178 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 760/1000\n","9264/9264 [==============================] - 2s 242us/sample - loss: 0.0178 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 761/1000\n","9264/9264 [==============================] - 2s 242us/sample - loss: 0.0178 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 762/1000\n","9264/9264 [==============================] - 2s 243us/sample - loss: 0.0178 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 763/1000\n","9264/9264 [==============================] - 2s 240us/sample - loss: 0.0178 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 764/1000\n","9264/9264 [==============================] - 2s 237us/sample - loss: 0.0178 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 765/1000\n","9264/9264 [==============================] - 2s 241us/sample - loss: 0.0178 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 766/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0178 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 767/1000\n","9264/9264 [==============================] - 2s 251us/sample - loss: 0.0177 - accuracy: 0.9957 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 768/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0177 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 769/1000\n","9264/9264 [==============================] - 2s 238us/sample - loss: 0.0177 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 770/1000\n","9264/9264 [==============================] - 2s 239us/sample - loss: 0.0177 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 771/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0177 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 772/1000\n","9264/9264 [==============================] - 2s 243us/sample - loss: 0.0177 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 773/1000\n","9264/9264 [==============================] - 2s 241us/sample - loss: 0.0177 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 774/1000\n","9264/9264 [==============================] - 2s 240us/sample - loss: 0.0177 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 775/1000\n","9264/9264 [==============================] - 2s 242us/sample - loss: 0.0177 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9214\n","Epoch 776/1000\n","9264/9264 [==============================] - 2s 243us/sample - loss: 0.0177 - accuracy: 0.9958 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 777/1000\n","9264/9264 [==============================] - 2s 241us/sample - loss: 0.0177 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 778/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0177 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 779/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0177 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 780/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0177 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 781/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0177 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 782/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0177 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 783/1000\n","9264/9264 [==============================] - 2s 241us/sample - loss: 0.0177 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 784/1000\n","9264/9264 [==============================] - 2s 238us/sample - loss: 0.0177 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 785/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0177 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 786/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.0177 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 787/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0177 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 788/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0177 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 789/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0177 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 790/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0177 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 791/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0177 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 792/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0177 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 793/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0177 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 794/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0177 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 795/1000\n","9264/9264 [==============================] - 2s 265us/sample - loss: 0.0177 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 796/1000\n","9264/9264 [==============================] - 2s 253us/sample - loss: 0.0177 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 797/1000\n","9264/9264 [==============================] - 2s 251us/sample - loss: 0.0177 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 798/1000\n","9264/9264 [==============================] - 2s 243us/sample - loss: 0.0177 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 799/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0177 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 800/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0177 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 801/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0177 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 802/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0177 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 803/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0177 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 804/1000\n","9264/9264 [==============================] - 2s 251us/sample - loss: 0.0177 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 805/1000\n","9264/9264 [==============================] - 2s 255us/sample - loss: 0.0177 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 806/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0177 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 807/1000\n","9264/9264 [==============================] - 2s 252us/sample - loss: 0.0177 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 808/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0177 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 809/1000\n","9264/9264 [==============================] - 2s 240us/sample - loss: 0.0177 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 810/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.0177 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 811/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0177 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 812/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0177 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 813/1000\n","9264/9264 [==============================] - 2s 255us/sample - loss: 0.0177 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 814/1000\n","9264/9264 [==============================] - 2s 252us/sample - loss: 0.0176 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 815/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0176 - accuracy: 0.9959 - val_loss: 0.2363 - val_accuracy: 0.9223\n","Epoch 816/1000\n","9264/9264 [==============================] - 2s 252us/sample - loss: 0.0176 - accuracy: 0.9959 - val_loss: 0.2363 - val_accuracy: 0.9223\n","Epoch 817/1000\n","9264/9264 [==============================] - 2s 256us/sample - loss: 0.0176 - accuracy: 0.9959 - val_loss: 0.2363 - val_accuracy: 0.9223\n","Epoch 818/1000\n","9264/9264 [==============================] - 2s 258us/sample - loss: 0.0176 - accuracy: 0.9959 - val_loss: 0.2363 - val_accuracy: 0.9223\n","Epoch 819/1000\n","9264/9264 [==============================] - 2s 254us/sample - loss: 0.0176 - accuracy: 0.9959 - val_loss: 0.2363 - val_accuracy: 0.9223\n","Epoch 820/1000\n","9264/9264 [==============================] - 2s 253us/sample - loss: 0.0176 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 821/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0176 - accuracy: 0.9959 - val_loss: 0.2363 - val_accuracy: 0.9223\n","Epoch 822/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0176 - accuracy: 0.9959 - val_loss: 0.2363 - val_accuracy: 0.9223\n","Epoch 823/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0176 - accuracy: 0.9959 - val_loss: 0.2363 - val_accuracy: 0.9223\n","Epoch 824/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0176 - accuracy: 0.9959 - val_loss: 0.2363 - val_accuracy: 0.9223\n","Epoch 825/1000\n","9264/9264 [==============================] - 2s 241us/sample - loss: 0.0176 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 826/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0176 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 827/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0176 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 828/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0176 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 829/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0176 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 830/1000\n","9264/9264 [==============================] - 2s 254us/sample - loss: 0.0176 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 831/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0176 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 832/1000\n","9264/9264 [==============================] - 2s 242us/sample - loss: 0.0176 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 833/1000\n","9264/9264 [==============================] - 2s 264us/sample - loss: 0.0176 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 834/1000\n","9264/9264 [==============================] - 3s 286us/sample - loss: 0.0176 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 835/1000\n","9264/9264 [==============================] - 3s 289us/sample - loss: 0.0176 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 836/1000\n","9264/9264 [==============================] - 3s 288us/sample - loss: 0.0176 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 837/1000\n","9264/9264 [==============================] - 3s 271us/sample - loss: 0.0176 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 838/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.0176 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9233\n","Epoch 839/1000\n","9264/9264 [==============================] - 2s 251us/sample - loss: 0.0176 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9233\n","Epoch 840/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0176 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9233\n","Epoch 841/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.0176 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9233\n","Epoch 842/1000\n","9264/9264 [==============================] - 2s 241us/sample - loss: 0.0176 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9233\n","Epoch 843/1000\n","9264/9264 [==============================] - 2s 254us/sample - loss: 0.0176 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9233\n","Epoch 844/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0176 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9233\n","Epoch 845/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0176 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9233\n","Epoch 846/1000\n","9264/9264 [==============================] - 2s 253us/sample - loss: 0.0176 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9233\n","Epoch 847/1000\n","9264/9264 [==============================] - 2s 269us/sample - loss: 0.0176 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9233\n","Epoch 848/1000\n","9264/9264 [==============================] - 3s 292us/sample - loss: 0.0176 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9233\n","Epoch 849/1000\n","9264/9264 [==============================] - 3s 270us/sample - loss: 0.0176 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9233\n","Epoch 850/1000\n","9264/9264 [==============================] - 2s 242us/sample - loss: 0.0176 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9233\n","Epoch 851/1000\n","9264/9264 [==============================] - 2s 238us/sample - loss: 0.0176 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9233\n","Epoch 852/1000\n","9264/9264 [==============================] - 2s 240us/sample - loss: 0.0176 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9233\n","Epoch 853/1000\n","9264/9264 [==============================] - 2s 241us/sample - loss: 0.0176 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9233\n","Epoch 854/1000\n","9264/9264 [==============================] - 2s 242us/sample - loss: 0.0176 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9233\n","Epoch 855/1000\n","9264/9264 [==============================] - 2s 240us/sample - loss: 0.0176 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9233\n","Epoch 856/1000\n","9264/9264 [==============================] - 2s 243us/sample - loss: 0.0176 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9233\n","Epoch 857/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0176 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9233\n","Epoch 858/1000\n","9264/9264 [==============================] - 2s 243us/sample - loss: 0.0176 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9233\n","Epoch 859/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0176 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9233\n","Epoch 860/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.0175 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9233\n","Epoch 861/1000\n","9264/9264 [==============================] - 2s 241us/sample - loss: 0.0175 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9233\n","Epoch 862/1000\n","9264/9264 [==============================] - 2s 240us/sample - loss: 0.0175 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9233\n","Epoch 863/1000\n","9264/9264 [==============================] - 2s 242us/sample - loss: 0.0175 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9233\n","Epoch 864/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0175 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9233\n","Epoch 865/1000\n","9264/9264 [==============================] - 2s 253us/sample - loss: 0.0175 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 866/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0175 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 867/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0175 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 868/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0175 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 869/1000\n","9264/9264 [==============================] - 2s 251us/sample - loss: 0.0175 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 870/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0175 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 871/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0175 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 872/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0175 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 873/1000\n","9264/9264 [==============================] - 2s 239us/sample - loss: 0.0175 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 874/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0175 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 875/1000\n","9264/9264 [==============================] - 2s 251us/sample - loss: 0.0175 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 876/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0175 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 877/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0175 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 878/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0175 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 879/1000\n","9264/9264 [==============================] - 2s 251us/sample - loss: 0.0175 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 880/1000\n","9264/9264 [==============================] - 2s 255us/sample - loss: 0.0175 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 881/1000\n","9264/9264 [==============================] - 2s 256us/sample - loss: 0.0175 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 882/1000\n","9264/9264 [==============================] - 2s 261us/sample - loss: 0.0175 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 883/1000\n","9264/9264 [==============================] - 2s 258us/sample - loss: 0.0175 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 884/1000\n","9264/9264 [==============================] - 2s 253us/sample - loss: 0.0175 - accuracy: 0.9960 - val_loss: 0.2361 - val_accuracy: 0.9243\n","Epoch 885/1000\n","9264/9264 [==============================] - 2s 255us/sample - loss: 0.0175 - accuracy: 0.9960 - val_loss: 0.2361 - val_accuracy: 0.9243\n","Epoch 886/1000\n","9264/9264 [==============================] - 2s 255us/sample - loss: 0.0175 - accuracy: 0.9960 - val_loss: 0.2361 - val_accuracy: 0.9243\n","Epoch 887/1000\n","9264/9264 [==============================] - 2s 251us/sample - loss: 0.0175 - accuracy: 0.9960 - val_loss: 0.2361 - val_accuracy: 0.9243\n","Epoch 888/1000\n","9264/9264 [==============================] - 2s 256us/sample - loss: 0.0175 - accuracy: 0.9960 - val_loss: 0.2361 - val_accuracy: 0.9243\n","Epoch 889/1000\n","9264/9264 [==============================] - 2s 255us/sample - loss: 0.0175 - accuracy: 0.9960 - val_loss: 0.2361 - val_accuracy: 0.9243\n","Epoch 890/1000\n","9264/9264 [==============================] - 2s 257us/sample - loss: 0.0175 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 891/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0175 - accuracy: 0.9960 - val_loss: 0.2361 - val_accuracy: 0.9243\n","Epoch 892/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0175 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 893/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0175 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 894/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0175 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 895/1000\n","9264/9264 [==============================] - 2s 252us/sample - loss: 0.0175 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 896/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0175 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 897/1000\n","9264/9264 [==============================] - 2s 252us/sample - loss: 0.0175 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 898/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0175 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 899/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0175 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 900/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.0175 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 901/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0175 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 902/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0175 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 903/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0175 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 904/1000\n","9264/9264 [==============================] - 2s 240us/sample - loss: 0.0175 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 905/1000\n","9264/9264 [==============================] - 2s 243us/sample - loss: 0.0175 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 906/1000\n","9264/9264 [==============================] - 2s 252us/sample - loss: 0.0174 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 907/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0174 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 908/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0174 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9223\n","Epoch 909/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0174 - accuracy: 0.9959 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 910/1000\n","9264/9264 [==============================] - 2s 256us/sample - loss: 0.0174 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 911/1000\n","9264/9264 [==============================] - 2s 255us/sample - loss: 0.0174 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 912/1000\n","9264/9264 [==============================] - 2s 256us/sample - loss: 0.0174 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 913/1000\n","9264/9264 [==============================] - 2s 255us/sample - loss: 0.0174 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 914/1000\n","9264/9264 [==============================] - 2s 259us/sample - loss: 0.0174 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 915/1000\n","9264/9264 [==============================] - 2s 253us/sample - loss: 0.0174 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 916/1000\n","9264/9264 [==============================] - 2s 252us/sample - loss: 0.0174 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 917/1000\n","9264/9264 [==============================] - 2s 254us/sample - loss: 0.0174 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 918/1000\n","9264/9264 [==============================] - 2s 257us/sample - loss: 0.0174 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 919/1000\n","9264/9264 [==============================] - 2s 256us/sample - loss: 0.0174 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 920/1000\n","9264/9264 [==============================] - 2s 253us/sample - loss: 0.0174 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 921/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0174 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 922/1000\n","9264/9264 [==============================] - 2s 251us/sample - loss: 0.0174 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 923/1000\n","9264/9264 [==============================] - 2s 252us/sample - loss: 0.0174 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 924/1000\n","9264/9264 [==============================] - 2s 253us/sample - loss: 0.0174 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 925/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0174 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 926/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0174 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 927/1000\n","9264/9264 [==============================] - 2s 253us/sample - loss: 0.0174 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 928/1000\n","9264/9264 [==============================] - 2s 252us/sample - loss: 0.0174 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 929/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.0174 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 930/1000\n","9264/9264 [==============================] - 2s 257us/sample - loss: 0.0174 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 931/1000\n","9264/9264 [==============================] - 2s 256us/sample - loss: 0.0174 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 932/1000\n","9264/9264 [==============================] - 2s 257us/sample - loss: 0.0174 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 933/1000\n","9264/9264 [==============================] - 2s 255us/sample - loss: 0.0174 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 934/1000\n","9264/9264 [==============================] - 2s 257us/sample - loss: 0.0174 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 935/1000\n","9264/9264 [==============================] - 2s 253us/sample - loss: 0.0174 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 936/1000\n","9264/9264 [==============================] - 2s 252us/sample - loss: 0.0174 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 937/1000\n","9264/9264 [==============================] - 2s 256us/sample - loss: 0.0174 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 938/1000\n","9264/9264 [==============================] - 2s 258us/sample - loss: 0.0174 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 939/1000\n","9264/9264 [==============================] - 2s 256us/sample - loss: 0.0174 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 940/1000\n","9264/9264 [==============================] - 2s 252us/sample - loss: 0.0174 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 941/1000\n","9264/9264 [==============================] - 2s 254us/sample - loss: 0.0174 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 942/1000\n","9264/9264 [==============================] - 2s 254us/sample - loss: 0.0174 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 943/1000\n","9264/9264 [==============================] - 2s 253us/sample - loss: 0.0174 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 944/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.0174 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 945/1000\n","9264/9264 [==============================] - 2s 251us/sample - loss: 0.0174 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 946/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0174 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 947/1000\n","9264/9264 [==============================] - 2s 247us/sample - loss: 0.0174 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 948/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0174 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 949/1000\n","9264/9264 [==============================] - 2s 255us/sample - loss: 0.0174 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 950/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0174 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 951/1000\n","9264/9264 [==============================] - 2s 252us/sample - loss: 0.0174 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 952/1000\n","9264/9264 [==============================] - 2s 253us/sample - loss: 0.0174 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 953/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0173 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 954/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0173 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 955/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0173 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 956/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0173 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 957/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0173 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 958/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0173 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 959/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0173 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 960/1000\n","9264/9264 [==============================] - 2s 254us/sample - loss: 0.0173 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 961/1000\n","9264/9264 [==============================] - 2s 252us/sample - loss: 0.0173 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 962/1000\n","9264/9264 [==============================] - 2s 255us/sample - loss: 0.0173 - accuracy: 0.9960 - val_loss: 0.2363 - val_accuracy: 0.9243\n","Epoch 963/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0173 - accuracy: 0.9960 - val_loss: 0.2363 - val_accuracy: 0.9243\n","Epoch 964/1000\n","9264/9264 [==============================] - 2s 251us/sample - loss: 0.0173 - accuracy: 0.9960 - val_loss: 0.2363 - val_accuracy: 0.9243\n","Epoch 965/1000\n","9264/9264 [==============================] - 2s 259us/sample - loss: 0.0173 - accuracy: 0.9960 - val_loss: 0.2363 - val_accuracy: 0.9243\n","Epoch 966/1000\n","9264/9264 [==============================] - 2s 258us/sample - loss: 0.0173 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 967/1000\n","9264/9264 [==============================] - 3s 286us/sample - loss: 0.0173 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 968/1000\n","9264/9264 [==============================] - 3s 322us/sample - loss: 0.0173 - accuracy: 0.9960 - val_loss: 0.2363 - val_accuracy: 0.9243\n","Epoch 969/1000\n","9264/9264 [==============================] - 4s 400us/sample - loss: 0.0173 - accuracy: 0.9960 - val_loss: 0.2363 - val_accuracy: 0.9243\n","Epoch 970/1000\n","9264/9264 [==============================] - 3s 296us/sample - loss: 0.0173 - accuracy: 0.9960 - val_loss: 0.2363 - val_accuracy: 0.9243\n","Epoch 971/1000\n","9264/9264 [==============================] - 2s 252us/sample - loss: 0.0173 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 972/1000\n","9264/9264 [==============================] - 2s 251us/sample - loss: 0.0173 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 973/1000\n","9264/9264 [==============================] - 2s 252us/sample - loss: 0.0173 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 974/1000\n","9264/9264 [==============================] - 2s 250us/sample - loss: 0.0173 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 975/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0173 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 976/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0173 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 977/1000\n","9264/9264 [==============================] - 2s 241us/sample - loss: 0.0173 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 978/1000\n","9264/9264 [==============================] - 2s 262us/sample - loss: 0.0173 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 979/1000\n","9264/9264 [==============================] - 3s 278us/sample - loss: 0.0173 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 980/1000\n","9264/9264 [==============================] - 2s 267us/sample - loss: 0.0173 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 981/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0173 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 982/1000\n","9264/9264 [==============================] - 2s 243us/sample - loss: 0.0173 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 983/1000\n","9264/9264 [==============================] - 2s 252us/sample - loss: 0.0173 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 984/1000\n","9264/9264 [==============================] - 2s 243us/sample - loss: 0.0173 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 985/1000\n","9264/9264 [==============================] - 2s 251us/sample - loss: 0.0173 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 986/1000\n","9264/9264 [==============================] - 2s 252us/sample - loss: 0.0173 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 987/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0173 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 988/1000\n","9264/9264 [==============================] - 2s 251us/sample - loss: 0.0173 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 989/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0173 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 990/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0173 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 991/1000\n","9264/9264 [==============================] - 2s 248us/sample - loss: 0.0173 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 992/1000\n","9264/9264 [==============================] - 2s 249us/sample - loss: 0.0173 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 993/1000\n","9264/9264 [==============================] - 2s 245us/sample - loss: 0.0173 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 994/1000\n","9264/9264 [==============================] - 2s 252us/sample - loss: 0.0173 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 995/1000\n","9264/9264 [==============================] - 2s 251us/sample - loss: 0.0173 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 996/1000\n","9264/9264 [==============================] - 2s 256us/sample - loss: 0.0173 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 997/1000\n","9264/9264 [==============================] - 2s 252us/sample - loss: 0.0173 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 998/1000\n","9264/9264 [==============================] - 2s 246us/sample - loss: 0.0173 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9243\n","Epoch 999/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0173 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9252\n","Epoch 1000/1000\n","9264/9264 [==============================] - 2s 244us/sample - loss: 0.0173 - accuracy: 0.9960 - val_loss: 0.2362 - val_accuracy: 0.9252\n","Weights from best epoch have been loaded into model.\n","              precision    recall  f1-score   support\n","\n","   not_funny       0.88      0.98      0.93       497\n","         fun       0.98      0.87      0.92       533\n","\n","    accuracy                           0.93      1030\n","   macro avg       0.93      0.93      0.93      1030\n","weighted avg       0.93      0.93      0.93      1030\n","\n","model /content/drive/My Drive/memotion/wgts/funnotfunclassesnot_funny_fun.h5\n","Model Saved\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Qu_PANIESwSd","colab_type":"code","outputId":"9473d7ed-4950-4842-dddc-9437768c5ff6","executionInfo":{"status":"ok","timestamp":1583963416717,"user_tz":-330,"elapsed":241241,"user":{"displayName":"Harsh Kataria","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimH0VsMEaiEzaEeo5nJzMe1WuJRQ2b1DK9fb3S=s64","userId":"07300304294680146354"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import ktrain\n","from ktrain import text\n","\n","\n","\n","modelslist = [['not_sarcastic','sarcastic']]\n","data_files = ['memotion_eq_sarcastic.csv']\n","for columnclass, data_file in zip(modelslist, data_files):\n","\n","    \n","    columns=columnclass      \n","    DATA_PATH = '/content/drive/My Drive/memotion/' + data_file\n","    print(DATA_PATH)\n","\n","    NUM_WORDS = 90000\n","    MAXLEN = 250\n","\n","    h5name = '_'.join(columnclass)\n","    print('model', h5name)\n","\n","    (x_train, y_train), (x_test, y_test), preproc = text.texts_from_csv(DATA_PATH,\n","                          'cleaned_ocr',\n","                          label_columns = columns,\n","                          val_filepath=None, # if None, 10% of data will be used for validation\n","                          max_features=NUM_WORDS, maxlen=MAXLEN,\n","                        ngram_range=5,)\n","\n","\n","    model = text.text_classifier('logreg', (x_train, y_train), preproc=preproc,)\n","\n","    learner = ktrain.get_learner(model, train_data=(x_train, y_train), val_data=(x_test, y_test),batch_size=20)\n","    a='/content/drive/My Drive/Weight_file/'+h5name\n","    learner.autofit(0.0001, 100,early_stopping=80, reduce_on_plateau=5)\n","\n","    a=learner.validate(val_data=(x_test, y_test), class_names=columns)\n","  \n","    predictor = ktrain.get_predictor(learner.model, preproc)\n","\n","   # score = 'xyz'########################\n","    h5name = \"/content/drive/My Drive/memotion/wgts/sarcasticnotsarcasticclasses\"+h5name +  '.h5'\n","    print('model', h5name)\n","\n","    predictor.save(h5name)\n","    print(\"Model Saved\")\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/memotion/memotion_eq_sarcastic.csv\n","model not_sarcastic_sarcastic\n","detected encoding: utf-8 (if wrong, set manually)\n","language: en\n","Word Counts: 11395\n","Nrows: 9892\n","9892 train sequences\n","train sequence lengths:\n","\tmean : 8\n","\t95percentile : 18\n","\t99percentile : 26\n","Adding 5-gram features\n","max_features changed to 223270 with addition of ngrams\n","Average train sequence length with ngrams: 30\n","train (w/ngrams) sequence lengths:\n","\tmean : 31\n","\t95percentile : 80\n","\t99percentile : 120\n","x_train shape: (9892,250)\n","y_train shape: (9892, 2)\n","1100 test sequences\n","test sequence lengths:\n","\tmean : 8\n","\t95percentile : 16\n","\t99percentile : 22\n","Average test sequence length with ngrams: 19\n","test (w/ngrams) sequence lengths:\n","\tmean : 19\n","\t95percentile : 60\n","\t99percentile : 90\n","x_test shape: (1100,250)\n","y_test shape: (1100, 2)\n","Is Multi-Label? False\n","compiling word ID features...\n","maxlen is 250\n","done.\n","\n","\n","begin training using triangular learning rate policy with max lr of 0.0001...\n","Train on 9892 samples, validate on 1100 samples\n","Epoch 1/100\n","9892/9892 [==============================] - 3s 293us/sample - loss: 0.6823 - accuracy: 0.6426 - val_loss: 0.6717 - val_accuracy: 0.7445\n","Epoch 2/100\n","9892/9892 [==============================] - 2s 244us/sample - loss: 0.6478 - accuracy: 0.8897 - val_loss: 0.6512 - val_accuracy: 0.7773\n","Epoch 3/100\n","9892/9892 [==============================] - 2s 242us/sample - loss: 0.6142 - accuracy: 0.9473 - val_loss: 0.6322 - val_accuracy: 0.7945\n","Epoch 4/100\n","9892/9892 [==============================] - 2s 244us/sample - loss: 0.5834 - accuracy: 0.9638 - val_loss: 0.6148 - val_accuracy: 0.8100\n","Epoch 5/100\n","9892/9892 [==============================] - 2s 241us/sample - loss: 0.5553 - accuracy: 0.9725 - val_loss: 0.5994 - val_accuracy: 0.8245\n","Epoch 6/100\n","9892/9892 [==============================] - 2s 246us/sample - loss: 0.5294 - accuracy: 0.9752 - val_loss: 0.5848 - val_accuracy: 0.8318\n","Epoch 7/100\n","9892/9892 [==============================] - 2s 248us/sample - loss: 0.5055 - accuracy: 0.9767 - val_loss: 0.5714 - val_accuracy: 0.8436\n","Epoch 8/100\n","9892/9892 [==============================] - 2s 242us/sample - loss: 0.4835 - accuracy: 0.9788 - val_loss: 0.5588 - val_accuracy: 0.8464\n","Epoch 9/100\n","9892/9892 [==============================] - 2s 239us/sample - loss: 0.4630 - accuracy: 0.9801 - val_loss: 0.5469 - val_accuracy: 0.8473\n","Epoch 10/100\n","9892/9892 [==============================] - 2s 243us/sample - loss: 0.4438 - accuracy: 0.9808 - val_loss: 0.5360 - val_accuracy: 0.8573\n","Epoch 11/100\n","9892/9892 [==============================] - 2s 240us/sample - loss: 0.4259 - accuracy: 0.9813 - val_loss: 0.5256 - val_accuracy: 0.8609\n","Epoch 12/100\n","9892/9892 [==============================] - 2s 240us/sample - loss: 0.4092 - accuracy: 0.9820 - val_loss: 0.5156 - val_accuracy: 0.8645\n","Epoch 13/100\n","9892/9892 [==============================] - 2s 242us/sample - loss: 0.3935 - accuracy: 0.9820 - val_loss: 0.5063 - val_accuracy: 0.8655\n","Epoch 14/100\n","9892/9892 [==============================] - 2s 239us/sample - loss: 0.3787 - accuracy: 0.9830 - val_loss: 0.4974 - val_accuracy: 0.8682\n","Epoch 15/100\n","9892/9892 [==============================] - 2s 237us/sample - loss: 0.3648 - accuracy: 0.9834 - val_loss: 0.4887 - val_accuracy: 0.8709\n","Epoch 16/100\n","9892/9892 [==============================] - 2s 240us/sample - loss: 0.3518 - accuracy: 0.9838 - val_loss: 0.4806 - val_accuracy: 0.8709\n","Epoch 17/100\n","9892/9892 [==============================] - 2s 242us/sample - loss: 0.3394 - accuracy: 0.9846 - val_loss: 0.4728 - val_accuracy: 0.8755\n","Epoch 18/100\n","9892/9892 [==============================] - 2s 240us/sample - loss: 0.3277 - accuracy: 0.9840 - val_loss: 0.4654 - val_accuracy: 0.8773\n","Epoch 19/100\n","9892/9892 [==============================] - 2s 238us/sample - loss: 0.3167 - accuracy: 0.9840 - val_loss: 0.4585 - val_accuracy: 0.8791\n","Epoch 20/100\n","9892/9892 [==============================] - 2s 240us/sample - loss: 0.3061 - accuracy: 0.9839 - val_loss: 0.4516 - val_accuracy: 0.8845\n","Epoch 21/100\n","9892/9892 [==============================] - 2s 242us/sample - loss: 0.2963 - accuracy: 0.9836 - val_loss: 0.4453 - val_accuracy: 0.8882\n","Epoch 22/100\n","9892/9892 [==============================] - 2s 241us/sample - loss: 0.2869 - accuracy: 0.9834 - val_loss: 0.4393 - val_accuracy: 0.8882\n","Epoch 23/100\n","9892/9892 [==============================] - 2s 238us/sample - loss: 0.2780 - accuracy: 0.9836 - val_loss: 0.4337 - val_accuracy: 0.8891\n","Epoch 24/100\n","9892/9892 [==============================] - 2s 237us/sample - loss: 0.2695 - accuracy: 0.9842 - val_loss: 0.4283 - val_accuracy: 0.8918\n","Epoch 25/100\n","9892/9892 [==============================] - 2s 236us/sample - loss: 0.2616 - accuracy: 0.9843 - val_loss: 0.4229 - val_accuracy: 0.8945\n","Epoch 26/100\n","9892/9892 [==============================] - 2s 243us/sample - loss: 0.2539 - accuracy: 0.9846 - val_loss: 0.4178 - val_accuracy: 0.8964\n","Epoch 27/100\n","9892/9892 [==============================] - 2s 243us/sample - loss: 0.2466 - accuracy: 0.9847 - val_loss: 0.4130 - val_accuracy: 0.8973\n","Epoch 28/100\n","9892/9892 [==============================] - 2s 235us/sample - loss: 0.2397 - accuracy: 0.9849 - val_loss: 0.4084 - val_accuracy: 0.8982\n","Epoch 29/100\n","9892/9892 [==============================] - 2s 240us/sample - loss: 0.2331 - accuracy: 0.9850 - val_loss: 0.4040 - val_accuracy: 0.8991\n","Epoch 30/100\n","9892/9892 [==============================] - 2s 238us/sample - loss: 0.2268 - accuracy: 0.9852 - val_loss: 0.3997 - val_accuracy: 0.9009\n","Epoch 31/100\n","9892/9892 [==============================] - 2s 238us/sample - loss: 0.2208 - accuracy: 0.9852 - val_loss: 0.3956 - val_accuracy: 0.9018\n","Epoch 32/100\n","9892/9892 [==============================] - 2s 243us/sample - loss: 0.2150 - accuracy: 0.9855 - val_loss: 0.3916 - val_accuracy: 0.9027\n","Epoch 33/100\n","9892/9892 [==============================] - 2s 237us/sample - loss: 0.2095 - accuracy: 0.9863 - val_loss: 0.3878 - val_accuracy: 0.9027\n","Epoch 34/100\n","9892/9892 [==============================] - 2s 237us/sample - loss: 0.2043 - accuracy: 0.9866 - val_loss: 0.3842 - val_accuracy: 0.9027\n","Epoch 35/100\n","9892/9892 [==============================] - 2s 237us/sample - loss: 0.1992 - accuracy: 0.9862 - val_loss: 0.3806 - val_accuracy: 0.9036\n","Epoch 36/100\n","9892/9892 [==============================] - 2s 238us/sample - loss: 0.1944 - accuracy: 0.9869 - val_loss: 0.3773 - val_accuracy: 0.9045\n","Epoch 37/100\n","9892/9892 [==============================] - 2s 245us/sample - loss: 0.1897 - accuracy: 0.9870 - val_loss: 0.3743 - val_accuracy: 0.9055\n","Epoch 38/100\n","9892/9892 [==============================] - 2s 240us/sample - loss: 0.1853 - accuracy: 0.9872 - val_loss: 0.3713 - val_accuracy: 0.9045\n","Epoch 39/100\n","9892/9892 [==============================] - 2s 236us/sample - loss: 0.1810 - accuracy: 0.9873 - val_loss: 0.3685 - val_accuracy: 0.9045\n","Epoch 40/100\n","9892/9892 [==============================] - 2s 242us/sample - loss: 0.1769 - accuracy: 0.9872 - val_loss: 0.3658 - val_accuracy: 0.9055\n","Epoch 41/100\n","9892/9892 [==============================] - 2s 240us/sample - loss: 0.1730 - accuracy: 0.9878 - val_loss: 0.3630 - val_accuracy: 0.9055\n","Epoch 42/100\n","9892/9892 [==============================] - 2s 245us/sample - loss: 0.1692 - accuracy: 0.9878 - val_loss: 0.3604 - val_accuracy: 0.9064\n","Epoch 43/100\n","9892/9892 [==============================] - 2s 243us/sample - loss: 0.1656 - accuracy: 0.9880 - val_loss: 0.3578 - val_accuracy: 0.9055\n","Epoch 44/100\n","9892/9892 [==============================] - 2s 242us/sample - loss: 0.1620 - accuracy: 0.9883 - val_loss: 0.3553 - val_accuracy: 0.9055\n","Epoch 45/100\n","9892/9892 [==============================] - 2s 241us/sample - loss: 0.1586 - accuracy: 0.9881 - val_loss: 0.3531 - val_accuracy: 0.9064\n","Epoch 46/100\n","9892/9892 [==============================] - 2s 232us/sample - loss: 0.1554 - accuracy: 0.9882 - val_loss: 0.3508 - val_accuracy: 0.9073\n","Epoch 47/100\n","9892/9892 [==============================] - 2s 236us/sample - loss: 0.1522 - accuracy: 0.9881 - val_loss: 0.3487 - val_accuracy: 0.9073\n","Epoch 48/100\n","9892/9892 [==============================] - 2s 240us/sample - loss: 0.1492 - accuracy: 0.9882 - val_loss: 0.3465 - val_accuracy: 0.9064\n","Epoch 49/100\n","9892/9892 [==============================] - 2s 234us/sample - loss: 0.1463 - accuracy: 0.9883 - val_loss: 0.3446 - val_accuracy: 0.9055\n","Epoch 50/100\n","9892/9892 [==============================] - 2s 237us/sample - loss: 0.1435 - accuracy: 0.9882 - val_loss: 0.3427 - val_accuracy: 0.9055\n","Epoch 51/100\n","9892/9892 [==============================] - 2s 250us/sample - loss: 0.1408 - accuracy: 0.9886 - val_loss: 0.3409 - val_accuracy: 0.9055\n","Epoch 52/100\n","9892/9892 [==============================] - 2s 246us/sample - loss: 0.1381 - accuracy: 0.9888 - val_loss: 0.3392 - val_accuracy: 0.9055\n","Epoch 53/100\n","9892/9892 [==============================] - 2s 243us/sample - loss: 0.1356 - accuracy: 0.9892 - val_loss: 0.3375 - val_accuracy: 0.9045\n","Epoch 54/100\n","9892/9892 [==============================] - 2s 242us/sample - loss: 0.1331 - accuracy: 0.9892 - val_loss: 0.3357 - val_accuracy: 0.9036\n","Epoch 55/100\n","9892/9892 [==============================] - 2s 240us/sample - loss: 0.1308 - accuracy: 0.9893 - val_loss: 0.3340 - val_accuracy: 0.9036\n","Epoch 56/100\n","9892/9892 [==============================] - 2s 243us/sample - loss: 0.1284 - accuracy: 0.9895 - val_loss: 0.3326 - val_accuracy: 0.9045\n","Epoch 57/100\n","9892/9892 [==============================] - 2s 249us/sample - loss: 0.1262 - accuracy: 0.9895 - val_loss: 0.3311 - val_accuracy: 0.9036\n","Epoch 58/100\n","9892/9892 [==============================] - 2s 247us/sample - loss: 0.1240 - accuracy: 0.9896 - val_loss: 0.3298 - val_accuracy: 0.9036\n","Epoch 59/100\n","9892/9892 [==============================] - 2s 244us/sample - loss: 0.1219 - accuracy: 0.9897 - val_loss: 0.3285 - val_accuracy: 0.9045\n","Epoch 60/100\n","9892/9892 [==============================] - 2s 238us/sample - loss: 0.1199 - accuracy: 0.9897 - val_loss: 0.3272 - val_accuracy: 0.9036\n","Epoch 61/100\n","9892/9892 [==============================] - 2s 234us/sample - loss: 0.1179 - accuracy: 0.9898 - val_loss: 0.3260 - val_accuracy: 0.9036\n","Epoch 62/100\n","9892/9892 [==============================] - 2s 237us/sample - loss: 0.1160 - accuracy: 0.9898 - val_loss: 0.3248 - val_accuracy: 0.9036\n","Epoch 63/100\n","9892/9892 [==============================] - 2s 236us/sample - loss: 0.1141 - accuracy: 0.9898 - val_loss: 0.3236 - val_accuracy: 0.9045\n","Epoch 64/100\n","9892/9892 [==============================] - 2s 234us/sample - loss: 0.1123 - accuracy: 0.9901 - val_loss: 0.3224 - val_accuracy: 0.9045\n","Epoch 65/100\n","9892/9892 [==============================] - 2s 230us/sample - loss: 0.1106 - accuracy: 0.9898 - val_loss: 0.3213 - val_accuracy: 0.9045\n","Epoch 66/100\n","9892/9892 [==============================] - 2s 234us/sample - loss: 0.1088 - accuracy: 0.9901 - val_loss: 0.3202 - val_accuracy: 0.9045\n","Epoch 67/100\n","9892/9892 [==============================] - 3s 268us/sample - loss: 0.1072 - accuracy: 0.9902 - val_loss: 0.3191 - val_accuracy: 0.9045\n","Epoch 68/100\n","9892/9892 [==============================] - 3s 268us/sample - loss: 0.1056 - accuracy: 0.9902 - val_loss: 0.3182 - val_accuracy: 0.9045\n","Epoch 69/100\n","9892/9892 [==============================] - 3s 266us/sample - loss: 0.1040 - accuracy: 0.9902 - val_loss: 0.3172 - val_accuracy: 0.9036\n","Epoch 70/100\n","9892/9892 [==============================] - 3s 271us/sample - loss: 0.1025 - accuracy: 0.9902 - val_loss: 0.3163 - val_accuracy: 0.9027\n","Epoch 71/100\n","9892/9892 [==============================] - 2s 252us/sample - loss: 0.1010 - accuracy: 0.9903 - val_loss: 0.3153 - val_accuracy: 0.9018\n","Epoch 72/100\n","9892/9892 [==============================] - 2s 245us/sample - loss: 0.0995 - accuracy: 0.9902 - val_loss: 0.3146 - val_accuracy: 0.9018\n","Epoch 73/100\n","9892/9892 [==============================] - 2s 247us/sample - loss: 0.0981 - accuracy: 0.9903 - val_loss: 0.3138 - val_accuracy: 0.9018\n","Epoch 74/100\n","9892/9892 [==============================] - 2s 237us/sample - loss: 0.0967 - accuracy: 0.9903 - val_loss: 0.3130 - val_accuracy: 0.9018\n","Epoch 75/100\n","9892/9892 [==============================] - 2s 231us/sample - loss: 0.0954 - accuracy: 0.9904 - val_loss: 0.3122 - val_accuracy: 0.9018\n","Epoch 76/100\n","9892/9892 [==============================] - 2s 234us/sample - loss: 0.0941 - accuracy: 0.9903 - val_loss: 0.3117 - val_accuracy: 0.9009\n","Epoch 77/100\n","9892/9892 [==============================] - 3s 260us/sample - loss: 0.0928 - accuracy: 0.9904 - val_loss: 0.3109 - val_accuracy: 0.9009\n","Epoch 78/100\n","9892/9892 [==============================] - 3s 261us/sample - loss: 0.0916 - accuracy: 0.9904 - val_loss: 0.3101 - val_accuracy: 0.9009\n","Epoch 79/100\n","9892/9892 [==============================] - 2s 228us/sample - loss: 0.0904 - accuracy: 0.9903 - val_loss: 0.3094 - val_accuracy: 0.9009\n","Epoch 80/100\n","9892/9892 [==============================] - 2s 228us/sample - loss: 0.0892 - accuracy: 0.9906 - val_loss: 0.3087 - val_accuracy: 0.9018\n","Epoch 81/100\n","9892/9892 [==============================] - 2s 232us/sample - loss: 0.0880 - accuracy: 0.9910 - val_loss: 0.3080 - val_accuracy: 0.9018\n","Epoch 82/100\n","9892/9892 [==============================] - 2s 232us/sample - loss: 0.0869 - accuracy: 0.9910 - val_loss: 0.3074 - val_accuracy: 0.9027\n","Epoch 83/100\n","9892/9892 [==============================] - 2s 233us/sample - loss: 0.0858 - accuracy: 0.9910 - val_loss: 0.3069 - val_accuracy: 0.9018\n","Epoch 84/100\n","9892/9892 [==============================] - 2s 226us/sample - loss: 0.0848 - accuracy: 0.9913 - val_loss: 0.3064 - val_accuracy: 0.9018\n","Epoch 85/100\n","9892/9892 [==============================] - 2s 232us/sample - loss: 0.0837 - accuracy: 0.9914 - val_loss: 0.3059 - val_accuracy: 0.9018\n","Epoch 86/100\n","9892/9892 [==============================] - 2s 230us/sample - loss: 0.0827 - accuracy: 0.9914 - val_loss: 0.3053 - val_accuracy: 0.9018\n","Epoch 87/100\n","9892/9892 [==============================] - 2s 231us/sample - loss: 0.0817 - accuracy: 0.9914 - val_loss: 0.3049 - val_accuracy: 0.9009\n","Epoch 88/100\n","9892/9892 [==============================] - 2s 231us/sample - loss: 0.0807 - accuracy: 0.9914 - val_loss: 0.3045 - val_accuracy: 0.9009\n","Epoch 89/100\n","9892/9892 [==============================] - 2s 229us/sample - loss: 0.0798 - accuracy: 0.9914 - val_loss: 0.3040 - val_accuracy: 0.9000\n","Epoch 90/100\n","9892/9892 [==============================] - 2s 233us/sample - loss: 0.0789 - accuracy: 0.9914 - val_loss: 0.3035 - val_accuracy: 0.9000\n","Epoch 91/100\n","9892/9892 [==============================] - 2s 235us/sample - loss: 0.0779 - accuracy: 0.9914 - val_loss: 0.3031 - val_accuracy: 0.9000\n","Epoch 92/100\n","9892/9892 [==============================] - 2s 229us/sample - loss: 0.0770 - accuracy: 0.9914 - val_loss: 0.3027 - val_accuracy: 0.9018\n","Epoch 93/100\n","9892/9892 [==============================] - 2s 236us/sample - loss: 0.0762 - accuracy: 0.9914 - val_loss: 0.3025 - val_accuracy: 0.9027\n","Epoch 94/100\n","9892/9892 [==============================] - 2s 238us/sample - loss: 0.0753 - accuracy: 0.9914 - val_loss: 0.3022 - val_accuracy: 0.9027\n","Epoch 95/100\n","9892/9892 [==============================] - 2s 237us/sample - loss: 0.0745 - accuracy: 0.9913 - val_loss: 0.3018 - val_accuracy: 0.9027\n","Epoch 96/100\n","9892/9892 [==============================] - 2s 233us/sample - loss: 0.0737 - accuracy: 0.9914 - val_loss: 0.3014 - val_accuracy: 0.9027\n","Epoch 97/100\n","9892/9892 [==============================] - 2s 232us/sample - loss: 0.0729 - accuracy: 0.9913 - val_loss: 0.3013 - val_accuracy: 0.9027\n","Epoch 98/100\n","9892/9892 [==============================] - 2s 233us/sample - loss: 0.0721 - accuracy: 0.9914 - val_loss: 0.3010 - val_accuracy: 0.9036\n","Epoch 99/100\n","9892/9892 [==============================] - 2s 223us/sample - loss: 0.0713 - accuracy: 0.9914 - val_loss: 0.3007 - val_accuracy: 0.9036\n","Epoch 100/100\n","9892/9892 [==============================] - 2s 224us/sample - loss: 0.0706 - accuracy: 0.9916 - val_loss: 0.3005 - val_accuracy: 0.9045\n","Weights from best epoch have been loaded into model.\n","               precision    recall  f1-score   support\n","\n","not_sarcastic       0.90      0.91      0.91       554\n","    sarcastic       0.91      0.90      0.90       546\n","\n","     accuracy                           0.90      1100\n","    macro avg       0.90      0.90      0.90      1100\n"," weighted avg       0.90      0.90      0.90      1100\n","\n","model /content/drive/My Drive/memotion/wgts/sarcasticnotsarcasticclassesnot_sarcastic_sarcastic.h5\n","Model Saved\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RtYUDaoFkHSG","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NKkHgeU1kHVb","colab_type":"code","outputId":"2df39049-7981-4610-a0aa-ff0d2b6c2c39","executionInfo":{"status":"ok","timestamp":1583966551697,"user_tz":-330,"elapsed":544739,"user":{"displayName":"Harsh Kataria","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimH0VsMEaiEzaEeo5nJzMe1WuJRQ2b1DK9fb3S=s64","userId":"07300304294680146354"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import os\n","import pandas as pd\n","import ktrain\n","\n","\n","##main\n","\n","\n","answers_dict = {'hilarious':'3','very_funny':'2','funny':'1','general':'1', 'twisted_meaning':'2', 'very_twisted':'3', 'not_sarcastic':'0','not_funny':'0','motivational':'1','not_motivational':'0', 'positive':'1', 'neutral':'0', 'negative':'-1','not_offensive':'0','very_offensive':'2', 'slight':'1', 'hateful_offensive':'3'}\n","\n","\n","\n","sarcasticpredictor = ktrain.load_predictor('/content/drive/My Drive/memotion/wgts/sarcasticnotsarcasticclassesnot_sarcastic_sarcastic.h5')\n","humourpredictor = ktrain.load_predictor('/content/drive/My Drive/memotion/wgts/funnotfunclassesnot_funny_fun.h5')\n","motivationpredictor = ktrain.load_predictor('/content/drive/My Drive/memotion/wgts/motivationalnotmoticlassesnot_motivational_motivational.h5')\n","sentipredictor = ktrain.load_predictor('/content/drive/My Drive/memotion/wgts/pnn3classespositive_negative_neutral.h5')\n","##onlyfunnypredictor = ktrain.load_predictor('/home/dgxuser136/ambuje1/senti_onlyfunny_ktrainbert.h5')\n","offensivepredictor = ktrain.load_predictor('/content/drive/My Drive/memotion/wgts/Copy of lastnewdozerooffensive_not_offensive.h5')\n","onlysarcasticpredictor = ktrain.load_predictor('/content/drive/My Drive/memotion/wgts/onlysar3classesgeneral_twisted_meaning_very_twisted.h5')\n","onlyoffensivepredictor = ktrain.load_predictor('/content/drive/My Drive/memotion/wgts/onlyoff3classesslight_hateful_offensive_very_offensive.h5')\n","\n","df = pd.read_csv('/content/drive/My Drive/memotion/wgts/2000_testdata.csv')\n","\n","file1 = open(\"/content/drive/My Drive/memotion/wgts/answer_8models-1-3.txt\",\"w\")\n","\n","\n","for i in range(len(df)):\n","    print(i)\n","    text = str(df['corrected_text'][i])\n","    #text = pre_preprocess(str(df['corrected_text'][i]))\n","\n","    taskA = '9'\n","    taskB = ['9']*4\n","    taskC = ['9']*4\n","    \n","\n","    pred = sentipredictor.predict(text)\n","    #print(pred)\n","    \n","##    pred = tup2dict(pred)\n","##    pred = max(pred, key=pred.get)\n","    taskA = answers_dict[pred]\n","    \n","    pred = humourpredictor.predict(text)\n","    #print(pred)\n","##    pred = tup2dict(pred)\n","##    pred = max(pred, key=pred.get)\n","    if pred == 'fun':\n","        taskB[0] = '1'\n","    else:\n","        taskB[0] = '0'\n","        taskC[0] = '0'\n","##    taskB[0] = answers_dict[pred]\n","##    if taskB[0] == '1':\n","##        pred = onlyfunnypredictor.predict(text)\n","##        print(pred)\n","##        pred = tup2dict(pred)\n","##        pred = max(pred, key=pred.get)\n","##        taskC[0] = answers_dict[pred]\n","##    else:\n","##        taskC[0] = '0'\n","\n","    pred = sarcasticpredictor.predict(text)\n","    #print(pred)\n","    if pred == 'sarcastic':\n","        taskB[1] = '1'\n","        pred = onlysarcasticpredictor.predict(text)\n","        #print(pred)\n","##        pred = tup2dict(pred)\n","##        pred = max(pred, key=pred.get)\n","        #taskC[1] = answers_dict[pred]\n","    else:\n","        taskB[1] = '0'\n","        taskC[1] = '0'\n","        \n","##    pred = tup2dict(pred)\n","##    pred = max(pred, key=pred.get)\n","##    taskB[1] = answers_dict[pred]\n","##    if taskB[1] == '1':\n","##        \n","##    else:\n","        \n","\n","    pred = offensivepredictor.predict(text)\n","    #print(pred)\n","    if pred == 'offensive':\n","        taskB[2] = '1'\n","        pred = onlyoffensivepredictor.predict(text)\n","        #print(pred)\n","        #taskC[2] = answers_dict[pred]\n","##        pred = tup2dict(pred)\n","##        pred = max(pred, key=pred.get)\n","    else:\n","        taskB[2] = '0'\n","        taskC[2] = '0'\n","##    pred = tup2dict(pred)\n","##    pred = max(pred, key=pred.get)\n","##    taskB[2] = answers_dict[pred]\n","##    if taskB[2] == '1':\n","##        \n","##        \n","##    else:\n","        \n","\n","    pred = motivationpredictor.predict(text)\n","    #print(pred)\n","##    pred = tup2dict(pred)\n","##    pred = max(pred, key=pred.get)\n","    taskB[3] = answers_dict[pred]\n","    taskC[3] = taskB[3]   \n","\n","    #taskC = ['9']*4\n","    ans = taskA + '_' + ''.join(taskB) + '_' + ''.join(taskC) + '\\n'\n","    \n","    \n","\n","    file1.write(ans)\n","    \n","\n","file1.close()\n","\n","##pred = [('hilarious', 0.10454379), ('not_funny', 0.21206911), ('very_funny', 0.32491603), ('funny', 0.36210373), ('general', 0.55724233), ('not_sarcastic', 0.11089532), ('twisted_meaning', 0.24999025), ('very_twisted', 0.09583253), ('not_offensive', 0.10120422), ('very_offensive', 0.30312324), ('slight', 0.5820346), ('hateful_offensive', 0.04012201), ('not_motivational', 0.9636585), ('motivational', 0.036573086), ('positive', 0.5848341), ('neutral', 0.28037217), ('negative', 0.14398904)]\n","##pred = tup2dict(pred)\n","##print(answers(pred))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","20\n","21\n","22\n","23\n","24\n","25\n","26\n","27\n","28\n","29\n","30\n","31\n","32\n","33\n","34\n","35\n","36\n","37\n","38\n","39\n","40\n","41\n","42\n","43\n","44\n","45\n","46\n","47\n","48\n","49\n","50\n","51\n","52\n","53\n","54\n","55\n","56\n","57\n","58\n","59\n","60\n","61\n","62\n","63\n","64\n","65\n","66\n","67\n","68\n","69\n","70\n","71\n","72\n","73\n","74\n","75\n","76\n","77\n","78\n","79\n","80\n","81\n","82\n","83\n","84\n","85\n","86\n","87\n","88\n","89\n","90\n","91\n","92\n","93\n","94\n","95\n","96\n","97\n","98\n","99\n","100\n","101\n","102\n","103\n","104\n","105\n","106\n","107\n","108\n","109\n","110\n","111\n","112\n","113\n","114\n","115\n","116\n","117\n","118\n","119\n","120\n","121\n","122\n","123\n","124\n","125\n","126\n","127\n","128\n","129\n","130\n","131\n","132\n","133\n","134\n","135\n","136\n","137\n","138\n","139\n","140\n","141\n","142\n","143\n","144\n","145\n","146\n","147\n","148\n","149\n","150\n","151\n","152\n","153\n","154\n","155\n","156\n","157\n","158\n","159\n","160\n","161\n","162\n","163\n","164\n","165\n","166\n","167\n","168\n","169\n","170\n","171\n","172\n","173\n","174\n","175\n","176\n","177\n","178\n","179\n","180\n","181\n","182\n","183\n","184\n","185\n","186\n","187\n","188\n","189\n","190\n","191\n","192\n","193\n","194\n","195\n","196\n","197\n","198\n","199\n","200\n","201\n","202\n","203\n","204\n","205\n","206\n","207\n","208\n","209\n","210\n","211\n","212\n","213\n","214\n","215\n","216\n","217\n","218\n","219\n","220\n","221\n","222\n","223\n","224\n","225\n","226\n","227\n","228\n","229\n","230\n","231\n","232\n","233\n","234\n","235\n","236\n","237\n","238\n","239\n","240\n","241\n","242\n","243\n","244\n","245\n","246\n","247\n","248\n","249\n","250\n","251\n","252\n","253\n","254\n","255\n","256\n","257\n","258\n","259\n","260\n","261\n","262\n","263\n","264\n","265\n","266\n","267\n","268\n","269\n","270\n","271\n","272\n","273\n","274\n","275\n","276\n","277\n","278\n","279\n","280\n","281\n","282\n","283\n","284\n","285\n","286\n","287\n","288\n","289\n","290\n","291\n","292\n","293\n","294\n","295\n","296\n","297\n","298\n","299\n","300\n","301\n","302\n","303\n","304\n","305\n","306\n","307\n","308\n","309\n","310\n","311\n","312\n","313\n","314\n","315\n","316\n","317\n","318\n","319\n","320\n","321\n","322\n","323\n","324\n","325\n","326\n","327\n","328\n","329\n","330\n","331\n","332\n","333\n","334\n","335\n","336\n","337\n","338\n","339\n","340\n","341\n","342\n","343\n","344\n","345\n","346\n","347\n","348\n","349\n","350\n","351\n","352\n","353\n","354\n","355\n","356\n","357\n","358\n","359\n","360\n","361\n","362\n","363\n","364\n","365\n","366\n","367\n","368\n","369\n","370\n","371\n","372\n","373\n","374\n","375\n","376\n","377\n","378\n","379\n","380\n","381\n","382\n","383\n","384\n","385\n","386\n","387\n","388\n","389\n","390\n","391\n","392\n","393\n","394\n","395\n","396\n","397\n","398\n","399\n","400\n","401\n","402\n","403\n","404\n","405\n","406\n","407\n","408\n","409\n","410\n","411\n","412\n","413\n","414\n","415\n","416\n","417\n","418\n","419\n","420\n","421\n","422\n","423\n","424\n","425\n","426\n","427\n","428\n","429\n","430\n","431\n","432\n","433\n","434\n","435\n","436\n","437\n","438\n","439\n","440\n","441\n","442\n","443\n","444\n","445\n","446\n","447\n","448\n","449\n","450\n","451\n","452\n","453\n","454\n","455\n","456\n","457\n","458\n","459\n","460\n","461\n","462\n","463\n","464\n","465\n","466\n","467\n","468\n","469\n","470\n","471\n","472\n","473\n","474\n","475\n","476\n","477\n","478\n","479\n","480\n","481\n","482\n","483\n","484\n","485\n","486\n","487\n","488\n","489\n","490\n","491\n","492\n","493\n","494\n","495\n","496\n","497\n","498\n","499\n","500\n","501\n","502\n","503\n","504\n","505\n","506\n","507\n","508\n","509\n","510\n","511\n","512\n","513\n","514\n","515\n","516\n","517\n","518\n","519\n","520\n","521\n","522\n","523\n","524\n","525\n","526\n","527\n","528\n","529\n","530\n","531\n","532\n","533\n","534\n","535\n","536\n","537\n","538\n","539\n","540\n","541\n","542\n","543\n","544\n","545\n","546\n","547\n","548\n","549\n","550\n","551\n","552\n","553\n","554\n","555\n","556\n","557\n","558\n","559\n","560\n","561\n","562\n","563\n","564\n","565\n","566\n","567\n","568\n","569\n","570\n","571\n","572\n","573\n","574\n","575\n","576\n","577\n","578\n","579\n","580\n","581\n","582\n","583\n","584\n","585\n","586\n","587\n","588\n","589\n","590\n","591\n","592\n","593\n","594\n","595\n","596\n","597\n","598\n","599\n","600\n","601\n","602\n","603\n","604\n","605\n","606\n","607\n","608\n","609\n","610\n","611\n","612\n","613\n","614\n","615\n","616\n","617\n","618\n","619\n","620\n","621\n","622\n","623\n","624\n","625\n","626\n","627\n","628\n","629\n","630\n","631\n","632\n","633\n","634\n","635\n","636\n","637\n","638\n","639\n","640\n","641\n","642\n","643\n","644\n","645\n","646\n","647\n","648\n","649\n","650\n","651\n","652\n","653\n","654\n","655\n","656\n","657\n","658\n","659\n","660\n","661\n","662\n","663\n","664\n","665\n","666\n","667\n","668\n","669\n","670\n","671\n","672\n","673\n","674\n","675\n","676\n","677\n","678\n","679\n","680\n","681\n","682\n","683\n","684\n","685\n","686\n","687\n","688\n","689\n","690\n","691\n","692\n","693\n","694\n","695\n","696\n","697\n","698\n","699\n","700\n","701\n","702\n","703\n","704\n","705\n","706\n","707\n","708\n","709\n","710\n","711\n","712\n","713\n","714\n","715\n","716\n","717\n","718\n","719\n","720\n","721\n","722\n","723\n","724\n","725\n","726\n","727\n","728\n","729\n","730\n","731\n","732\n","733\n","734\n","735\n","736\n","737\n","738\n","739\n","740\n","741\n","742\n","743\n","744\n","745\n","746\n","747\n","748\n","749\n","750\n","751\n","752\n","753\n","754\n","755\n","756\n","757\n","758\n","759\n","760\n","761\n","762\n","763\n","764\n","765\n","766\n","767\n","768\n","769\n","770\n","771\n","772\n","773\n","774\n","775\n","776\n","777\n","778\n","779\n","780\n","781\n","782\n","783\n","784\n","785\n","786\n","787\n","788\n","789\n","790\n","791\n","792\n","793\n","794\n","795\n","796\n","797\n","798\n","799\n","800\n","801\n","802\n","803\n","804\n","805\n","806\n","807\n","808\n","809\n","810\n","811\n","812\n","813\n","814\n","815\n","816\n","817\n","818\n","819\n","820\n","821\n","822\n","823\n","824\n","825\n","826\n","827\n","828\n","829\n","830\n","831\n","832\n","833\n","834\n","835\n","836\n","837\n","838\n","839\n","840\n","841\n","842\n","843\n","844\n","845\n","846\n","847\n","848\n","849\n","850\n","851\n","852\n","853\n","854\n","855\n","856\n","857\n","858\n","859\n","860\n","861\n","862\n","863\n","864\n","865\n","866\n","867\n","868\n","869\n","870\n","871\n","872\n","873\n","874\n","875\n","876\n","877\n","878\n","879\n","880\n","881\n","882\n","883\n","884\n","885\n","886\n","887\n","888\n","889\n","890\n","891\n","892\n","893\n","894\n","895\n","896\n","897\n","898\n","899\n","900\n","901\n","902\n","903\n","904\n","905\n","906\n","907\n","908\n","909\n","910\n","911\n","912\n","913\n","914\n","915\n","916\n","917\n","918\n","919\n","920\n","921\n","922\n","923\n","924\n","925\n","926\n","927\n","928\n","929\n","930\n","931\n","932\n","933\n","934\n","935\n","936\n","937\n","938\n","939\n","940\n","941\n","942\n","943\n","944\n","945\n","946\n","947\n","948\n","949\n","950\n","951\n","952\n","953\n","954\n","955\n","956\n","957\n","958\n","959\n","960\n","961\n","962\n","963\n","964\n","965\n","966\n","967\n","968\n","969\n","970\n","971\n","972\n","973\n","974\n","975\n","976\n","977\n","978\n","979\n","980\n","981\n","982\n","983\n","984\n","985\n","986\n","987\n","988\n","989\n","990\n","991\n","992\n","993\n","994\n","995\n","996\n","997\n","998\n","999\n","1000\n","1001\n","1002\n","1003\n","1004\n","1005\n","1006\n","1007\n","1008\n","1009\n","1010\n","1011\n","1012\n","1013\n","1014\n","1015\n","1016\n","1017\n","1018\n","1019\n","1020\n","1021\n","1022\n","1023\n","1024\n","1025\n","1026\n","1027\n","1028\n","1029\n","1030\n","1031\n","1032\n","1033\n","1034\n","1035\n","1036\n","1037\n","1038\n","1039\n","1040\n","1041\n","1042\n","1043\n","1044\n","1045\n","1046\n","1047\n","1048\n","1049\n","1050\n","1051\n","1052\n","1053\n","1054\n","1055\n","1056\n","1057\n","1058\n","1059\n","1060\n","1061\n","1062\n","1063\n","1064\n","1065\n","1066\n","1067\n","1068\n","1069\n","1070\n","1071\n","1072\n","1073\n","1074\n","1075\n","1076\n","1077\n","1078\n","1079\n","1080\n","1081\n","1082\n","1083\n","1084\n","1085\n","1086\n","1087\n","1088\n","1089\n","1090\n","1091\n","1092\n","1093\n","1094\n","1095\n","1096\n","1097\n","1098\n","1099\n","1100\n","1101\n","1102\n","1103\n","1104\n","1105\n","1106\n","1107\n","1108\n","1109\n","1110\n","1111\n","1112\n","1113\n","1114\n","1115\n","1116\n","1117\n","1118\n","1119\n","1120\n","1121\n","1122\n","1123\n","1124\n","1125\n","1126\n","1127\n","1128\n","1129\n","1130\n","1131\n","1132\n","1133\n","1134\n","1135\n","1136\n","1137\n","1138\n","1139\n","1140\n","1141\n","1142\n","1143\n","1144\n","1145\n","1146\n","1147\n","1148\n","1149\n","1150\n","1151\n","1152\n","1153\n","1154\n","1155\n","1156\n","1157\n","1158\n","1159\n","1160\n","1161\n","1162\n","1163\n","1164\n","1165\n","1166\n","1167\n","1168\n","1169\n","1170\n","1171\n","1172\n","1173\n","1174\n","1175\n","1176\n","1177\n","1178\n","1179\n","1180\n","1181\n","1182\n","1183\n","1184\n","1185\n","1186\n","1187\n","1188\n","1189\n","1190\n","1191\n","1192\n","1193\n","1194\n","1195\n","1196\n","1197\n","1198\n","1199\n","1200\n","1201\n","1202\n","1203\n","1204\n","1205\n","1206\n","1207\n","1208\n","1209\n","1210\n","1211\n","1212\n","1213\n","1214\n","1215\n","1216\n","1217\n","1218\n","1219\n","1220\n","1221\n","1222\n","1223\n","1224\n","1225\n","1226\n","1227\n","1228\n","1229\n","1230\n","1231\n","1232\n","1233\n","1234\n","1235\n","1236\n","1237\n","1238\n","1239\n","1240\n","1241\n","1242\n","1243\n","1244\n","1245\n","1246\n","1247\n","1248\n","1249\n","1250\n","1251\n","1252\n","1253\n","1254\n","1255\n","1256\n","1257\n","1258\n","1259\n","1260\n","1261\n","1262\n","1263\n","1264\n","1265\n","1266\n","1267\n","1268\n","1269\n","1270\n","1271\n","1272\n","1273\n","1274\n","1275\n","1276\n","1277\n","1278\n","1279\n","1280\n","1281\n","1282\n","1283\n","1284\n","1285\n","1286\n","1287\n","1288\n","1289\n","1290\n","1291\n","1292\n","1293\n","1294\n","1295\n","1296\n","1297\n","1298\n","1299\n","1300\n","1301\n","1302\n","1303\n","1304\n","1305\n","1306\n","1307\n","1308\n","1309\n","1310\n","1311\n","1312\n","1313\n","1314\n","1315\n","1316\n","1317\n","1318\n","1319\n","1320\n","1321\n","1322\n","1323\n","1324\n","1325\n","1326\n","1327\n","1328\n","1329\n","1330\n","1331\n","1332\n","1333\n","1334\n","1335\n","1336\n","1337\n","1338\n","1339\n","1340\n","1341\n","1342\n","1343\n","1344\n","1345\n","1346\n","1347\n","1348\n","1349\n","1350\n","1351\n","1352\n","1353\n","1354\n","1355\n","1356\n","1357\n","1358\n","1359\n","1360\n","1361\n","1362\n","1363\n","1364\n","1365\n","1366\n","1367\n","1368\n","1369\n","1370\n","1371\n","1372\n","1373\n","1374\n","1375\n","1376\n","1377\n","1378\n","1379\n","1380\n","1381\n","1382\n","1383\n","1384\n","1385\n","1386\n","1387\n","1388\n","1389\n","1390\n","1391\n","1392\n","1393\n","1394\n","1395\n","1396\n","1397\n","1398\n","1399\n","1400\n","1401\n","1402\n","1403\n","1404\n","1405\n","1406\n","1407\n","1408\n","1409\n","1410\n","1411\n","1412\n","1413\n","1414\n","1415\n","1416\n","1417\n","1418\n","1419\n","1420\n","1421\n","1422\n","1423\n","1424\n","1425\n","1426\n","1427\n","1428\n","1429\n","1430\n","1431\n","1432\n","1433\n","1434\n","1435\n","1436\n","1437\n","1438\n","1439\n","1440\n","1441\n","1442\n","1443\n","1444\n","1445\n","1446\n","1447\n","1448\n","1449\n","1450\n","1451\n","1452\n","1453\n","1454\n","1455\n","1456\n","1457\n","1458\n","1459\n","1460\n","1461\n","1462\n","1463\n","1464\n","1465\n","1466\n","1467\n","1468\n","1469\n","1470\n","1471\n","1472\n","1473\n","1474\n","1475\n","1476\n","1477\n","1478\n","1479\n","1480\n","1481\n","1482\n","1483\n","1484\n","1485\n","1486\n","1487\n","1488\n","1489\n","1490\n","1491\n","1492\n","1493\n","1494\n","1495\n","1496\n","1497\n","1498\n","1499\n","1500\n","1501\n","1502\n","1503\n","1504\n","1505\n","1506\n","1507\n","1508\n","1509\n","1510\n","1511\n","1512\n","1513\n","1514\n","1515\n","1516\n","1517\n","1518\n","1519\n","1520\n","1521\n","1522\n","1523\n","1524\n","1525\n","1526\n","1527\n","1528\n","1529\n","1530\n","1531\n","1532\n","1533\n","1534\n","1535\n","1536\n","1537\n","1538\n","1539\n","1540\n","1541\n","1542\n","1543\n","1544\n","1545\n","1546\n","1547\n","1548\n","1549\n","1550\n","1551\n","1552\n","1553\n","1554\n","1555\n","1556\n","1557\n","1558\n","1559\n","1560\n","1561\n","1562\n","1563\n","1564\n","1565\n","1566\n","1567\n","1568\n","1569\n","1570\n","1571\n","1572\n","1573\n","1574\n","1575\n","1576\n","1577\n","1578\n","1579\n","1580\n","1581\n","1582\n","1583\n","1584\n","1585\n","1586\n","1587\n","1588\n","1589\n","1590\n","1591\n","1592\n","1593\n","1594\n","1595\n","1596\n","1597\n","1598\n","1599\n","1600\n","1601\n","1602\n","1603\n","1604\n","1605\n","1606\n","1607\n","1608\n","1609\n","1610\n","1611\n","1612\n","1613\n","1614\n","1615\n","1616\n","1617\n","1618\n","1619\n","1620\n","1621\n","1622\n","1623\n","1624\n","1625\n","1626\n","1627\n","1628\n","1629\n","1630\n","1631\n","1632\n","1633\n","1634\n","1635\n","1636\n","1637\n","1638\n","1639\n","1640\n","1641\n","1642\n","1643\n","1644\n","1645\n","1646\n","1647\n","1648\n","1649\n","1650\n","1651\n","1652\n","1653\n","1654\n","1655\n","1656\n","1657\n","1658\n","1659\n","1660\n","1661\n","1662\n","1663\n","1664\n","1665\n","1666\n","1667\n","1668\n","1669\n","1670\n","1671\n","1672\n","1673\n","1674\n","1675\n","1676\n","1677\n","1678\n","1679\n","1680\n","1681\n","1682\n","1683\n","1684\n","1685\n","1686\n","1687\n","1688\n","1689\n","1690\n","1691\n","1692\n","1693\n","1694\n","1695\n","1696\n","1697\n","1698\n","1699\n","1700\n","1701\n","1702\n","1703\n","1704\n","1705\n","1706\n","1707\n","1708\n","1709\n","1710\n","1711\n","1712\n","1713\n","1714\n","1715\n","1716\n","1717\n","1718\n","1719\n","1720\n","1721\n","1722\n","1723\n","1724\n","1725\n","1726\n","1727\n","1728\n","1729\n","1730\n","1731\n","1732\n","1733\n","1734\n","1735\n","1736\n","1737\n","1738\n","1739\n","1740\n","1741\n","1742\n","1743\n","1744\n","1745\n","1746\n","1747\n","1748\n","1749\n","1750\n","1751\n","1752\n","1753\n","1754\n","1755\n","1756\n","1757\n","1758\n","1759\n","1760\n","1761\n","1762\n","1763\n","1764\n","1765\n","1766\n","1767\n","1768\n","1769\n","1770\n","1771\n","1772\n","1773\n","1774\n","1775\n","1776\n","1777\n","1778\n","1779\n","1780\n","1781\n","1782\n","1783\n","1784\n","1785\n","1786\n","1787\n","1788\n","1789\n","1790\n","1791\n","1792\n","1793\n","1794\n","1795\n","1796\n","1797\n","1798\n","1799\n","1800\n","1801\n","1802\n","1803\n","1804\n","1805\n","1806\n","1807\n","1808\n","1809\n","1810\n","1811\n","1812\n","1813\n","1814\n","1815\n","1816\n","1817\n","1818\n","1819\n","1820\n","1821\n","1822\n","1823\n","1824\n","1825\n","1826\n","1827\n","1828\n","1829\n","1830\n","1831\n","1832\n","1833\n","1834\n","1835\n","1836\n","1837\n","1838\n","1839\n","1840\n","1841\n","1842\n","1843\n","1844\n","1845\n","1846\n","1847\n","1848\n","1849\n","1850\n","1851\n","1852\n","1853\n","1854\n","1855\n","1856\n","1857\n","1858\n","1859\n","1860\n","1861\n","1862\n","1863\n","1864\n","1865\n","1866\n","1867\n","1868\n","1869\n","1870\n","1871\n","1872\n","1873\n","1874\n","1875\n","1876\n","1877\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ulsVYPrRkHbH","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Q_mp7v1kHYh","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AecdLXXlr2Fv","colab_type":"code","outputId":"6d75473e-d1ae-4d52-ca95-507d69e611f9","executionInfo":{"status":"ok","timestamp":1583839900191,"user_tz":-330,"elapsed":4463111,"user":{"displayName":"AMBUJE GUPTA","photoUrl":"","userId":"08374827863947652219"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import ktrain\n","from ktrain import text\n","\n","\n","\n","modelslist = [['funny','very_funny'],['funny','hilarious'],['hilarious','very_funny'],['twisted_meaning', 'very_twisted'],['general', 'very_twisted'],['twisted_meaning', 'general'],['very_offensive', 'slight'],['hateful_offensive', 'slight'],['very_offensive', 'hateful_offensive']]\n","data_files = ['Fun-V-fun-memotion_eq_onlyfunny.csv','Hil-fun-memotion_eq_onlyfunny.csv','Hil-V-fun-memotion_eq_onlyfunny.csv' ,'Twisted-V-twisted-memotion_eq_onlysarcastic.csv','Gen-V-twisted-memotion_eq_onlysarcastic.csv','Gen-twisted-memotion_eq_onlysarcastic.csv', 'Slight-V-off-memotion_eq_onlyoffensive.csv','Slight-H-off-memotion_eq_onlyoffensive.csv','V-off-H-off-memotion_eq_onlyoffensive.csv']\n","\n","for columnclass, data_file in zip(modelslist, data_files):\n","\n","    \n","    columns=columnclass      \n","    DATA_PATH = '/content/drive/My Drive/data_eqd/' + data_file\n","    print(DATA_PATH)\n","\n","    NUM_WORDS = 90000\n","    MAXLEN = 100\n","\n","    h5name = '_'.join(columnclass)\n","    print('model', h5name)\n","\n","    (x_train, y_train), (x_test, y_test), preproc = text.texts_from_csv(DATA_PATH,\n","                          'cleaned_ocr',\n","                          label_columns = columns,\n","                          val_filepath=None, # if None, 10% of data will be used for validation\n","                          max_features=NUM_WORDS, maxlen=MAXLEN,\n","                        ngram_range=1,)\n","\n","\n","    model = text.text_classifier('bigru', (x_train, y_train), preproc=preproc)\n","\n","    learner = ktrain.get_learner(model, train_data=(x_train, y_train), val_data=(x_test, y_test))\n","    a='/content/drive/My Drive/Weight_file/'+h5name\n","    learner.autofit(0.001, 10,early_stopping=5,checkpoint_folder=a, reduce_on_plateau=3)\n","\n","    a=learner.validate(val_data=(x_test, y_test), class_names=columns)\n","  \n","    predictor = ktrain.get_predictor(learner.model, preproc)\n","\n","   # score = 'xyz'########################\n","    h5name = \"/content/drive/My Drive/Weight_file/\"+\"last\"+h5name +  '.h5'\n","    print('model', h5name)\n","\n","    predictor.save(h5name)\n","    print(\"Model Saved\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/data_eqd/Fun-V-fun-memotion_eq_onlyfunny.csv\n","model funny_very_funny\n","detected encoding: utf-8 (if wrong, set manually)\n","language: en\n","Word Counts: 8681\n","Nrows: 4221\n","4221 train sequences\n","train sequence lengths:\n","\tmean : 8\n","\t95percentile : 17\n","\t99percentile : 25\n","x_train shape: (4221,100)\n","y_train shape: (4221, 2)\n","469 test sequences\n","test sequence lengths:\n","\tmean : 7\n","\t95percentile : 15\n","\t99percentile : 18\n","x_test shape: (469,100)\n","y_test shape: (469, 2)\n","Is Multi-Label? False\n","compiling word ID features...\n","maxlen is 100\n","processing pretrained word vectors...\n","Loading pretrained word vectors...this may take a few moments...\n","Done.\n","done.\n","\n","\n","begin training using triangular learning rate policy with max lr of 0.001...\n","Train on 4221 samples, validate on 469 samples\n","Epoch 1/10\n","4221/4221 [==============================] - 48s 11ms/sample - loss: 0.6985 - accuracy: 0.5008 - val_loss: 0.7006 - val_accuracy: 0.4925\n","Epoch 2/10\n","4221/4221 [==============================] - 44s 10ms/sample - loss: 0.6704 - accuracy: 0.5949 - val_loss: 0.7088 - val_accuracy: 0.4904\n","Epoch 3/10\n","4221/4221 [==============================] - 44s 10ms/sample - loss: 0.6267 - accuracy: 0.6769 - val_loss: 0.7267 - val_accuracy: 0.5053\n","Epoch 4/10\n","4192/4221 [============================>.] - ETA: 0s - loss: 0.5522 - accuracy: 0.7333\n","Epoch 00004: Reducing Max LR on Plateau: new max lr will be 0.0005 (if not early_stopping).\n","4221/4221 [==============================] - 44s 10ms/sample - loss: 0.5514 - accuracy: 0.7344 - val_loss: 0.7874 - val_accuracy: 0.4947\n","Epoch 5/10\n","4221/4221 [==============================] - 44s 10ms/sample - loss: 0.4255 - accuracy: 0.8195 - val_loss: 0.9093 - val_accuracy: 0.4904\n","Epoch 6/10\n","4192/4221 [============================>.] - ETA: 0s - loss: 0.3598 - accuracy: 0.8442Restoring model weights from the end of the best epoch.\n","4221/4221 [==============================] - 44s 10ms/sample - loss: 0.3591 - accuracy: 0.8446 - val_loss: 1.0053 - val_accuracy: 0.4968\n","Epoch 00006: early stopping\n","Weights from best epoch have been loaded into model.\n","              precision    recall  f1-score   support\n","\n","       funny       0.51      0.78      0.61       242\n","  very_funny       0.44      0.19      0.26       227\n","\n","    accuracy                           0.49       469\n","   macro avg       0.47      0.48      0.44       469\n","weighted avg       0.47      0.49      0.44       469\n","\n","model /content/drive/My Drive/Weight_file/lastfunny_very_funny.h5\n","Model Saved\n","/content/drive/My Drive/data_eqd/Hil-fun-memotion_eq_onlyfunny.csv\n","model funny_hilarious\n","detected encoding: utf-8 (if wrong, set manually)\n","language: en\n","Word Counts: 6754\n","Nrows: 3242\n","3242 train sequences\n","train sequence lengths:\n","\tmean : 8\n","\t95percentile : 17\n","\t99percentile : 24\n","x_train shape: (3242,100)\n","y_train shape: (3242, 2)\n","361 test sequences\n","test sequence lengths:\n","\tmean : 7\n","\t95percentile : 14\n","\t99percentile : 22\n","x_test shape: (361,100)\n","y_test shape: (361, 2)\n","Is Multi-Label? False\n","compiling word ID features...\n","maxlen is 100\n","processing pretrained word vectors...\n","Loading pretrained word vectors...this may take a few moments...\n","Done.\n","done.\n","\n","\n","begin training using triangular learning rate policy with max lr of 0.001...\n","Train on 3242 samples, validate on 361 samples\n","Epoch 1/10\n","3242/3242 [==============================] - 40s 12ms/sample - loss: 0.6325 - accuracy: 0.6749 - val_loss: 0.5843 - val_accuracy: 0.7230\n","Epoch 2/10\n","3242/3242 [==============================] - 36s 11ms/sample - loss: 0.5872 - accuracy: 0.6820 - val_loss: 0.5472 - val_accuracy: 0.7285\n","Epoch 3/10\n","3242/3242 [==============================] - 34s 10ms/sample - loss: 0.5065 - accuracy: 0.7551 - val_loss: 0.4950 - val_accuracy: 0.7950\n","Epoch 4/10\n","3242/3242 [==============================] - 34s 10ms/sample - loss: 0.3639 - accuracy: 0.8541 - val_loss: 0.4386 - val_accuracy: 0.7978\n","Epoch 5/10\n","3242/3242 [==============================] - 34s 11ms/sample - loss: 0.2353 - accuracy: 0.9090 - val_loss: 0.4156 - val_accuracy: 0.8227\n","Epoch 6/10\n","3242/3242 [==============================] - 34s 11ms/sample - loss: 0.1637 - accuracy: 0.9429 - val_loss: 0.4737 - val_accuracy: 0.8199\n","Epoch 7/10\n","3242/3242 [==============================] - 34s 11ms/sample - loss: 0.1094 - accuracy: 0.9602 - val_loss: 0.4974 - val_accuracy: 0.8089\n","Epoch 8/10\n","3232/3242 [============================>.] - ETA: 0s - loss: 0.0854 - accuracy: 0.9678\n","Epoch 00008: Reducing Max LR on Plateau: new max lr will be 0.0005 (if not early_stopping).\n","3242/3242 [==============================] - 34s 11ms/sample - loss: 0.0853 - accuracy: 0.9679 - val_loss: 0.4964 - val_accuracy: 0.8255\n","Epoch 9/10\n","3242/3242 [==============================] - 34s 11ms/sample - loss: 0.0550 - accuracy: 0.9827 - val_loss: 0.5518 - val_accuracy: 0.8255\n","Epoch 10/10\n","3232/3242 [============================>.] - ETA: 0s - loss: 0.0484 - accuracy: 0.9839Restoring model weights from the end of the best epoch.\n","3242/3242 [==============================] - 34s 11ms/sample - loss: 0.0483 - accuracy: 0.9840 - val_loss: 0.5759 - val_accuracy: 0.8393\n","Epoch 00010: early stopping\n","Weights from best epoch have been loaded into model.\n","              precision    recall  f1-score   support\n","\n","       funny       0.89      0.86      0.88       261\n","   hilarious       0.66      0.73      0.70       100\n","\n","    accuracy                           0.82       361\n","   macro avg       0.78      0.79      0.79       361\n","weighted avg       0.83      0.82      0.83       361\n","\n","model /content/drive/My Drive/Weight_file/lastfunny_hilarious.h5\n","Model Saved\n","/content/drive/My Drive/data_eqd/Hil-V-fun-memotion_eq_onlyfunny.csv\n","model hilarious_very_funny\n","detected encoding: utf-8 (if wrong, set manually)\n","language: en\n","Word Counts: 6543\n","Nrows: 3050\n","3050 train sequences\n","train sequence lengths:\n","\tmean : 8\n","\t95percentile : 17\n","\t99percentile : 26\n","x_train shape: (3050,100)\n","y_train shape: (3050, 2)\n","339 test sequences\n","test sequence lengths:\n","\tmean : 6\n","\t95percentile : 14\n","\t99percentile : 19\n","x_test shape: (339,100)\n","y_test shape: (339, 2)\n","Is Multi-Label? False\n","compiling word ID features...\n","maxlen is 100\n","processing pretrained word vectors...\n","Loading pretrained word vectors...this may take a few moments...\n","Done.\n","done.\n","\n","\n","begin training using triangular learning rate policy with max lr of 0.001...\n","Train on 3050 samples, validate on 339 samples\n","Epoch 1/10\n","3050/3050 [==============================] - 37s 12ms/sample - loss: 0.6432 - accuracy: 0.6567 - val_loss: 0.6384 - val_accuracy: 0.6578\n","Epoch 2/10\n","3050/3050 [==============================] - 33s 11ms/sample - loss: 0.5988 - accuracy: 0.6689 - val_loss: 0.6226 - val_accuracy: 0.6667\n","Epoch 3/10\n","3050/3050 [==============================] - 33s 11ms/sample - loss: 0.5170 - accuracy: 0.7439 - val_loss: 0.5959 - val_accuracy: 0.6991\n","Epoch 4/10\n","3050/3050 [==============================] - 33s 11ms/sample - loss: 0.3934 - accuracy: 0.8370 - val_loss: 0.5898 - val_accuracy: 0.7050\n","Epoch 5/10\n","3050/3050 [==============================] - 33s 11ms/sample - loss: 0.2706 - accuracy: 0.8970 - val_loss: 0.6516 - val_accuracy: 0.7463\n","Epoch 6/10\n","3050/3050 [==============================] - 33s 11ms/sample - loss: 0.1732 - accuracy: 0.9318 - val_loss: 0.7643 - val_accuracy: 0.7522\n","Epoch 7/10\n","3040/3050 [============================>.] - ETA: 0s - loss: 0.1284 - accuracy: 0.9566\n","Epoch 00007: Reducing Max LR on Plateau: new max lr will be 0.0005 (if not early_stopping).\n","3050/3050 [==============================] - 33s 11ms/sample - loss: 0.1286 - accuracy: 0.9564 - val_loss: 0.7294 - val_accuracy: 0.7906\n","Epoch 8/10\n","3050/3050 [==============================] - 33s 11ms/sample - loss: 0.0867 - accuracy: 0.9705 - val_loss: 0.7659 - val_accuracy: 0.7935\n","Epoch 9/10\n","3040/3050 [============================>.] - ETA: 0s - loss: 0.0669 - accuracy: 0.9789Restoring model weights from the end of the best epoch.\n","3050/3050 [==============================] - 34s 11ms/sample - loss: 0.0674 - accuracy: 0.9787 - val_loss: 0.8256 - val_accuracy: 0.7876\n","Epoch 00009: early stopping\n","Weights from best epoch have been loaded into model.\n","              precision    recall  f1-score   support\n","\n","   hilarious       0.57      0.55      0.56       115\n","  very_funny       0.77      0.79      0.78       224\n","\n","    accuracy                           0.71       339\n","   macro avg       0.67      0.67      0.67       339\n","weighted avg       0.70      0.71      0.70       339\n","\n","model /content/drive/My Drive/Weight_file/lasthilarious_very_funny.h5\n","Model Saved\n","/content/drive/My Drive/data_eqd/Twisted-V-twisted-memotion_eq_onlysarcastic.csv\n","model twisted_meaning_very_twisted\n","detected encoding: utf-8 (if wrong, set manually)\n","language: en\n","Word Counts: 5039\n","Nrows: 1746\n","1746 train sequences\n","train sequence lengths:\n","\tmean : 8\n","\t95percentile : 18\n","\t99percentile : 27\n","x_train shape: (1746,100)\n","y_train shape: (1746, 2)\n","195 test sequences\n","test sequence lengths:\n","\tmean : 6\n","\t95percentile : 14\n","\t99percentile : 26\n","x_test shape: (195,100)\n","y_test shape: (195, 2)\n","Is Multi-Label? False\n","compiling word ID features...\n","maxlen is 100\n","processing pretrained word vectors...\n","Loading pretrained word vectors...this may take a few moments...\n","Done.\n","done.\n","\n","\n","begin training using triangular learning rate policy with max lr of 0.001...\n","Train on 1746 samples, validate on 195 samples\n","Epoch 1/10\n","1746/1746 [==============================] - 23s 13ms/sample - loss: 0.5304 - accuracy: 0.7944 - val_loss: 0.4986 - val_accuracy: 0.8103\n","Epoch 2/10\n","1746/1746 [==============================] - 19s 11ms/sample - loss: 0.4895 - accuracy: 0.7955 - val_loss: 0.5019 - val_accuracy: 0.8103\n","Epoch 3/10\n","1746/1746 [==============================] - 19s 11ms/sample - loss: 0.4667 - accuracy: 0.7955 - val_loss: 0.5088 - val_accuracy: 0.8103\n","Epoch 4/10\n","1728/1746 [============================>.] - ETA: 0s - loss: 0.4274 - accuracy: 0.7963\n","Epoch 00004: Reducing Max LR on Plateau: new max lr will be 0.0005 (if not early_stopping).\n","1746/1746 [==============================] - 19s 11ms/sample - loss: 0.4253 - accuracy: 0.7978 - val_loss: 0.5155 - val_accuracy: 0.8051\n","Epoch 5/10\n","1746/1746 [==============================] - 19s 11ms/sample - loss: 0.3626 - accuracy: 0.8162 - val_loss: 0.5369 - val_accuracy: 0.7949\n","Epoch 6/10\n","1728/1746 [============================>.] - ETA: 0s - loss: 0.3072 - accuracy: 0.8611Restoring model weights from the end of the best epoch.\n","1746/1746 [==============================] - 19s 11ms/sample - loss: 0.3066 - accuracy: 0.8614 - val_loss: 0.5819 - val_accuracy: 0.7795\n","Epoch 00006: early stopping\n","Weights from best epoch have been loaded into model.\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n","  'precision', 'predicted', average, warn_for)\n"],"name":"stderr"},{"output_type":"stream","text":["                 precision    recall  f1-score   support\n","\n","twisted_meaning       0.81      1.00      0.90       158\n","   very_twisted       0.00      0.00      0.00        37\n","\n","       accuracy                           0.81       195\n","      macro avg       0.41      0.50      0.45       195\n","   weighted avg       0.66      0.81      0.73       195\n","\n","model /content/drive/My Drive/Weight_file/lasttwisted_meaning_very_twisted.h5\n","Model Saved\n","/content/drive/My Drive/data_eqd/Gen-V-twisted-memotion_eq_onlysarcastic.csv\n","model general_very_twisted\n","detected encoding: utf-8 (if wrong, set manually)\n","language: en\n","Word Counts: 7785\n","Nrows: 3510\n","3510 train sequences\n","train sequence lengths:\n","\tmean : 8\n","\t95percentile : 17\n","\t99percentile : 25\n","x_train shape: (3510,100)\n","y_train shape: (3510, 2)\n","391 test sequences\n","test sequence lengths:\n","\tmean : 7\n","\t95percentile : 16\n","\t99percentile : 20\n","x_test shape: (391,100)\n","y_test shape: (391, 2)\n","Is Multi-Label? False\n","compiling word ID features...\n","maxlen is 100\n","processing pretrained word vectors...\n","Loading pretrained word vectors...this may take a few moments...\n","Done.\n","done.\n","\n","\n","begin training using triangular learning rate policy with max lr of 0.001...\n","Train on 3510 samples, validate on 391 samples\n","Epoch 1/10\n","3510/3510 [==============================] - 42s 12ms/sample - loss: 0.3634 - accuracy: 0.9006 - val_loss: 0.3527 - val_accuracy: 0.8849\n","Epoch 2/10\n","3510/3510 [==============================] - 38s 11ms/sample - loss: 0.3157 - accuracy: 0.9006 - val_loss: 0.3518 - val_accuracy: 0.8849\n","Epoch 3/10\n","3510/3510 [==============================] - 37s 10ms/sample - loss: 0.2947 - accuracy: 0.9006 - val_loss: 0.3620 - val_accuracy: 0.8849\n","Epoch 4/10\n","3510/3510 [==============================] - 37s 10ms/sample - loss: 0.2598 - accuracy: 0.9023 - val_loss: 0.3814 - val_accuracy: 0.8798\n","Epoch 5/10\n","3488/3510 [============================>.] - ETA: 0s - loss: 0.1890 - accuracy: 0.9243\n","Epoch 00005: Reducing Max LR on Plateau: new max lr will be 0.0005 (if not early_stopping).\n","3510/3510 [==============================] - 37s 10ms/sample - loss: 0.1886 - accuracy: 0.9245 - val_loss: 0.4553 - val_accuracy: 0.8645\n","Epoch 6/10\n","3510/3510 [==============================] - 37s 10ms/sample - loss: 0.1155 - accuracy: 0.9558 - val_loss: 0.5422 - val_accuracy: 0.8593\n","Epoch 7/10\n","3488/3510 [============================>.] - ETA: 0s - loss: 0.0851 - accuracy: 0.9702Restoring model weights from the end of the best epoch.\n","3510/3510 [==============================] - 37s 11ms/sample - loss: 0.0857 - accuracy: 0.9698 - val_loss: 0.6181 - val_accuracy: 0.8593\n","Epoch 00007: early stopping\n","Weights from best epoch have been loaded into model.\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n","  'precision', 'predicted', average, warn_for)\n"],"name":"stderr"},{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","     general       0.88      1.00      0.94       346\n","very_twisted       0.00      0.00      0.00        45\n","\n","    accuracy                           0.88       391\n","   macro avg       0.44      0.50      0.47       391\n","weighted avg       0.78      0.88      0.83       391\n","\n","model /content/drive/My Drive/Weight_file/lastgeneral_very_twisted.h5\n","Model Saved\n","/content/drive/My Drive/data_eqd/Gen-twisted-memotion_eq_onlysarcastic.csv\n","model twisted_meaning_general\n","detected encoding: utf-8 (if wrong, set manually)\n","language: en\n","Word Counts: 9224\n","Nrows: 4548\n","4548 train sequences\n","train sequence lengths:\n","\tmean : 8\n","\t95percentile : 17\n","\t99percentile : 26\n","x_train shape: (4548,100)\n","y_train shape: (4548, 2)\n","506 test sequences\n","test sequence lengths:\n","\tmean : 7\n","\t95percentile : 16\n","\t99percentile : 21\n","x_test shape: (506,100)\n","y_test shape: (506, 2)\n","Is Multi-Label? False\n","compiling word ID features...\n","maxlen is 100\n","processing pretrained word vectors...\n","Loading pretrained word vectors...this may take a few moments...\n","Done.\n","done.\n","\n","\n","begin training using triangular learning rate policy with max lr of 0.001...\n","Train on 4548 samples, validate on 506 samples\n","Epoch 1/10\n","4548/4548 [==============================] - 53s 12ms/sample - loss: 0.6258 - accuracy: 0.6895 - val_loss: 0.6270 - val_accuracy: 0.6937\n","Epoch 2/10\n","4548/4548 [==============================] - 47s 10ms/sample - loss: 0.5963 - accuracy: 0.6942 - val_loss: 0.6238 - val_accuracy: 0.6917\n","Epoch 3/10\n","4548/4548 [==============================] - 47s 10ms/sample - loss: 0.5566 - accuracy: 0.7034 - val_loss: 0.6450 - val_accuracy: 0.6838\n","Epoch 4/10\n","4548/4548 [==============================] - 47s 10ms/sample - loss: 0.4738 - accuracy: 0.7799 - val_loss: 0.7031 - val_accuracy: 0.6423\n","Epoch 5/10\n","4544/4548 [============================>.] - ETA: 0s - loss: 0.3760 - accuracy: 0.8330\n","Epoch 00005: Reducing Max LR on Plateau: new max lr will be 0.0005 (if not early_stopping).\n","4548/4548 [==============================] - 47s 10ms/sample - loss: 0.3760 - accuracy: 0.8329 - val_loss: 0.7759 - val_accuracy: 0.6324\n","Epoch 6/10\n","4548/4548 [==============================] - 47s 10ms/sample - loss: 0.2586 - accuracy: 0.8982 - val_loss: 0.9059 - val_accuracy: 0.6285\n","Epoch 7/10\n","4544/4548 [============================>.] - ETA: 0s - loss: 0.2141 - accuracy: 0.9153Restoring model weights from the end of the best epoch.\n","4548/4548 [==============================] - 48s 10ms/sample - loss: 0.2140 - accuracy: 0.9153 - val_loss: 0.9833 - val_accuracy: 0.6245\n","Epoch 00007: early stopping\n","Weights from best epoch have been loaded into model.\n","                 precision    recall  f1-score   support\n","\n","twisted_meaning       0.00      0.00      0.00       155\n","        general       0.69      1.00      0.82       351\n","\n","       accuracy                           0.69       506\n","      macro avg       0.35      0.50      0.41       506\n","   weighted avg       0.48      0.69      0.57       506\n","\n","model /content/drive/My Drive/Weight_file/lasttwisted_meaning_general.h5\n","Model Saved\n","/content/drive/My Drive/data_eqd/Slight-V-off-memotion_eq_onlyoffensive.csv\n","model very_offensive_slight\n","detected encoding: utf-8 (if wrong, set manually)\n","language: en\n","Word Counts: 8094\n","Nrows: 4552\n","4552 train sequences\n","train sequence lengths:\n","\tmean : 8\n","\t95percentile : 17\n","\t99percentile : 25\n","x_train shape: (4552,100)\n","y_train shape: (4552, 2)\n","506 test sequences\n","test sequence lengths:\n","\tmean : 7\n","\t95percentile : 15\n","\t99percentile : 21\n","x_test shape: (506,100)\n","y_test shape: (506, 2)\n","Is Multi-Label? False\n","compiling word ID features...\n","maxlen is 100\n","processing pretrained word vectors...\n","Loading pretrained word vectors...this may take a few moments...\n","Done.\n","done.\n","\n","\n","begin training using triangular learning rate policy with max lr of 0.001...\n","Train on 4552 samples, validate on 506 samples\n","Epoch 1/10\n","4552/4552 [==============================] - 52s 11ms/sample - loss: 0.6917 - accuracy: 0.5310 - val_loss: 0.6825 - val_accuracy: 0.5553\n","Epoch 2/10\n","4552/4552 [==============================] - 48s 11ms/sample - loss: 0.6450 - accuracy: 0.6573 - val_loss: 0.6557 - val_accuracy: 0.6107\n","Epoch 3/10\n","4552/4552 [==============================] - 49s 11ms/sample - loss: 0.5539 - accuracy: 0.7412 - val_loss: 0.6356 - val_accuracy: 0.6443\n","Epoch 4/10\n","4552/4552 [==============================] - 49s 11ms/sample - loss: 0.4066 - accuracy: 0.8260 - val_loss: 0.6291 - val_accuracy: 0.6719\n","Epoch 5/10\n","4552/4552 [==============================] - 49s 11ms/sample - loss: 0.3025 - accuracy: 0.8765 - val_loss: 0.6381 - val_accuracy: 0.7036\n","Epoch 6/10\n","4552/4552 [==============================] - 49s 11ms/sample - loss: 0.2173 - accuracy: 0.9163 - val_loss: 0.6733 - val_accuracy: 0.7194\n","Epoch 7/10\n","4544/4552 [============================>.] - ETA: 0s - loss: 0.1688 - accuracy: 0.9355\n","Epoch 00007: Reducing Max LR on Plateau: new max lr will be 0.0005 (if not early_stopping).\n","4552/4552 [==============================] - 49s 11ms/sample - loss: 0.1688 - accuracy: 0.9354 - val_loss: 0.7075 - val_accuracy: 0.7233\n","Epoch 8/10\n","4552/4552 [==============================] - 49s 11ms/sample - loss: 0.1148 - accuracy: 0.9576 - val_loss: 0.7820 - val_accuracy: 0.7391\n","Epoch 9/10\n","4544/4552 [============================>.] - ETA: 0s - loss: 0.0968 - accuracy: 0.9654Restoring model weights from the end of the best epoch.\n","4552/4552 [==============================] - 48s 11ms/sample - loss: 0.0968 - accuracy: 0.9655 - val_loss: 0.8222 - val_accuracy: 0.7431\n","Epoch 00009: early stopping\n","Weights from best epoch have been loaded into model.\n","                precision    recall  f1-score   support\n","\n","very_offensive       0.65      0.69      0.67       243\n","        slight       0.70      0.65      0.67       263\n","\n","      accuracy                           0.67       506\n","     macro avg       0.67      0.67      0.67       506\n","  weighted avg       0.67      0.67      0.67       506\n","\n","model /content/drive/My Drive/Weight_file/lastvery_offensive_slight.h5\n","Model Saved\n","/content/drive/My Drive/data_eqd/Slight-H-off-memotion_eq_onlyoffensive.csv\n","model hateful_offensive_slight\n","detected encoding: utf-8 (if wrong, set manually)\n","language: en\n","Word Counts: 6450\n","Nrows: 4520\n","4520 train sequences\n","train sequence lengths:\n","\tmean : 8\n","\t95percentile : 18\n","\t99percentile : 27\n","x_train shape: (4520,100)\n","y_train shape: (4520, 2)\n","503 test sequences\n","test sequence lengths:\n","\tmean : 8\n","\t95percentile : 17\n","\t99percentile : 28\n","x_test shape: (503,100)\n","y_test shape: (503, 2)\n","Is Multi-Label? False\n","compiling word ID features...\n","maxlen is 100\n","processing pretrained word vectors...\n","Loading pretrained word vectors...this may take a few moments...\n","Done.\n","done.\n","\n","\n","begin training using triangular learning rate policy with max lr of 0.001...\n","Train on 4520 samples, validate on 503 samples\n","Epoch 1/10\n","4520/4520 [==============================] - 52s 12ms/sample - loss: 0.6173 - accuracy: 0.6790 - val_loss: 0.5198 - val_accuracy: 0.7734\n","Epoch 2/10\n","4520/4520 [==============================] - 47s 10ms/sample - loss: 0.3127 - accuracy: 0.8978 - val_loss: 0.1900 - val_accuracy: 0.9364\n","Epoch 3/10\n","4520/4520 [==============================] - 48s 11ms/sample - loss: 0.0966 - accuracy: 0.9730 - val_loss: 0.1241 - val_accuracy: 0.9543\n","Epoch 4/10\n","4520/4520 [==============================] - 47s 10ms/sample - loss: 0.0445 - accuracy: 0.9900 - val_loss: 0.1091 - val_accuracy: 0.9642\n","Epoch 5/10\n","4520/4520 [==============================] - 46s 10ms/sample - loss: 0.0313 - accuracy: 0.9925 - val_loss: 0.0936 - val_accuracy: 0.9682\n","Epoch 6/10\n","4520/4520 [==============================] - 48s 11ms/sample - loss: 0.0205 - accuracy: 0.9951 - val_loss: 0.0985 - val_accuracy: 0.9662\n","Epoch 7/10\n","4520/4520 [==============================] - 47s 10ms/sample - loss: 0.0144 - accuracy: 0.9967 - val_loss: 0.1411 - val_accuracy: 0.9543\n","Epoch 8/10\n","4512/4520 [============================>.] - ETA: 0s - loss: 0.0117 - accuracy: 0.9973\n","Epoch 00008: Reducing Max LR on Plateau: new max lr will be 0.0005 (if not early_stopping).\n","4520/4520 [==============================] - 47s 10ms/sample - loss: 0.0117 - accuracy: 0.9973 - val_loss: 0.1174 - val_accuracy: 0.9682\n","Epoch 9/10\n","4520/4520 [==============================] - 46s 10ms/sample - loss: 0.0096 - accuracy: 0.9985 - val_loss: 0.1053 - val_accuracy: 0.9682\n","Epoch 10/10\n","4512/4520 [============================>.] - ETA: 0s - loss: 0.0091 - accuracy: 0.9982Restoring model weights from the end of the best epoch.\n","4520/4520 [==============================] - 46s 10ms/sample - loss: 0.0091 - accuracy: 0.9982 - val_loss: 0.1159 - val_accuracy: 0.9682\n","Epoch 00010: early stopping\n","Weights from best epoch have been loaded into model.\n","                   precision    recall  f1-score   support\n","\n","hateful_offensive       0.94      1.00      0.97       246\n","           slight       1.00      0.94      0.97       257\n","\n","         accuracy                           0.97       503\n","        macro avg       0.97      0.97      0.97       503\n","     weighted avg       0.97      0.97      0.97       503\n","\n","model /content/drive/My Drive/Weight_file/lasthateful_offensive_slight.h5\n","Model Saved\n","/content/drive/My Drive/data_eqd/V-off-H-off-memotion_eq_onlyoffensive.csv\n","model very_offensive_hateful_offensive\n","detected encoding: utf-8 (if wrong, set manually)\n","language: en\n","Word Counts: 4739\n","Nrows: 4407\n","4407 train sequences\n","train sequence lengths:\n","\tmean : 8\n","\t95percentile : 18\n","\t99percentile : 27\n","x_train shape: (4407,100)\n","y_train shape: (4407, 2)\n","490 test sequences\n","test sequence lengths:\n","\tmean : 8\n","\t95percentile : 19\n","\t99percentile : 27\n","x_test shape: (490,100)\n","y_test shape: (490, 2)\n","Is Multi-Label? False\n","compiling word ID features...\n","maxlen is 100\n","processing pretrained word vectors...\n","Loading pretrained word vectors...this may take a few moments...\n","Done.\n","done.\n","\n","\n","begin training using triangular learning rate policy with max lr of 0.001...\n","Train on 4407 samples, validate on 490 samples\n","Epoch 1/10\n","4407/4407 [==============================] - 52s 12ms/sample - loss: 0.6138 - accuracy: 0.6971 - val_loss: 0.4890 - val_accuracy: 0.8388\n","Epoch 2/10\n","4407/4407 [==============================] - 48s 11ms/sample - loss: 0.2732 - accuracy: 0.9126 - val_loss: 0.1773 - val_accuracy: 0.9531\n","Epoch 3/10\n","4407/4407 [==============================] - 48s 11ms/sample - loss: 0.0829 - accuracy: 0.9805 - val_loss: 0.1208 - val_accuracy: 0.9571\n","Epoch 4/10\n","4407/4407 [==============================] - 49s 11ms/sample - loss: 0.0402 - accuracy: 0.9905 - val_loss: 0.0993 - val_accuracy: 0.9694\n","Epoch 5/10\n","4407/4407 [==============================] - 49s 11ms/sample - loss: 0.0211 - accuracy: 0.9950 - val_loss: 0.0891 - val_accuracy: 0.9776\n","Epoch 6/10\n","4407/4407 [==============================] - 53s 12ms/sample - loss: 0.0201 - accuracy: 0.9941 - val_loss: 0.0871 - val_accuracy: 0.9714\n","Epoch 7/10\n","4407/4407 [==============================] - 48s 11ms/sample - loss: 0.0129 - accuracy: 0.9973 - val_loss: 0.0750 - val_accuracy: 0.9816\n","Epoch 8/10\n","4407/4407 [==============================] - 48s 11ms/sample - loss: 0.0103 - accuracy: 0.9973 - val_loss: 0.1042 - val_accuracy: 0.9735\n","Epoch 9/10\n","4407/4407 [==============================] - 47s 11ms/sample - loss: 0.0083 - accuracy: 0.9982 - val_loss: 0.0885 - val_accuracy: 0.9796\n","Epoch 10/10\n","4384/4407 [============================>.] - ETA: 0s - loss: 0.0119 - accuracy: 0.9964\n","Epoch 00010: Reducing Max LR on Plateau: new max lr will be 0.0005 (if not early_stopping).\n","4407/4407 [==============================] - 47s 11ms/sample - loss: 0.0118 - accuracy: 0.9964 - val_loss: 0.1202 - val_accuracy: 0.9694\n","Weights from best epoch have been loaded into model.\n","                   precision    recall  f1-score   support\n","\n","   very_offensive       1.00      0.94      0.97       261\n","hateful_offensive       0.94      1.00      0.97       229\n","\n","         accuracy                           0.97       490\n","        macro avg       0.97      0.97      0.97       490\n","     weighted avg       0.97      0.97      0.97       490\n","\n","model /content/drive/My Drive/Weight_file/lastvery_offensive_hateful_offensive.h5\n","Model Saved\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jyS1NE-3tksf","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cHJQcBPwMz2_","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"O1y4ts2bMzyS","colab_type":"code","outputId":"3be71dc5-dc21-47f0-8f14-cc7058fa28d3","executionInfo":{"status":"ok","timestamp":1583844131684,"user_tz":-330,"elapsed":3874781,"user":{"displayName":"AMBUJE GUPTA","photoUrl":"","userId":"08374827863947652219"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import ktrain\n","from ktrain import text\n","\n","\n","\n","modelslist = [['not_funny','fun'],['not_motivational','motivational'],['not_offensive','offensive'],['not_sarcastic','sarcastic']]\n","data_files = ['memotion_eq_humour.csv','memotion_eq_motivation.csv','memotion_eq_offensive.csv','memotion_eq_sarcastic.csv']\n","\n","for columnclass, data_file in zip(modelslist, data_files):\n","\n","    \n","    columns=columnclass      \n","    DATA_PATH = '/content/drive/My Drive/data_eqd/' + data_file\n","    print(DATA_PATH)\n","\n","    NUM_WORDS = 90000\n","    MAXLEN = 100\n","\n","    h5name = '_'.join(columnclass)\n","    print('model', h5name)\n","\n","    (x_train, y_train), (x_test, y_test), preproc = text.texts_from_csv(DATA_PATH,\n","                          'cleaned_ocr',\n","                          label_columns = columns,\n","                          val_filepath=None, # if None, 10% of data will be used for validation\n","                          max_features=NUM_WORDS, maxlen=MAXLEN,\n","                        ngram_range=1,)\n","\n","\n","    model = text.text_classifier('bigru', (x_train, y_train), preproc=preproc)\n","\n","    learner = ktrain.get_learner(model, train_data=(x_train, y_train), val_data=(x_test, y_test))\n","    a='/content/drive/My Drive/Weight_file/'+h5name\n","    learner.autofit(0.001, 10,early_stopping=5,checkpoint_folder=a, reduce_on_plateau=3)\n","\n","    a=learner.validate(val_data=(x_test, y_test), class_names=columns)\n","  \n","    predictor = ktrain.get_predictor(learner.model, preproc)\n","\n","   # score = 'xyz'########################\n","    h5name = \"/content/drive/My Drive/Weight_file/\"+\"last\"+h5name +  '.h5'\n","    print('model', h5name)\n","\n","    predictor.save(h5name)\n","    print(\"Model Saved\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/data_eqd/memotion_eq_humour.csv\n","model not_funny_fun\n","detected encoding: utf-8 (if wrong, set manually)\n","language: en\n","Word Counts: 11501\n","Nrows: 9264\n","9264 train sequences\n","train sequence lengths:\n","\tmean : 8\n","\t95percentile : 18\n","\t99percentile : 27\n","x_train shape: (9264,100)\n","y_train shape: (9264, 2)\n","1030 test sequences\n","test sequence lengths:\n","\tmean : 7\n","\t95percentile : 16\n","\t99percentile : 24\n","x_test shape: (1030,100)\n","y_test shape: (1030, 2)\n","Is Multi-Label? False\n","compiling word ID features...\n","maxlen is 100\n","processing pretrained word vectors...\n","Loading pretrained word vectors...this may take a few moments...\n","Done.\n","done.\n","\n","\n","begin training using triangular learning rate policy with max lr of 0.001...\n","Train on 9264 samples, validate on 1030 samples\n","Epoch 1/10\n","9264/9264 [==============================] - 100s 11ms/sample - loss: 0.6774 - accuracy: 0.5678 - val_loss: 0.6546 - val_accuracy: 0.6301\n","Epoch 2/10\n","9264/9264 [==============================] - 101s 11ms/sample - loss: 0.5829 - accuracy: 0.7064 - val_loss: 0.5657 - val_accuracy: 0.7010\n","Epoch 3/10\n","9264/9264 [==============================] - 98s 11ms/sample - loss: 0.4074 - accuracy: 0.8195 - val_loss: 0.4786 - val_accuracy: 0.7709\n","Epoch 4/10\n","9264/9264 [==============================] - 97s 10ms/sample - loss: 0.2649 - accuracy: 0.8930 - val_loss: 0.4113 - val_accuracy: 0.8252\n","Epoch 5/10\n","9264/9264 [==============================] - 95s 10ms/sample - loss: 0.1772 - accuracy: 0.9297 - val_loss: 0.4024 - val_accuracy: 0.8388\n","Epoch 6/10\n","9264/9264 [==============================] - 95s 10ms/sample - loss: 0.1238 - accuracy: 0.9509 - val_loss: 0.3761 - val_accuracy: 0.8680\n","Epoch 7/10\n","9264/9264 [==============================] - 94s 10ms/sample - loss: 0.0899 - accuracy: 0.9682 - val_loss: 0.4418 - val_accuracy: 0.8553\n","Epoch 8/10\n","9264/9264 [==============================] - 94s 10ms/sample - loss: 0.0710 - accuracy: 0.9732 - val_loss: 0.3929 - val_accuracy: 0.8845\n","Epoch 9/10\n","9248/9264 [============================>.] - ETA: 0s - loss: 0.0565 - accuracy: 0.9798\n","Epoch 00009: Reducing Max LR on Plateau: new max lr will be 0.0005 (if not early_stopping).\n","9264/9264 [==============================] - 95s 10ms/sample - loss: 0.0564 - accuracy: 0.9798 - val_loss: 0.4097 - val_accuracy: 0.8806\n","Epoch 10/10\n","9264/9264 [==============================] - 95s 10ms/sample - loss: 0.0424 - accuracy: 0.9869 - val_loss: 0.4467 - val_accuracy: 0.8864\n","Weights from best epoch have been loaded into model.\n","              precision    recall  f1-score   support\n","\n","   not_funny       0.81      0.99      0.89       491\n","         fun       0.99      0.79      0.88       539\n","\n","    accuracy                           0.89      1030\n","   macro avg       0.90      0.89      0.89      1030\n","weighted avg       0.91      0.89      0.89      1030\n","\n","model /content/drive/My Drive/Weight_file/lastnot_funny_fun.h5\n","Model Saved\n","/content/drive/My Drive/data_eqd/memotion_eq_motivation.csv\n","model not_motivational_motivational\n","detected encoding: utf-8 (if wrong, set manually)\n","language: en\n","Word Counts: 11473\n","Nrows: 7704\n","7704 train sequences\n","train sequence lengths:\n","\tmean : 8\n","\t95percentile : 17\n","\t99percentile : 26\n","x_train shape: (7704,100)\n","y_train shape: (7704, 2)\n","857 test sequences\n","test sequence lengths:\n","\tmean : 7\n","\t95percentile : 17\n","\t99percentile : 23\n","x_test shape: (857,100)\n","y_test shape: (857, 2)\n","Is Multi-Label? False\n","compiling word ID features...\n","maxlen is 100\n","processing pretrained word vectors...\n","Loading pretrained word vectors...this may take a few moments...\n","Done.\n","done.\n","\n","\n","begin training using triangular learning rate policy with max lr of 0.001...\n","Train on 7704 samples, validate on 857 samples\n","Epoch 1/10\n","7704/7704 [==============================] - 87s 11ms/sample - loss: 0.6911 - accuracy: 0.5334 - val_loss: 0.6820 - val_accuracy: 0.5648\n","Epoch 2/10\n","7704/7704 [==============================] - 84s 11ms/sample - loss: 0.6439 - accuracy: 0.6455 - val_loss: 0.6643 - val_accuracy: 0.5986\n","Epoch 3/10\n","7704/7704 [==============================] - 84s 11ms/sample - loss: 0.5435 - accuracy: 0.7373 - val_loss: 0.6618 - val_accuracy: 0.6243\n","Epoch 4/10\n","7704/7704 [==============================] - 84s 11ms/sample - loss: 0.4141 - accuracy: 0.8150 - val_loss: 0.6694 - val_accuracy: 0.6511\n","Epoch 5/10\n","7704/7704 [==============================] - 83s 11ms/sample - loss: 0.3057 - accuracy: 0.8723 - val_loss: 0.7067 - val_accuracy: 0.6791\n","Epoch 6/10\n","7680/7704 [============================>.] - ETA: 0s - loss: 0.2305 - accuracy: 0.9069\n","Epoch 00006: Reducing Max LR on Plateau: new max lr will be 0.0005 (if not early_stopping).\n","7704/7704 [==============================] - 82s 11ms/sample - loss: 0.2305 - accuracy: 0.9069 - val_loss: 0.7487 - val_accuracy: 0.6919\n","Epoch 7/10\n","7704/7704 [==============================] - 83s 11ms/sample - loss: 0.1638 - accuracy: 0.9380 - val_loss: 0.8219 - val_accuracy: 0.6978\n","Epoch 8/10\n","7680/7704 [============================>.] - ETA: 0s - loss: 0.1312 - accuracy: 0.9504Restoring model weights from the end of the best epoch.\n","7704/7704 [==============================] - 84s 11ms/sample - loss: 0.1317 - accuracy: 0.9502 - val_loss: 0.8805 - val_accuracy: 0.7106\n","Epoch 00008: early stopping\n","Weights from best epoch have been loaded into model.\n","                  precision    recall  f1-score   support\n","\n","not_motivational       0.63      0.69      0.66       455\n","    motivational       0.61      0.55      0.58       402\n","\n","        accuracy                           0.62       857\n","       macro avg       0.62      0.62      0.62       857\n","    weighted avg       0.62      0.62      0.62       857\n","\n","model /content/drive/My Drive/Weight_file/lastnot_motivational_motivational.h5\n","Model Saved\n","/content/drive/My Drive/data_eqd/memotion_eq_offensive.csv\n","model not_offensive_offensive\n","detected encoding: utf-8 (if wrong, set manually)\n","language: en\n","Word Counts: 11374\n","Nrows: 7704\n","7704 train sequences\n","train sequence lengths:\n","\tmean : 8\n","\t95percentile : 18\n","\t99percentile : 26\n","x_train shape: (7704,100)\n","y_train shape: (7704, 2)\n","857 test sequences\n","test sequence lengths:\n","\tmean : 7\n","\t95percentile : 16\n","\t99percentile : 24\n","x_test shape: (857,100)\n","y_test shape: (857, 2)\n","Is Multi-Label? False\n","compiling word ID features...\n","maxlen is 100\n","processing pretrained word vectors...\n","Loading pretrained word vectors...this may take a few moments...\n","Done.\n","done.\n","\n","\n","begin training using triangular learning rate policy with max lr of 0.001...\n","Train on 7704 samples, validate on 857 samples\n","Epoch 1/10\n","7704/7704 [==============================] - 95s 12ms/sample - loss: 0.6926 - accuracy: 0.5213 - val_loss: 0.6824 - val_accuracy: 0.5788\n","Epoch 2/10\n","7704/7704 [==============================] - 79s 10ms/sample - loss: 0.6538 - accuracy: 0.6250 - val_loss: 0.6727 - val_accuracy: 0.5951\n","Epoch 3/10\n","7704/7704 [==============================] - 78s 10ms/sample - loss: 0.5697 - accuracy: 0.7137 - val_loss: 0.6655 - val_accuracy: 0.6383\n","Epoch 4/10\n","7704/7704 [==============================] - 76s 10ms/sample - loss: 0.4446 - accuracy: 0.8006 - val_loss: 0.6986 - val_accuracy: 0.6464\n","Epoch 5/10\n","7704/7704 [==============================] - 77s 10ms/sample - loss: 0.3309 - accuracy: 0.8577 - val_loss: 0.7292 - val_accuracy: 0.6476\n","Epoch 6/10\n","7680/7704 [============================>.] - ETA: 0s - loss: 0.2475 - accuracy: 0.8980\n","Epoch 00006: Reducing Max LR on Plateau: new max lr will be 0.0005 (if not early_stopping).\n","7704/7704 [==============================] - 77s 10ms/sample - loss: 0.2476 - accuracy: 0.8977 - val_loss: 0.7997 - val_accuracy: 0.6791\n","Epoch 7/10\n","7704/7704 [==============================] - 76s 10ms/sample - loss: 0.1748 - accuracy: 0.9329 - val_loss: 0.9070 - val_accuracy: 0.6814\n","Epoch 8/10\n","7680/7704 [============================>.] - ETA: 0s - loss: 0.1456 - accuracy: 0.9422Restoring model weights from the end of the best epoch.\n","7704/7704 [==============================] - 76s 10ms/sample - loss: 0.1454 - accuracy: 0.9424 - val_loss: 0.9604 - val_accuracy: 0.6908\n","Epoch 00008: early stopping\n","Weights from best epoch have been loaded into model.\n","               precision    recall  f1-score   support\n","\n","not_offensive       0.64      0.66      0.65       435\n","    offensive       0.64      0.61      0.62       422\n","\n","     accuracy                           0.64       857\n","    macro avg       0.64      0.64      0.64       857\n"," weighted avg       0.64      0.64      0.64       857\n","\n","model /content/drive/My Drive/Weight_file/lastnot_offensive_offensive.h5\n","Model Saved\n","/content/drive/My Drive/data_eqd/memotion_eq_sarcastic.csv\n","model not_sarcastic_sarcastic\n","detected encoding: utf-8 (if wrong, set manually)\n","language: en\n","Word Counts: 11361\n","Nrows: 9892\n","9892 train sequences\n","train sequence lengths:\n","\tmean : 8\n","\t95percentile : 17\n","\t99percentile : 26\n","x_train shape: (9892,100)\n","y_train shape: (9892, 2)\n","1100 test sequences\n","test sequence lengths:\n","\tmean : 8\n","\t95percentile : 17\n","\t99percentile : 25\n","x_test shape: (1100,100)\n","y_test shape: (1100, 2)\n","Is Multi-Label? False\n","compiling word ID features...\n","maxlen is 100\n","processing pretrained word vectors...\n","Loading pretrained word vectors...this may take a few moments...\n","Done.\n","done.\n","\n","\n","begin training using triangular learning rate policy with max lr of 0.001...\n","Train on 9892 samples, validate on 1100 samples\n","Epoch 1/10\n","9892/9892 [==============================] - 111s 11ms/sample - loss: 0.6718 - accuracy: 0.5761 - val_loss: 0.6262 - val_accuracy: 0.6864\n","Epoch 2/10\n","9892/9892 [==============================] - 107s 11ms/sample - loss: 0.5305 - accuracy: 0.7469 - val_loss: 0.5114 - val_accuracy: 0.7555\n","Epoch 3/10\n","9892/9892 [==============================] - 108s 11ms/sample - loss: 0.3372 - accuracy: 0.8588 - val_loss: 0.4340 - val_accuracy: 0.8100\n","Epoch 4/10\n","9892/9892 [==============================] - 108s 11ms/sample - loss: 0.2301 - accuracy: 0.9145 - val_loss: 0.4167 - val_accuracy: 0.8391\n","Epoch 5/10\n","9892/9892 [==============================] - 106s 11ms/sample - loss: 0.1681 - accuracy: 0.9369 - val_loss: 0.4316 - val_accuracy: 0.8400\n","Epoch 6/10\n","9892/9892 [==============================] - 110s 11ms/sample - loss: 0.1243 - accuracy: 0.9558 - val_loss: 0.4649 - val_accuracy: 0.8445\n","Epoch 7/10\n","9888/9892 [============================>.] - ETA: 0s - loss: 0.0963 - accuracy: 0.9656\n","Epoch 00007: Reducing Max LR on Plateau: new max lr will be 0.0005 (if not early_stopping).\n","9892/9892 [==============================] - 111s 11ms/sample - loss: 0.0963 - accuracy: 0.9656 - val_loss: 0.5778 - val_accuracy: 0.8200\n","Epoch 8/10\n","9892/9892 [==============================] - 107s 11ms/sample - loss: 0.0638 - accuracy: 0.9790 - val_loss: 0.5714 - val_accuracy: 0.8473\n","Epoch 9/10\n","9888/9892 [============================>.] - ETA: 0s - loss: 0.0565 - accuracy: 0.9813Restoring model weights from the end of the best epoch.\n","9892/9892 [==============================] - 108s 11ms/sample - loss: 0.0565 - accuracy: 0.9813 - val_loss: 0.5541 - val_accuracy: 0.8545\n","Epoch 00009: early stopping\n","Weights from best epoch have been loaded into model.\n","               precision    recall  f1-score   support\n","\n","not_sarcastic       0.80      0.91      0.85       540\n","    sarcastic       0.89      0.78      0.83       560\n","\n","     accuracy                           0.84      1100\n","    macro avg       0.84      0.84      0.84      1100\n"," weighted avg       0.85      0.84      0.84      1100\n","\n","model /content/drive/My Drive/Weight_file/lastnot_sarcastic_sarcastic.h5\n","Model Saved\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"g8oBZHDIPGLg","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pW2czHG7Wqg7","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PNYJsd1OWqeA","colab_type":"code","outputId":"31381e29-0da4-480c-fd26-feda0f73bb15","executionInfo":{"status":"ok","timestamp":1583848448522,"user_tz":-330,"elapsed":262382,"user":{"displayName":"AMBUJE GUPTA","photoUrl":"","userId":"08374827863947652219"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import ktrain\n","from ktrain import text\n","\n","\n","\n","modelslist = [['not_motivational','motivational'],['not_offensive','offensive']]\n","data_files = ['memotion_eq_motivation.csv','memotion_eq_offensive.csv']\n","\n","for columnclass, data_file in zip(modelslist, data_files):\n","\n","    \n","    columns=columnclass      \n","    DATA_PATH = '/content/drive/My Drive/data_eqd/' + data_file\n","    print(DATA_PATH)\n","\n","    NUM_WORDS = 90000\n","    MAXLEN = 100\n","\n","    h5name = '_'.join(columnclass)\n","    print('model', h5name)\n","\n","    (x_train, y_train), (x_test, y_test), preproc = text.texts_from_csv(DATA_PATH,\n","                          'cleaned_ocr',\n","                          label_columns = columns,\n","                          val_filepath=None, # if None, 10% of data will be used for validation\n","                          max_features=NUM_WORDS, maxlen=MAXLEN,\n","                        ngram_range=1,)\n","\n","\n","    model = text.text_classifier('fasttext', (x_train, y_train), preproc=preproc)\n","\n","    learner = ktrain.get_learner(model, train_data=(x_train, y_train), val_data=(x_test, y_test))\n","    a='/content/drive/My Drive/Weight_file/'+h5name\n","    learner.autofit(0.001, 10,early_stopping=20,checkpoint_folder=a, reduce_on_plateau=10)\n","\n","    a=learner.validate(val_data=(x_test, y_test), class_names=columns)\n","  \n","    predictor = ktrain.get_predictor(learner.model, preproc)\n","\n","   # score = 'xyz'########################\n","    h5name = \"/content/drive/My Drive/Weight_file/\"+\"last\"+h5name +  '.h5'\n","    print('model', h5name)\n","\n","    predictor.save(h5name)\n","    print(\"Model Saved\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/data_eqd/memotion_eq_motivation.csv\n","model not_motivational_motivational\n","detected encoding: utf-8 (if wrong, set manually)\n","language: en\n","Word Counts: 11426\n","Nrows: 7704\n","7704 train sequences\n","train sequence lengths:\n","\tmean : 8\n","\t95percentile : 17\n","\t99percentile : 26\n","x_train shape: (7704,100)\n","y_train shape: (7704, 2)\n","857 test sequences\n","test sequence lengths:\n","\tmean : 7\n","\t95percentile : 17\n","\t99percentile : 23\n","x_test shape: (857,100)\n","y_test shape: (857, 2)\n","Is Multi-Label? False\n","compiling word ID features...\n","maxlen is 100\n","done.\n","\n","\n","begin training using triangular learning rate policy with max lr of 0.001...\n","Train on 7704 samples, validate on 857 samples\n","Epoch 1/10\n","7704/7704 [==============================] - 13s 2ms/sample - loss: 0.7943 - accuracy: 0.5164 - val_loss: 0.6887 - val_accuracy: 0.5508\n","Epoch 2/10\n","7704/7704 [==============================] - 13s 2ms/sample - loss: 0.7005 - accuracy: 0.5666 - val_loss: 0.6827 - val_accuracy: 0.5671\n","Epoch 3/10\n","7704/7704 [==============================] - 13s 2ms/sample - loss: 0.6544 - accuracy: 0.6167 - val_loss: 0.6651 - val_accuracy: 0.6196\n","Epoch 4/10\n","7704/7704 [==============================] - 12s 2ms/sample - loss: 0.5939 - accuracy: 0.6838 - val_loss: 0.6527 - val_accuracy: 0.6278\n","Epoch 5/10\n","7704/7704 [==============================] - 13s 2ms/sample - loss: 0.5096 - accuracy: 0.7574 - val_loss: 0.6349 - val_accuracy: 0.6511\n","Epoch 6/10\n","7704/7704 [==============================] - 12s 2ms/sample - loss: 0.4249 - accuracy: 0.8071 - val_loss: 0.6385 - val_accuracy: 0.6639\n","Epoch 7/10\n","7704/7704 [==============================] - 12s 2ms/sample - loss: 0.3600 - accuracy: 0.8415 - val_loss: 0.6484 - val_accuracy: 0.6873\n","Epoch 8/10\n","7704/7704 [==============================] - 12s 2ms/sample - loss: 0.2973 - accuracy: 0.8764 - val_loss: 0.6515 - val_accuracy: 0.7036\n","Epoch 9/10\n","7704/7704 [==============================] - 12s 2ms/sample - loss: 0.2670 - accuracy: 0.8916 - val_loss: 0.6819 - val_accuracy: 0.7083\n","Epoch 10/10\n","7704/7704 [==============================] - 12s 2ms/sample - loss: 0.2365 - accuracy: 0.9098 - val_loss: 0.7128 - val_accuracy: 0.7130\n","Weights from best epoch have been loaded into model.\n","                  precision    recall  f1-score   support\n","\n","not_motivational       0.80      0.65      0.71       474\n","    motivational       0.65      0.79      0.71       383\n","\n","        accuracy                           0.71       857\n","       macro avg       0.72      0.72      0.71       857\n","    weighted avg       0.73      0.71      0.71       857\n","\n","model /content/drive/My Drive/Weight_file/lastnot_motivational_motivational.h5\n","Model Saved\n","/content/drive/My Drive/data_eqd/memotion_eq_offensive.csv\n","model not_offensive_offensive\n","detected encoding: utf-8 (if wrong, set manually)\n","language: en\n","Word Counts: 11493\n","Nrows: 7704\n","7704 train sequences\n","train sequence lengths:\n","\tmean : 8\n","\t95percentile : 17\n","\t99percentile : 26\n","x_train shape: (7704,100)\n","y_train shape: (7704, 2)\n","857 test sequences\n","test sequence lengths:\n","\tmean : 7\n","\t95percentile : 17\n","\t99percentile : 25\n","x_test shape: (857,100)\n","y_test shape: (857, 2)\n","Is Multi-Label? False\n","compiling word ID features...\n","maxlen is 100\n","done.\n","\n","\n","begin training using triangular learning rate policy with max lr of 0.001...\n","Train on 7704 samples, validate on 857 samples\n","Epoch 1/10\n","7704/7704 [==============================] - 14s 2ms/sample - loss: 0.9236 - accuracy: 0.5132 - val_loss: 0.6992 - val_accuracy: 0.4901\n","Epoch 2/10\n","7704/7704 [==============================] - 12s 2ms/sample - loss: 0.7554 - accuracy: 0.5448 - val_loss: 0.6932 - val_accuracy: 0.5018\n","Epoch 3/10\n","7704/7704 [==============================] - 12s 2ms/sample - loss: 0.7007 - accuracy: 0.5771 - val_loss: 0.6715 - val_accuracy: 0.5939\n","Epoch 4/10\n","7704/7704 [==============================] - 13s 2ms/sample - loss: 0.6473 - accuracy: 0.6258 - val_loss: 0.6546 - val_accuracy: 0.6161\n","Epoch 5/10\n","7704/7704 [==============================] - 12s 2ms/sample - loss: 0.5883 - accuracy: 0.6819 - val_loss: 0.6535 - val_accuracy: 0.5998\n","Epoch 6/10\n","7704/7704 [==============================] - 12s 2ms/sample - loss: 0.5113 - accuracy: 0.7474 - val_loss: 0.6600 - val_accuracy: 0.6231\n","Epoch 7/10\n","7704/7704 [==============================] - 12s 2ms/sample - loss: 0.4560 - accuracy: 0.7848 - val_loss: 0.6689 - val_accuracy: 0.6231\n","Epoch 8/10\n","7704/7704 [==============================] - 12s 2ms/sample - loss: 0.3892 - accuracy: 0.8226 - val_loss: 0.6737 - val_accuracy: 0.6511\n","Epoch 9/10\n","7704/7704 [==============================] - 12s 2ms/sample - loss: 0.3321 - accuracy: 0.8596 - val_loss: 0.7096 - val_accuracy: 0.6651\n","Epoch 10/10\n","7704/7704 [==============================] - 12s 2ms/sample - loss: 0.2986 - accuracy: 0.8754 - val_loss: 0.7471 - val_accuracy: 0.6581\n","Weights from best epoch have been loaded into model.\n","               precision    recall  f1-score   support\n","\n","not_offensive       0.62      0.77      0.69       420\n","    offensive       0.71      0.55      0.62       437\n","\n","     accuracy                           0.66       857\n","    macro avg       0.67      0.66      0.65       857\n"," weighted avg       0.67      0.66      0.65       857\n","\n","model /content/drive/My Drive/Weight_file/lastnot_offensive_offensive.h5\n","Model Saved\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dfBFfXFMWqZr","colab_type":"code","outputId":"852db900-b102-4fa0-ff59-3ef35cb420d4","executionInfo":{"status":"ok","timestamp":1583848761932,"user_tz":-330,"elapsed":270685,"user":{"displayName":"AMBUJE GUPTA","photoUrl":"","userId":"08374827863947652219"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import ktrain\n","from ktrain import text\n","\n","\n","\n","modelslist = [['not_offensive','offensive']]\n","data_files = ['memotion_eq_offensive.csv']\n","\n","for columnclass, data_file in zip(modelslist, data_files):\n","\n","    \n","    columns=columnclass      \n","    DATA_PATH = '/content/drive/My Drive/data_eqd/' + data_file\n","    print(DATA_PATH)\n","\n","    NUM_WORDS = 90000\n","    MAXLEN = 50\n","\n","    h5name = '_'.join(columnclass)\n","    print('model', h5name)\n","\n","    (x_train, y_train), (x_test, y_test), preproc = text.texts_from_csv(DATA_PATH,\n","                          'cleaned_ocr',\n","                          label_columns = columns,\n","                          val_filepath=None, # if None, 10% of data will be used for validation\n","                          max_features=NUM_WORDS, maxlen=MAXLEN,\n","                        ngram_range=1,)\n","\n","\n","    model = text.text_classifier('fasttext', (x_train, y_train), preproc=preproc)\n","\n","    learner = ktrain.get_learner(model, train_data=(x_train, y_train), val_data=(x_test, y_test))\n","    a='/content/drive/My Drive/Weight_file/'+h5name\n","    learner.autofit(0.001, 20,early_stopping=20,checkpoint_folder=a, reduce_on_plateau=10)\n","\n","    a=learner.validate(val_data=(x_test, y_test), class_names=columns)\n","  \n","    predictor = ktrain.get_predictor(learner.model, preproc)\n","\n","   # score = 'xyz'########################\n","    h5name = \"/content/drive/My Drive/Weight_file/\"+\"last\"+h5name +  '.h5'\n","    print('model', h5name)\n","\n","    predictor.save(h5name)\n","    print(\"Model Saved\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/data_eqd/memotion_eq_offensive.csv\n","model not_offensive_offensive\n","detected encoding: utf-8 (if wrong, set manually)\n","language: en\n","Word Counts: 11455\n","Nrows: 7704\n","7704 train sequences\n","train sequence lengths:\n","\tmean : 8\n","\t95percentile : 18\n","\t99percentile : 26\n","x_train shape: (7704,50)\n","y_train shape: (7704, 2)\n","857 test sequences\n","test sequence lengths:\n","\tmean : 7\n","\t95percentile : 16\n","\t99percentile : 24\n","x_test shape: (857,50)\n","y_test shape: (857, 2)\n","Is Multi-Label? False\n","compiling word ID features...\n","maxlen is 50\n","done.\n","\n","\n","begin training using triangular learning rate policy with max lr of 0.001...\n","Train on 7704 samples, validate on 857 samples\n","Epoch 1/20\n","7704/7704 [==============================] - 15s 2ms/sample - loss: 0.8344 - accuracy: 0.5000 - val_loss: 0.6920 - val_accuracy: 0.5239\n","Epoch 2/20\n","7704/7704 [==============================] - 14s 2ms/sample - loss: 0.7281 - accuracy: 0.5422 - val_loss: 0.6857 - val_accuracy: 0.5706\n","Epoch 3/20\n","7704/7704 [==============================] - 14s 2ms/sample - loss: 0.6832 - accuracy: 0.5837 - val_loss: 0.6723 - val_accuracy: 0.5939\n","Epoch 4/20\n","7704/7704 [==============================] - 14s 2ms/sample - loss: 0.6423 - accuracy: 0.6234 - val_loss: 0.6586 - val_accuracy: 0.6044\n","Epoch 5/20\n","7704/7704 [==============================] - 14s 2ms/sample - loss: 0.5825 - accuracy: 0.6874 - val_loss: 0.6425 - val_accuracy: 0.6324\n","Epoch 6/20\n","7704/7704 [==============================] - 14s 2ms/sample - loss: 0.5110 - accuracy: 0.7552 - val_loss: 0.6425 - val_accuracy: 0.6429\n","Epoch 7/20\n","7704/7704 [==============================] - 14s 2ms/sample - loss: 0.4390 - accuracy: 0.7970 - val_loss: 0.6674 - val_accuracy: 0.6569\n","Epoch 8/20\n","7704/7704 [==============================] - 14s 2ms/sample - loss: 0.3641 - accuracy: 0.8380 - val_loss: 0.6764 - val_accuracy: 0.6826\n","Epoch 9/20\n","7704/7704 [==============================] - 14s 2ms/sample - loss: 0.3221 - accuracy: 0.8680 - val_loss: 0.6889 - val_accuracy: 0.6826\n","Epoch 10/20\n","7704/7704 [==============================] - 14s 2ms/sample - loss: 0.2794 - accuracy: 0.8875 - val_loss: 0.7335 - val_accuracy: 0.6849\n","Epoch 11/20\n","7704/7704 [==============================] - 12s 2ms/sample - loss: 0.2510 - accuracy: 0.8967 - val_loss: 0.7812 - val_accuracy: 0.6861\n","Epoch 12/20\n","7704/7704 [==============================] - 12s 2ms/sample - loss: 0.2217 - accuracy: 0.9108 - val_loss: 0.7754 - val_accuracy: 0.7025\n","Epoch 13/20\n","7704/7704 [==============================] - 12s 2ms/sample - loss: 0.2039 - accuracy: 0.9163 - val_loss: 0.7947 - val_accuracy: 0.6943\n","Epoch 14/20\n","7704/7704 [==============================] - 12s 2ms/sample - loss: 0.1871 - accuracy: 0.9283 - val_loss: 0.8593 - val_accuracy: 0.6943\n","Epoch 15/20\n","7680/7704 [============================>.] - ETA: 0s - loss: 0.1756 - accuracy: 0.9324\n","Epoch 00015: Reducing Max LR on Plateau: new max lr will be 0.0005 (if not early_stopping).\n","7704/7704 [==============================] - 12s 2ms/sample - loss: 0.1760 - accuracy: 0.9324 - val_loss: 0.8818 - val_accuracy: 0.6814\n","Epoch 16/20\n","7704/7704 [==============================] - 12s 2ms/sample - loss: 0.1563 - accuracy: 0.9420 - val_loss: 0.8791 - val_accuracy: 0.6943\n","Epoch 17/20\n","7704/7704 [==============================] - 12s 2ms/sample - loss: 0.1419 - accuracy: 0.9459 - val_loss: 0.9229 - val_accuracy: 0.6954\n","Epoch 18/20\n","7704/7704 [==============================] - 12s 2ms/sample - loss: 0.1359 - accuracy: 0.9492 - val_loss: 0.9407 - val_accuracy: 0.6978\n","Epoch 19/20\n","7704/7704 [==============================] - 12s 2ms/sample - loss: 0.1246 - accuracy: 0.9539 - val_loss: 0.9452 - val_accuracy: 0.6966\n","Epoch 20/20\n","7704/7704 [==============================] - 12s 2ms/sample - loss: 0.1150 - accuracy: 0.9590 - val_loss: 0.9803 - val_accuracy: 0.7025\n","Weights from best epoch have been loaded into model.\n","               precision    recall  f1-score   support\n","\n","not_offensive       0.67      0.78      0.72       428\n","    offensive       0.74      0.62      0.68       429\n","\n","     accuracy                           0.70       857\n","    macro avg       0.71      0.70      0.70       857\n"," weighted avg       0.71      0.70      0.70       857\n","\n","model /content/drive/My Drive/Weight_file/lastnot_offensive_offensive.h5\n","Model Saved\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Hlu0oBoeef5f","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oR9b8mRVmZyA","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qKYcZ3zhmaCw","colab_type":"code","outputId":"19577b3e-1a33-4212-dbd9-783af03df8a2","executionInfo":{"status":"error","timestamp":1583848957980,"user_tz":-330,"elapsed":87954,"user":{"displayName":"AMBUJE GUPTA","photoUrl":"","userId":"08374827863947652219"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import ktrain\n","from ktrain import text\n","\n","\n","\n","modelslist = [['funny','very_funny'],['twisted_meaning', 'general'],['very_offensive', 'slight']]\n","data_files = ['Fun-V-fun-memotion_eq_onlyfunny.csv','Twisted-V-twisted-memotion_eq_onlysarcastic.csv','Gen-twisted-memotion_eq_onlysarcastic.csv', 'Slight-V-off-memotion_eq_onlyoffensive.csv']\n","\n","for columnclass, data_file in zip(modelslist, data_files):\n","\n","    \n","    columns=columnclass      \n","    DATA_PATH = '/content/drive/My Drive/data_eqd/' + data_file\n","    print(DATA_PATH)\n","\n","    NUM_WORDS = 90000\n","    MAXLEN = 100\n","\n","    h5name = '_'.join(columnclass)\n","    print('model', h5name)\n","\n","    (x_train, y_train), (x_test, y_test), preproc = text.texts_from_csv(DATA_PATH,\n","                          'cleaned_ocr',\n","                          label_columns = columns,\n","                          val_filepath=None, # if None, 10% of data will be used for validation\n","                          max_features=NUM_WORDS, maxlen=MAXLEN,\n","                        ngram_range=1,)\n","\n","\n","    model = text.text_classifier('fasttext', (x_train, y_train), preproc=preproc)\n","\n","    learner = ktrain.get_learner(model, train_data=(x_train, y_train), val_data=(x_test, y_test))\n","    a='/content/drive/My Drive/Weight_file/'+h5name\n","    learner.autofit(0.001, 15,early_stopping=8,checkpoint_folder=a, reduce_on_plateau=5)\n","\n","    a=learner.validate(val_data=(x_test, y_test), class_names=columns)\n","  \n","    predictor = ktrain.get_predictor(learner.model, preproc)\n","\n","   # score = 'xyz'########################\n","    h5name = \"/content/drive/My Drive/Weight_file/\"+\"last\"+h5name +  '.h5'\n","    print('model', h5name)\n","\n","    predictor.save(h5name)\n","    print(\"Model Saved\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/data_eqd/Fun-V-fun-memotion_eq_onlyfunny.csv\n","model funny_very_funny\n","detected encoding: utf-8 (if wrong, set manually)\n","language: en\n","Word Counts: 8689\n","Nrows: 4221\n","4221 train sequences\n","train sequence lengths:\n","\tmean : 8\n","\t95percentile : 17\n","\t99percentile : 25\n","x_train shape: (4221,100)\n","y_train shape: (4221, 2)\n","469 test sequences\n","test sequence lengths:\n","\tmean : 7\n","\t95percentile : 14\n","\t99percentile : 18\n","x_test shape: (469,100)\n","y_test shape: (469, 2)\n","Is Multi-Label? False\n","compiling word ID features...\n","maxlen is 100\n","done.\n","\n","\n","begin training using triangular learning rate policy with max lr of 0.001...\n","Train on 4221 samples, validate on 469 samples\n","Epoch 1/15\n","4221/4221 [==============================] - 8s 2ms/sample - loss: 0.9203 - accuracy: 0.4949 - val_loss: 0.6930 - val_accuracy: 0.5224\n","Epoch 2/15\n","4221/4221 [==============================] - 8s 2ms/sample - loss: 0.7848 - accuracy: 0.5380 - val_loss: 0.6904 - val_accuracy: 0.5693\n","Epoch 3/15\n","4221/4221 [==============================] - 8s 2ms/sample - loss: 0.7214 - accuracy: 0.5788 - val_loss: 0.6902 - val_accuracy: 0.5501\n","Epoch 4/15\n","4221/4221 [==============================] - 8s 2ms/sample - loss: 0.6796 - accuracy: 0.6018 - val_loss: 0.6942 - val_accuracy: 0.5245\n","Epoch 5/15\n","4221/4221 [==============================] - 7s 2ms/sample - loss: 0.6183 - accuracy: 0.6541 - val_loss: 0.7087 - val_accuracy: 0.4904\n","Epoch 6/15\n","4221/4221 [==============================] - 7s 2ms/sample - loss: 0.5649 - accuracy: 0.6994 - val_loss: 0.7449 - val_accuracy: 0.5075\n","Epoch 7/15\n","4221/4221 [==============================] - 7s 2ms/sample - loss: 0.5022 - accuracy: 0.7546 - val_loss: 0.7928 - val_accuracy: 0.5053\n","Epoch 8/15\n","4192/4221 [============================>.] - ETA: 0s - loss: 0.4552 - accuracy: 0.7875\n","Epoch 00008: Reducing Max LR on Plateau: new max lr will be 0.0005 (if not early_stopping).\n","4221/4221 [==============================] - 7s 2ms/sample - loss: 0.4550 - accuracy: 0.7875 - val_loss: 0.8500 - val_accuracy: 0.4989\n","Epoch 9/15\n","4221/4221 [==============================] - 7s 2ms/sample - loss: 0.3940 - accuracy: 0.8244 - val_loss: 0.8838 - val_accuracy: 0.5053\n","Epoch 10/15\n","4221/4221 [==============================] - 7s 2ms/sample - loss: 0.3521 - accuracy: 0.8519 - val_loss: 0.9350 - val_accuracy: 0.5032\n","Epoch 11/15\n","4160/4221 [============================>.] - ETA: 0s - loss: 0.3149 - accuracy: 0.8601Restoring model weights from the end of the best epoch.\n","4221/4221 [==============================] - 7s 2ms/sample - loss: 0.3149 - accuracy: 0.8600 - val_loss: 0.9833 - val_accuracy: 0.5181\n","Epoch 00011: early stopping\n","Weights from best epoch have been loaded into model.\n","              precision    recall  f1-score   support\n","\n","       funny       0.57      0.82      0.67       263\n","  very_funny       0.47      0.20      0.28       206\n","\n","    accuracy                           0.55       469\n","   macro avg       0.52      0.51      0.48       469\n","weighted avg       0.53      0.55      0.50       469\n","\n","model /content/drive/My Drive/Weight_file/lastfunny_very_funny.h5\n","Model Saved\n","/content/drive/My Drive/data_eqd/Twisted-V-twisted-memotion_eq_onlysarcastic.csv\n","model twisted_meaning_general\n","detected encoding: utf-8 (if wrong, set manually)\n"],"name":"stdout"},{"output_type":"error","ename":"KeyError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-31-83b81ce582fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m                           \u001b[0mval_filepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# if None, 10% of data will be used for validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                           \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_WORDS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAXLEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                         ngram_range=1,)\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ktrain/text/data.py\u001b[0m in \u001b[0;36mtexts_from_csv\u001b[0;34m(train_filepath, text_column, label_columns, val_filepath, max_features, maxlen, val_pct, ngram_range, preprocess_mode, encoding, lang, sep, random_state, verbose)\u001b[0m\n\u001b[1;32m    191\u001b[0m                          \u001b[0mpreprocess_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreprocess_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m                          \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m                          verbose=verbose)\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ktrain/text/data.py\u001b[0m in \u001b[0;36mtexts_from_df\u001b[0;34m(train_df, text_column, label_columns, val_df, max_features, maxlen, val_pct, ngram_range, preprocess_mode, lang, random_state, verbose)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext_column\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fillna'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel_columns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mval_df\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2999\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3000\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3001\u001b[0;31m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3003\u001b[0m         \u001b[0;31m# check if we are modifying a copy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter, raise_missing)\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_null_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1090\u001b[0m             \u001b[0;31m# this preserves dtype of the value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m             \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1092\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1093\u001b[0m                 \u001b[0;31m# GH#22717 handle casting compatibility that np.concatenate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m                 \u001b[0;31m#  does incorrectly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m                 \u001b[0;31m# multiple aligners (or null slices)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mis_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1186\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0msingle_aligner\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_null_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m                         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: \"['general'] not in index\""]}]},{"cell_type":"code","metadata":{"id":"GLfJlbcVmZut","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lg1J1m1DmZqL","colab_type":"code","outputId":"72bec9d6-7e7b-4db7-fa61-9a6c2e77dcec","executionInfo":{"status":"ok","timestamp":1583853780948,"user_tz":-330,"elapsed":401501,"user":{"displayName":"AMBUJE GUPTA","photoUrl":"","userId":"08374827863947652219"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import ktrain\n","from ktrain import text\n","\n","\n","modelslist = [['funny','hilarious'],['hilarious','very_funny'],['twisted_meaning', 'very_twisted'],['general', 'very_twisted']]\n","data_files = ['Hil-fun-memotion_eq_onlyfunny.csv','Hil-V-fun-memotion_eq_onlyfunny.csv','Twisted-V-twisted-memotion_eq_onlysarcastic.csv','Gen-V-twisted-memotion_eq_onlysarcastic.csv']\n","\n","for columnclass, data_file in zip(modelslist, data_files):\n","\n","    \n","    columns=columnclass      \n","    DATA_PATH = '/content/drive/My Drive/updData/' + data_file\n","    print(DATA_PATH)\n","\n","    NUM_WORDS = 90000\n","    MAXLEN = 100\n","\n","    h5name = '_'.join(columnclass)\n","    print('model', h5name)\n","\n","    (x_train, y_train), (x_test, y_test), preproc = text.texts_from_csv(DATA_PATH,\n","                          'cleaned_ocr',\n","                          label_columns = columns,\n","                          val_filepath=None, # if None, 10% of data will be used for validation\n","                          max_features=NUM_WORDS, maxlen=MAXLEN,\n","                        ngram_range=1,)\n","\n","\n","    model = text.text_classifier('fasttext', (x_train, y_train), preproc=preproc)\n","\n","    learner = ktrain.get_learner(model, train_data=(x_train, y_train), val_data=(x_test, y_test))\n","    a='/content/drive/My Drive/Weight_file/'+h5name\n","    learner.autofit(0.001, 15,early_stopping=8,reduce_on_plateau=5)\n","\n","    a=learner.validate(val_data=(x_test, y_test), class_names=columns)\n","  \n","    predictor = ktrain.get_predictor(learner.model, preproc)\n","\n","   # score = 'xyz'########################\n","    h5name = \"/content/drive/My Drive/Weight_file/\"+h5name +  '.h5'\n","    print('model', h5name)\n","\n","    predictor.save(h5name)\n","    print(\"Model Saved\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/updData/Hil-fun-memotion_eq_onlyfunny.csv\n","model funny_hilarious\n","detected encoding: utf-8 (if wrong, set manually)\n","language: en\n","Word Counts: 6787\n","Nrows: 4278\n","4278 train sequences\n","train sequence lengths:\n","\tmean : 8\n","\t95percentile : 17\n","\t99percentile : 24\n","x_train shape: (4278,100)\n","y_train shape: (4278, 2)\n","476 test sequences\n","test sequence lengths:\n","\tmean : 7\n","\t95percentile : 15\n","\t99percentile : 22\n","x_test shape: (476,100)\n","y_test shape: (476, 2)\n","Is Multi-Label? False\n","compiling word ID features...\n","maxlen is 100\n","done.\n","\n","\n","begin training using triangular learning rate policy with max lr of 0.001...\n","Train on 4278 samples, validate on 476 samples\n","Epoch 1/15\n","4278/4278 [==============================] - 7s 2ms/sample - loss: 0.8673 - accuracy: 0.5103 - val_loss: 0.6902 - val_accuracy: 0.5126\n","Epoch 2/15\n","4278/4278 [==============================] - 6s 1ms/sample - loss: 0.7143 - accuracy: 0.5774 - val_loss: 0.6795 - val_accuracy: 0.5126\n","Epoch 3/15\n","4278/4278 [==============================] - 6s 1ms/sample - loss: 0.6183 - accuracy: 0.6613 - val_loss: 0.6420 - val_accuracy: 0.7080\n","Epoch 4/15\n","4278/4278 [==============================] - 6s 1ms/sample - loss: 0.5169 - accuracy: 0.7431 - val_loss: 0.5395 - val_accuracy: 0.8235\n","Epoch 5/15\n","4278/4278 [==============================] - 6s 1ms/sample - loss: 0.3967 - accuracy: 0.8287 - val_loss: 0.4180 - val_accuracy: 0.8319\n","Epoch 6/15\n","4278/4278 [==============================] - 6s 1ms/sample - loss: 0.3006 - accuracy: 0.8813 - val_loss: 0.3581 - val_accuracy: 0.8445\n","Epoch 7/15\n","4278/4278 [==============================] - 6s 1ms/sample - loss: 0.2327 - accuracy: 0.9112 - val_loss: 0.3217 - val_accuracy: 0.8697\n","Epoch 8/15\n","4278/4278 [==============================] - 6s 1ms/sample - loss: 0.1883 - accuracy: 0.9268 - val_loss: 0.2641 - val_accuracy: 0.8971\n","Epoch 9/15\n","4278/4278 [==============================] - 6s 1ms/sample - loss: 0.1482 - accuracy: 0.9462 - val_loss: 0.2491 - val_accuracy: 0.9013\n","Epoch 10/15\n","4278/4278 [==============================] - 6s 2ms/sample - loss: 0.1292 - accuracy: 0.9507 - val_loss: 0.2733 - val_accuracy: 0.8950\n","Epoch 11/15\n","4278/4278 [==============================] - 7s 2ms/sample - loss: 0.1208 - accuracy: 0.9570 - val_loss: 0.2716 - val_accuracy: 0.9034\n","Epoch 12/15\n","4278/4278 [==============================] - 7s 2ms/sample - loss: 0.1097 - accuracy: 0.9612 - val_loss: 0.2788 - val_accuracy: 0.9013\n","Epoch 13/15\n","4278/4278 [==============================] - 6s 1ms/sample - loss: 0.0803 - accuracy: 0.9717 - val_loss: 0.2747 - val_accuracy: 0.9181\n","Epoch 14/15\n","4224/4278 [============================>.] - ETA: 0s - loss: 0.0884 - accuracy: 0.9690\n","Epoch 00014: Reducing Max LR on Plateau: new max lr will be 0.0005 (if not early_stopping).\n","4278/4278 [==============================] - 6s 2ms/sample - loss: 0.0896 - accuracy: 0.9691 - val_loss: 0.2698 - val_accuracy: 0.9139\n","Epoch 15/15\n","4278/4278 [==============================] - 7s 2ms/sample - loss: 0.0716 - accuracy: 0.9759 - val_loss: 0.2639 - val_accuracy: 0.9202\n","Weights from best epoch have been loaded into model.\n","              precision    recall  f1-score   support\n","\n","       funny       0.98      0.86      0.92       244\n","   hilarious       0.87      0.98      0.92       232\n","\n","    accuracy                           0.92       476\n","   macro avg       0.93      0.92      0.92       476\n","weighted avg       0.93      0.92      0.92       476\n","\n","model /content/drive/My Drive/Weight_file/funny_hilarious.h5\n","Model Saved\n","/content/drive/My Drive/updData/Hil-V-fun-memotion_eq_onlyfunny.csv\n","model hilarious_very_funny\n","detected encoding: utf-8 (if wrong, set manually)\n","language: en\n","Word Counts: 6492\n","Nrows: 4086\n","4086 train sequences\n","train sequence lengths:\n","\tmean : 8\n","\t95percentile : 17\n","\t99percentile : 25\n","x_train shape: (4086,100)\n","y_train shape: (4086, 2)\n","454 test sequences\n","test sequence lengths:\n","\tmean : 7\n","\t95percentile : 14\n","\t99percentile : 21\n","x_test shape: (454,100)\n","y_test shape: (454, 2)\n","Is Multi-Label? False\n","compiling word ID features...\n","maxlen is 100\n","done.\n","\n","\n","begin training using triangular learning rate policy with max lr of 0.001...\n","Train on 4086 samples, validate on 454 samples\n","Epoch 1/15\n","4086/4086 [==============================] - 7s 2ms/sample - loss: 0.8903 - accuracy: 0.5338 - val_loss: 0.6900 - val_accuracy: 0.5198\n","Epoch 2/15\n","4086/4086 [==============================] - 6s 2ms/sample - loss: 0.7538 - accuracy: 0.5832 - val_loss: 0.6820 - val_accuracy: 0.6013\n","Epoch 3/15\n","4086/4086 [==============================] - 6s 2ms/sample - loss: 0.6486 - accuracy: 0.6385 - val_loss: 0.6500 - val_accuracy: 0.7775\n","Epoch 4/15\n","4086/4086 [==============================] - 6s 2ms/sample - loss: 0.5456 - accuracy: 0.7186 - val_loss: 0.5547 - val_accuracy: 0.8128\n","Epoch 5/15\n","4086/4086 [==============================] - 6s 2ms/sample - loss: 0.4386 - accuracy: 0.7944 - val_loss: 0.4313 - val_accuracy: 0.8392\n","Epoch 6/15\n","4086/4086 [==============================] - 6s 2ms/sample - loss: 0.3551 - accuracy: 0.8397 - val_loss: 0.3513 - val_accuracy: 0.8612\n","Epoch 7/15\n","4086/4086 [==============================] - 6s 2ms/sample - loss: 0.2877 - accuracy: 0.8759 - val_loss: 0.3125 - val_accuracy: 0.8811\n","Epoch 8/15\n","4086/4086 [==============================] - 6s 2ms/sample - loss: 0.2283 - accuracy: 0.9146 - val_loss: 0.2972 - val_accuracy: 0.8811\n","Epoch 9/15\n","4086/4086 [==============================] - 6s 2ms/sample - loss: 0.1801 - accuracy: 0.9293 - val_loss: 0.2947 - val_accuracy: 0.8767\n","Epoch 10/15\n","4086/4086 [==============================] - 6s 2ms/sample - loss: 0.1560 - accuracy: 0.9361 - val_loss: 0.2860 - val_accuracy: 0.8722\n","Epoch 11/15\n","4086/4086 [==============================] - 6s 2ms/sample - loss: 0.1394 - accuracy: 0.9437 - val_loss: 0.2995 - val_accuracy: 0.8767\n","Epoch 12/15\n","4086/4086 [==============================] - 6s 2ms/sample - loss: 0.1205 - accuracy: 0.9513 - val_loss: 0.2890 - val_accuracy: 0.8789\n","Epoch 13/15\n","4086/4086 [==============================] - 6s 2ms/sample - loss: 0.1048 - accuracy: 0.9611 - val_loss: 0.2937 - val_accuracy: 0.8789\n","Epoch 14/15\n","4086/4086 [==============================] - 6s 2ms/sample - loss: 0.0929 - accuracy: 0.9697 - val_loss: 0.3023 - val_accuracy: 0.8899\n","Epoch 15/15\n","4086/4086 [==============================] - 6s 2ms/sample - loss: 0.0912 - accuracy: 0.9650 - val_loss: 0.2859 - val_accuracy: 0.8943\n","Weights from best epoch have been loaded into model.\n","              precision    recall  f1-score   support\n","\n","   hilarious       0.84      0.98      0.91       235\n","  very_funny       0.98      0.80      0.88       219\n","\n","    accuracy                           0.89       454\n","   macro avg       0.91      0.89      0.89       454\n","weighted avg       0.91      0.89      0.89       454\n","\n","model /content/drive/My Drive/Weight_file/hilarious_very_funny.h5\n","Model Saved\n","/content/drive/My Drive/updData/Twisted-V-twisted-memotion_eq_onlysarcastic.csv\n","model twisted_meaning_very_twisted\n","detected encoding: utf-8 (if wrong, set manually)\n","language: en\n","Word Counts: 5134\n","Nrows: 2810\n","2810 train sequences\n","train sequence lengths:\n","\tmean : 8\n","\t95percentile : 17\n","\t99percentile : 24\n","x_train shape: (2810,100)\n","y_train shape: (2810, 2)\n","313 test sequences\n","test sequence lengths:\n","\tmean : 7\n","\t95percentile : 18\n","\t99percentile : 31\n","x_test shape: (313,100)\n","y_test shape: (313, 2)\n","Is Multi-Label? False\n","compiling word ID features...\n","maxlen is 100\n","done.\n","\n","\n","begin training using triangular learning rate policy with max lr of 0.001...\n","Train on 2810 samples, validate on 313 samples\n","Epoch 1/15\n","2810/2810 [==============================] - 7s 3ms/sample - loss: 0.9287 - accuracy: 0.5107 - val_loss: 0.6911 - val_accuracy: 0.5335\n","Epoch 2/15\n","2810/2810 [==============================] - 4s 1ms/sample - loss: 0.7448 - accuracy: 0.5961 - val_loss: 0.6848 - val_accuracy: 0.7412\n","Epoch 3/15\n","2810/2810 [==============================] - 4s 1ms/sample - loss: 0.6351 - accuracy: 0.6548 - val_loss: 0.6694 - val_accuracy: 0.7700\n","Epoch 4/15\n","2810/2810 [==============================] - 4s 1ms/sample - loss: 0.5610 - accuracy: 0.7100 - val_loss: 0.6343 - val_accuracy: 0.8275\n","Epoch 5/15\n","2810/2810 [==============================] - 4s 1ms/sample - loss: 0.4636 - accuracy: 0.7762 - val_loss: 0.5523 - val_accuracy: 0.8818\n","Epoch 6/15\n","2810/2810 [==============================] - 4s 1ms/sample - loss: 0.3714 - accuracy: 0.8363 - val_loss: 0.4404 - val_accuracy: 0.9073\n","Epoch 7/15\n","2810/2810 [==============================] - 4s 1ms/sample - loss: 0.2922 - accuracy: 0.8797 - val_loss: 0.3185 - val_accuracy: 0.9169\n","Epoch 8/15\n","2810/2810 [==============================] - 4s 1ms/sample - loss: 0.2298 - accuracy: 0.9121 - val_loss: 0.2390 - val_accuracy: 0.9393\n","Epoch 9/15\n","2810/2810 [==============================] - 4s 1ms/sample - loss: 0.1990 - accuracy: 0.9228 - val_loss: 0.2053 - val_accuracy: 0.9393\n","Epoch 10/15\n","2810/2810 [==============================] - 4s 1ms/sample - loss: 0.1658 - accuracy: 0.9391 - val_loss: 0.1926 - val_accuracy: 0.9329\n","Epoch 11/15\n","2810/2810 [==============================] - 4s 1ms/sample - loss: 0.1408 - accuracy: 0.9484 - val_loss: 0.1842 - val_accuracy: 0.9297\n","Epoch 12/15\n","2810/2810 [==============================] - 4s 1ms/sample - loss: 0.1079 - accuracy: 0.9637 - val_loss: 0.1763 - val_accuracy: 0.9361\n","Epoch 13/15\n","2810/2810 [==============================] - 4s 1ms/sample - loss: 0.0933 - accuracy: 0.9690 - val_loss: 0.1582 - val_accuracy: 0.9457\n","Epoch 14/15\n","2810/2810 [==============================] - 4s 1ms/sample - loss: 0.1060 - accuracy: 0.9601 - val_loss: 0.1696 - val_accuracy: 0.9329\n","Epoch 15/15\n","2810/2810 [==============================] - 4s 1ms/sample - loss: 0.0761 - accuracy: 0.9737 - val_loss: 0.1555 - val_accuracy: 0.9489\n","Weights from best epoch have been loaded into model.\n","                 precision    recall  f1-score   support\n","\n","twisted_meaning       1.00      0.89      0.94       151\n","   very_twisted       0.91      1.00      0.95       162\n","\n","       accuracy                           0.95       313\n","      macro avg       0.96      0.95      0.95       313\n","   weighted avg       0.95      0.95      0.95       313\n","\n","model /content/drive/My Drive/Weight_file/twisted_meaning_very_twisted.h5\n","Model Saved\n","/content/drive/My Drive/updData/Gen-V-twisted-memotion_eq_onlysarcastic.csv\n","model general_very_twisted\n","detected encoding: utf-8 (if wrong, set manually)\n","language: en\n","Word Counts: 7898\n","Nrows: 6347\n","6347 train sequences\n","train sequence lengths:\n","\tmean : 8\n","\t95percentile : 18\n","\t99percentile : 24\n","x_train shape: (6347,100)\n","y_train shape: (6347, 2)\n","706 test sequences\n","test sequence lengths:\n","\tmean : 7\n","\t95percentile : 16\n","\t99percentile : 23\n","x_test shape: (706,100)\n","y_test shape: (706, 2)\n","Is Multi-Label? False\n","compiling word ID features...\n","maxlen is 100\n","done.\n","\n","\n","begin training using triangular learning rate policy with max lr of 0.001...\n","Train on 6347 samples, validate on 706 samples\n","Epoch 1/15\n","6347/6347 [==============================] - 10s 2ms/sample - loss: 0.8084 - accuracy: 0.5548 - val_loss: 0.6841 - val_accuracy: 0.7550\n","Epoch 2/15\n","6347/6347 [==============================] - 9s 1ms/sample - loss: 0.5964 - accuracy: 0.6816 - val_loss: 0.5949 - val_accuracy: 0.9150\n","Epoch 3/15\n","6347/6347 [==============================] - 9s 1ms/sample - loss: 0.4119 - accuracy: 0.8095 - val_loss: 0.3105 - val_accuracy: 0.9533\n","Epoch 4/15\n","6347/6347 [==============================] - 9s 1ms/sample - loss: 0.2675 - accuracy: 0.8929 - val_loss: 0.1548 - val_accuracy: 0.9533\n","Epoch 5/15\n","6347/6347 [==============================] - 9s 1ms/sample - loss: 0.1816 - accuracy: 0.9316 - val_loss: 0.1170 - val_accuracy: 0.9618\n","Epoch 6/15\n","6347/6347 [==============================] - 9s 1ms/sample - loss: 0.1286 - accuracy: 0.9535 - val_loss: 0.1057 - val_accuracy: 0.9575\n","Epoch 7/15\n","6347/6347 [==============================] - 9s 1ms/sample - loss: 0.0956 - accuracy: 0.9679 - val_loss: 0.1007 - val_accuracy: 0.9589\n","Epoch 8/15\n","6347/6347 [==============================] - 10s 2ms/sample - loss: 0.0749 - accuracy: 0.9734 - val_loss: 0.0947 - val_accuracy: 0.9618\n","Epoch 9/15\n","6347/6347 [==============================] - 9s 1ms/sample - loss: 0.0680 - accuracy: 0.9756 - val_loss: 0.0971 - val_accuracy: 0.9660\n","Epoch 10/15\n","6347/6347 [==============================] - 9s 1ms/sample - loss: 0.0569 - accuracy: 0.9801 - val_loss: 0.1291 - val_accuracy: 0.9533\n","Epoch 11/15\n","6347/6347 [==============================] - 9s 1ms/sample - loss: 0.0493 - accuracy: 0.9842 - val_loss: 0.1067 - val_accuracy: 0.9646\n","Epoch 12/15\n","6347/6347 [==============================] - 9s 1ms/sample - loss: 0.0537 - accuracy: 0.9825 - val_loss: 0.1130 - val_accuracy: 0.9632\n","Epoch 13/15\n","6304/6347 [============================>.] - ETA: 0s - loss: 0.0492 - accuracy: 0.9843\n","Epoch 00013: Reducing Max LR on Plateau: new max lr will be 0.0005 (if not early_stopping).\n","6347/6347 [==============================] - 9s 1ms/sample - loss: 0.0490 - accuracy: 0.9844 - val_loss: 0.1181 - val_accuracy: 0.9618\n","Epoch 14/15\n","6347/6347 [==============================] - 9s 1ms/sample - loss: 0.0439 - accuracy: 0.9869 - val_loss: 0.1028 - val_accuracy: 0.9603\n","Epoch 15/15\n","6347/6347 [==============================] - 9s 1ms/sample - loss: 0.0418 - accuracy: 0.9874 - val_loss: 0.0935 - val_accuracy: 0.9632\n","Weights from best epoch have been loaded into model.\n","              precision    recall  f1-score   support\n","\n","     general       1.00      0.92      0.96       327\n","very_twisted       0.94      1.00      0.97       379\n","\n","    accuracy                           0.96       706\n","   macro avg       0.97      0.96      0.96       706\n","weighted avg       0.97      0.96      0.96       706\n","\n","model /content/drive/My Drive/Weight_file/general_very_twisted.h5\n","Model Saved\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"o9EPfBGn91vz","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nmmk1RiPAH8P","colab_type":"code","outputId":"ce5b4d87-0904-4241-b087-2925967b0bfe","executionInfo":{"status":"ok","timestamp":1583853998707,"user_tz":-330,"elapsed":191877,"user":{"displayName":"AMBUJE GUPTA","photoUrl":"","userId":"08374827863947652219"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import ktrain\n","from ktrain import text\n","\n","\n","modelslist = [['twisted_meaning', 'general']]\n","data_files = ['Gen-twisted-memotion_eq_onlysarcastic.csv']\n","\n","for columnclass, data_file in zip(modelslist, data_files):\n","\n","    \n","    columns=columnclass      \n","    DATA_PATH = '/content/drive/My Drive/updData/' + data_file\n","    print(DATA_PATH)\n","\n","    NUM_WORDS = 90000\n","    MAXLEN = 100\n","\n","    h5name = '_'.join(columnclass)\n","    print('model', h5name)\n","\n","    (x_train, y_train), (x_test, y_test), preproc = text.texts_from_csv(DATA_PATH,\n","                          'cleaned_ocr',\n","                          label_columns = columns,\n","                          val_filepath=None, # if None, 10% of data will be used for validation\n","                          max_features=NUM_WORDS, maxlen=MAXLEN,\n","                        ngram_range=1,)\n","\n","\n","    model = text.text_classifier('fasttext', (x_train, y_train), preproc=preproc)\n","\n","    learner = ktrain.get_learner(model, train_data=(x_train, y_train), val_data=(x_test, y_test))\n","    a='/content/drive/My Drive/Weight_file/'+h5name\n","    learner.autofit(0.001, 30,early_stopping=12, reduce_on_plateau=5)\n","\n","    a=learner.validate(val_data=(x_test, y_test), class_names=columns)\n","  \n","    predictor = ktrain.get_predictor(learner.model, preproc)\n","\n","   # score = 'xyz'########################\n","    h5name = \"/content/drive/My Drive/Weight_file/\"+h5name +  '.h5'\n","    print('model', h5name)\n","\n","    predictor.save(h5name)\n","    print(\"Model Saved\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/updData/Gen-twisted-memotion_eq_onlysarcastic.csv\n","model twisted_meaning_general\n","detected encoding: utf-8 (if wrong, set manually)\n","language: en\n","Word Counts: 9407\n","Nrows: 5940\n","5940 train sequences\n","train sequence lengths:\n","\tmean : 8\n","\t95percentile : 17\n","\t99percentile : 27\n","x_train shape: (5940,100)\n","y_train shape: (5940, 2)\n","661 test sequences\n","test sequence lengths:\n","\tmean : 7\n","\t95percentile : 15\n","\t99percentile : 26\n","x_test shape: (661,100)\n","y_test shape: (661, 2)\n","Is Multi-Label? False\n","compiling word ID features...\n","maxlen is 100\n","done.\n","\n","\n","begin training using triangular learning rate policy with max lr of 0.001...\n","Train on 5940 samples, validate on 661 samples\n","Epoch 1/30\n","5940/5940 [==============================] - 10s 2ms/sample - loss: 0.8712 - accuracy: 0.5251 - val_loss: 0.6896 - val_accuracy: 0.5295\n","Epoch 2/30\n","5940/5940 [==============================] - 9s 1ms/sample - loss: 0.7596 - accuracy: 0.5460 - val_loss: 0.6860 - val_accuracy: 0.5567\n","Epoch 3/30\n","5940/5940 [==============================] - 9s 1ms/sample - loss: 0.6725 - accuracy: 0.5951 - val_loss: 0.6652 - val_accuracy: 0.6369\n","Epoch 4/30\n","5940/5940 [==============================] - 9s 1ms/sample - loss: 0.6038 - accuracy: 0.6715 - val_loss: 0.6245 - val_accuracy: 0.6778\n","Epoch 5/30\n","5940/5940 [==============================] - 9s 1ms/sample - loss: 0.5406 - accuracy: 0.7301 - val_loss: 0.5771 - val_accuracy: 0.7156\n","Epoch 6/30\n","5940/5940 [==============================] - 9s 1ms/sample - loss: 0.4510 - accuracy: 0.7884 - val_loss: 0.5293 - val_accuracy: 0.7474\n","Epoch 7/30\n","5940/5940 [==============================] - 9s 1ms/sample - loss: 0.3730 - accuracy: 0.8377 - val_loss: 0.5036 - val_accuracy: 0.7776\n","Epoch 8/30\n","5940/5940 [==============================] - 8s 1ms/sample - loss: 0.3058 - accuracy: 0.8689 - val_loss: 0.5017 - val_accuracy: 0.7700\n","Epoch 9/30\n","5940/5940 [==============================] - 8s 1ms/sample - loss: 0.2753 - accuracy: 0.8854 - val_loss: 0.5201 - val_accuracy: 0.7685\n","Epoch 10/30\n","5940/5940 [==============================] - 8s 1ms/sample - loss: 0.2184 - accuracy: 0.9128 - val_loss: 0.4954 - val_accuracy: 0.7897\n","Epoch 11/30\n","5940/5940 [==============================] - 8s 1ms/sample - loss: 0.1969 - accuracy: 0.9234 - val_loss: 0.5400 - val_accuracy: 0.7973\n","Epoch 12/30\n","5940/5940 [==============================] - 9s 1ms/sample - loss: 0.1768 - accuracy: 0.9310 - val_loss: 0.5357 - val_accuracy: 0.7943\n","Epoch 13/30\n","5940/5940 [==============================] - 9s 1ms/sample - loss: 0.1547 - accuracy: 0.9418 - val_loss: 0.5422 - val_accuracy: 0.8094\n","Epoch 14/30\n","5940/5940 [==============================] - 9s 1ms/sample - loss: 0.1434 - accuracy: 0.9444 - val_loss: 0.5713 - val_accuracy: 0.8094\n","Epoch 15/30\n","5888/5940 [============================>.] - ETA: 0s - loss: 0.1371 - accuracy: 0.9479\n","Epoch 00015: Reducing Max LR on Plateau: new max lr will be 0.0005 (if not early_stopping).\n","5940/5940 [==============================] - 9s 1ms/sample - loss: 0.1365 - accuracy: 0.9483 - val_loss: 0.5561 - val_accuracy: 0.8169\n","Epoch 16/30\n","5940/5940 [==============================] - 9s 1ms/sample - loss: 0.1232 - accuracy: 0.9582 - val_loss: 0.5811 - val_accuracy: 0.8169\n","Epoch 17/30\n","5940/5940 [==============================] - 9s 1ms/sample - loss: 0.0965 - accuracy: 0.9652 - val_loss: 0.5804 - val_accuracy: 0.8139\n","Epoch 18/30\n","5940/5940 [==============================] - 9s 1ms/sample - loss: 0.0959 - accuracy: 0.9635 - val_loss: 0.6105 - val_accuracy: 0.8064\n","Epoch 19/30\n","5940/5940 [==============================] - 9s 1ms/sample - loss: 0.0838 - accuracy: 0.9709 - val_loss: 0.5967 - val_accuracy: 0.8124\n","Epoch 20/30\n","5888/5940 [============================>.] - ETA: 0s - loss: 0.0760 - accuracy: 0.9716\n","Epoch 00020: Reducing Max LR on Plateau: new max lr will be 0.00025 (if not early_stopping).\n","5940/5940 [==============================] - 8s 1ms/sample - loss: 0.0757 - accuracy: 0.9717 - val_loss: 0.5948 - val_accuracy: 0.8215\n","Epoch 21/30\n","5940/5940 [==============================] - 9s 1ms/sample - loss: 0.0870 - accuracy: 0.9662 - val_loss: 0.6158 - val_accuracy: 0.8185\n","Epoch 22/30\n","5920/5940 [============================>.] - ETA: 0s - loss: 0.0668 - accuracy: 0.9720Restoring model weights from the end of the best epoch.\n","5940/5940 [==============================] - 9s 1ms/sample - loss: 0.0668 - accuracy: 0.9719 - val_loss: 0.6016 - val_accuracy: 0.8154\n","Epoch 00022: early stopping\n","Weights from best epoch have been loaded into model.\n","                 precision    recall  f1-score   support\n","\n","twisted_meaning       0.73      0.89      0.80       313\n","        general       0.88      0.70      0.78       348\n","\n","       accuracy                           0.79       661\n","      macro avg       0.80      0.79      0.79       661\n","   weighted avg       0.81      0.79      0.79       661\n","\n","model /content/drive/My Drive/Weight_file/twisted_meaning_general.h5\n","Model Saved\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pkNWYovZAH3D","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QVFa0Xk2Dy7F","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"I_CQ0XK6Dy6A","colab_type":"code","outputId":"ed904226-ad2b-451d-b2a6-741651e363e5","executionInfo":{"status":"ok","timestamp":1583854777883,"user_tz":-330,"elapsed":359905,"user":{"displayName":"AMBUJE GUPTA","photoUrl":"","userId":"08374827863947652219"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import ktrain\n","from ktrain import text\n","\n","\n","\n","modelslist = [['funny','very_funny'],['very_offensive', 'slight'],['hateful_offensive', 'slight'],['very_offensive', 'hateful_offensive']]\n","data_files = ['Fun-V-fun-memotion_eq_onlyfunny.csv','Slight-V-off-memotion_eq_onlyoffensive.csv','Slight-H-off-memotion_eq_onlyoffensive.csv','V-off-H-off-memotion_eq_onlyoffensive.csv']\n","\n","for columnclass, data_file in zip(modelslist, data_files):\n","\n","    \n","    columns=columnclass      \n","    DATA_PATH = '/content/drive/My Drive/data_eqd/' + data_file\n","    print(DATA_PATH)\n","\n","    NUM_WORDS = 90000\n","    MAXLEN = 100\n","\n","    h5name = '_'.join(columnclass)\n","    print('model', h5name)\n","\n","    (x_train, y_train), (x_test, y_test), preproc = text.texts_from_csv(DATA_PATH,\n","                          'cleaned_ocr',\n","                          label_columns = columns,\n","                          val_filepath=None, # if None, 10% of data will be used for validation\n","                          max_features=NUM_WORDS, maxlen=MAXLEN,\n","                        ngram_range=1,)\n","\n","\n","    model = text.text_classifier('fasttext', (x_train, y_train), preproc=preproc)\n","\n","    learner = ktrain.get_learner(model, train_data=(x_train, y_train), val_data=(x_test, y_test))\n","    a='/content/drive/My Drive/Weight_file/'+h5name\n","    learner.autofit(0.001, 15,early_stopping=8, reduce_on_plateau=5)\n","\n","    a=learner.validate(val_data=(x_test, y_test), class_names=columns)\n","  \n","    predictor = ktrain.get_predictor(learner.model, preproc)\n","\n","   # score = 'xyz'########################\n","    h5name = \"/content/drive/My Drive/Weight_file/\"+h5name +  '.h5'\n","    print('model', h5name)\n","\n","    predictor.save(h5name)\n","    print(\"Model Saved\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/data_eqd/Fun-V-fun-memotion_eq_onlyfunny.csv\n","model funny_very_funny\n","detected encoding: utf-8 (if wrong, set manually)\n","language: en\n","Word Counts: 8668\n","Nrows: 4221\n","4221 train sequences\n","train sequence lengths:\n","\tmean : 8\n","\t95percentile : 17\n","\t99percentile : 25\n","x_train shape: (4221,100)\n","y_train shape: (4221, 2)\n","469 test sequences\n","test sequence lengths:\n","\tmean : 6\n","\t95percentile : 14\n","\t99percentile : 19\n","x_test shape: (469,100)\n","y_test shape: (469, 2)\n","Is Multi-Label? False\n","compiling word ID features...\n","maxlen is 100\n","done.\n","\n","\n","begin training using triangular learning rate policy with max lr of 0.001...\n","Train on 4221 samples, validate on 469 samples\n","Epoch 1/15\n","4221/4221 [==============================] - 7s 2ms/sample - loss: 0.8824 - accuracy: 0.5051 - val_loss: 0.6924 - val_accuracy: 0.5288\n","Epoch 2/15\n","4221/4221 [==============================] - 6s 1ms/sample - loss: 0.7901 - accuracy: 0.5195 - val_loss: 0.6922 - val_accuracy: 0.5288\n","Epoch 3/15\n","4221/4221 [==============================] - 6s 1ms/sample - loss: 0.7193 - accuracy: 0.5669 - val_loss: 0.6939 - val_accuracy: 0.5394\n","Epoch 4/15\n","4221/4221 [==============================] - 6s 1ms/sample - loss: 0.6848 - accuracy: 0.5890 - val_loss: 0.7037 - val_accuracy: 0.4712\n","Epoch 5/15\n","4221/4221 [==============================] - 6s 1ms/sample - loss: 0.6379 - accuracy: 0.6399 - val_loss: 0.7179 - val_accuracy: 0.4925\n","Epoch 6/15\n","4221/4221 [==============================] - 6s 1ms/sample - loss: 0.5983 - accuracy: 0.6752 - val_loss: 0.7462 - val_accuracy: 0.4819\n","Epoch 7/15\n","4160/4221 [============================>.] - ETA: 0s - loss: 0.5415 - accuracy: 0.7288\n","Epoch 00007: Reducing Max LR on Plateau: new max lr will be 0.0005 (if not early_stopping).\n","4221/4221 [==============================] - 6s 1ms/sample - loss: 0.5417 - accuracy: 0.7287 - val_loss: 0.7831 - val_accuracy: 0.4755\n","Epoch 8/15\n","4221/4221 [==============================] - 6s 1ms/sample - loss: 0.4783 - accuracy: 0.7685 - val_loss: 0.8053 - val_accuracy: 0.4968\n","Epoch 9/15\n","4221/4221 [==============================] - 6s 1ms/sample - loss: 0.4267 - accuracy: 0.8036 - val_loss: 0.8484 - val_accuracy: 0.4925\n","Epoch 10/15\n","4192/4221 [============================>.] - ETA: 0s - loss: 0.4096 - accuracy: 0.8108Restoring model weights from the end of the best epoch.\n","4221/4221 [==============================] - 6s 1ms/sample - loss: 0.4093 - accuracy: 0.8109 - val_loss: 0.8798 - val_accuracy: 0.4925\n","Epoch 00010: early stopping\n","Weights from best epoch have been loaded into model.\n","              precision    recall  f1-score   support\n","\n","       funny       0.53      0.99      0.69       248\n","  very_funny       0.50      0.01      0.03       221\n","\n","    accuracy                           0.53       469\n","   macro avg       0.51      0.50      0.36       469\n","weighted avg       0.52      0.53      0.38       469\n","\n","model /content/drive/My Drive/Weight_file/funny_very_funny.h5\n","Model Saved\n","/content/drive/My Drive/data_eqd/Slight-V-off-memotion_eq_onlyoffensive.csv\n","model very_offensive_slight\n","detected encoding: utf-8 (if wrong, set manually)\n","language: en\n","Word Counts: 8076\n","Nrows: 4552\n","4552 train sequences\n","train sequence lengths:\n","\tmean : 8\n","\t95percentile : 17\n","\t99percentile : 25\n","x_train shape: (4552,100)\n","y_train shape: (4552, 2)\n","506 test sequences\n","test sequence lengths:\n","\tmean : 7\n","\t95percentile : 16\n","\t99percentile : 23\n","x_test shape: (506,100)\n","y_test shape: (506, 2)\n","Is Multi-Label? False\n","compiling word ID features...\n","maxlen is 100\n","done.\n","\n","\n","begin training using triangular learning rate policy with max lr of 0.001...\n","Train on 4552 samples, validate on 506 samples\n","Epoch 1/15\n","4552/4552 [==============================] - 7s 2ms/sample - loss: 0.9126 - accuracy: 0.4965 - val_loss: 0.6927 - val_accuracy: 0.5079\n","Epoch 2/15\n","4552/4552 [==============================] - 7s 1ms/sample - loss: 0.7714 - accuracy: 0.5609 - val_loss: 0.6904 - val_accuracy: 0.5455\n","Epoch 3/15\n","4552/4552 [==============================] - 7s 1ms/sample - loss: 0.7085 - accuracy: 0.5830 - val_loss: 0.6809 - val_accuracy: 0.6087\n","Epoch 4/15\n","4552/4552 [==============================] - 6s 1ms/sample - loss: 0.6351 - accuracy: 0.6360 - val_loss: 0.6564 - val_accuracy: 0.6383\n","Epoch 5/15\n","4552/4552 [==============================] - 6s 1ms/sample - loss: 0.5679 - accuracy: 0.7041 - val_loss: 0.6162 - val_accuracy: 0.6818\n","Epoch 6/15\n","4552/4552 [==============================] - 6s 1ms/sample - loss: 0.5166 - accuracy: 0.7436 - val_loss: 0.5952 - val_accuracy: 0.6957\n","Epoch 7/15\n","4552/4552 [==============================] - 6s 1ms/sample - loss: 0.4228 - accuracy: 0.8089 - val_loss: 0.5824 - val_accuracy: 0.6798\n","Epoch 8/15\n","4552/4552 [==============================] - 6s 1ms/sample - loss: 0.3672 - accuracy: 0.8361 - val_loss: 0.5801 - val_accuracy: 0.6897\n","Epoch 9/15\n","4552/4552 [==============================] - 6s 1ms/sample - loss: 0.3078 - accuracy: 0.8702 - val_loss: 0.5848 - val_accuracy: 0.7016\n","Epoch 10/15\n","4552/4552 [==============================] - 6s 1ms/sample - loss: 0.2574 - accuracy: 0.8930 - val_loss: 0.6003 - val_accuracy: 0.7174\n","Epoch 11/15\n","4552/4552 [==============================] - 6s 1ms/sample - loss: 0.2358 - accuracy: 0.9058 - val_loss: 0.6046 - val_accuracy: 0.7312\n","Epoch 12/15\n","4552/4552 [==============================] - 6s 1ms/sample - loss: 0.2084 - accuracy: 0.9189 - val_loss: 0.6302 - val_accuracy: 0.7490\n","Epoch 13/15\n","4544/4552 [============================>.] - ETA: 0s - loss: 0.1853 - accuracy: 0.9342\n","Epoch 00013: Reducing Max LR on Plateau: new max lr will be 0.0005 (if not early_stopping).\n","4552/4552 [==============================] - 6s 1ms/sample - loss: 0.1852 - accuracy: 0.9343 - val_loss: 0.6369 - val_accuracy: 0.7470\n","Epoch 14/15\n","4552/4552 [==============================] - 6s 1ms/sample - loss: 0.1614 - accuracy: 0.9348 - val_loss: 0.6283 - val_accuracy: 0.7431\n","Epoch 15/15\n","4552/4552 [==============================] - 6s 1ms/sample - loss: 0.1511 - accuracy: 0.9435 - val_loss: 0.6302 - val_accuracy: 0.7549\n","Weights from best epoch have been loaded into model.\n","                precision    recall  f1-score   support\n","\n","very_offensive       0.71      0.84      0.77       250\n","        slight       0.81      0.67      0.74       256\n","\n","      accuracy                           0.75       506\n","     macro avg       0.76      0.76      0.75       506\n","  weighted avg       0.76      0.75      0.75       506\n","\n","model /content/drive/My Drive/Weight_file/very_offensive_slight.h5\n","Model Saved\n","/content/drive/My Drive/data_eqd/Slight-H-off-memotion_eq_onlyoffensive.csv\n","model hateful_offensive_slight\n","detected encoding: utf-8 (if wrong, set manually)\n","language: en\n","Word Counts: 6376\n","Nrows: 4520\n","4520 train sequences\n","train sequence lengths:\n","\tmean : 8\n","\t95percentile : 18\n","\t99percentile : 27\n","x_train shape: (4520,100)\n","y_train shape: (4520, 2)\n","503 test sequences\n","test sequence lengths:\n","\tmean : 7\n","\t95percentile : 16\n","\t99percentile : 25\n","x_test shape: (503,100)\n","y_test shape: (503, 2)\n","Is Multi-Label? False\n","compiling word ID features...\n","maxlen is 100\n","done.\n","\n","\n","begin training using triangular learning rate policy with max lr of 0.001...\n","Train on 4520 samples, validate on 503 samples\n","Epoch 1/15\n","4520/4520 [==============================] - 7s 2ms/sample - loss: 0.8468 - accuracy: 0.5597 - val_loss: 0.6915 - val_accuracy: 0.4811\n","Epoch 2/15\n","4520/4520 [==============================] - 6s 1ms/sample - loss: 0.6319 - accuracy: 0.6688 - val_loss: 0.6531 - val_accuracy: 0.5706\n","Epoch 3/15\n","4520/4520 [==============================] - 6s 1ms/sample - loss: 0.4256 - accuracy: 0.8007 - val_loss: 0.5093 - val_accuracy: 0.8350\n","Epoch 4/15\n","4520/4520 [==============================] - 6s 1ms/sample - loss: 0.2642 - accuracy: 0.8973 - val_loss: 0.2780 - val_accuracy: 0.9085\n","Epoch 5/15\n","4520/4520 [==============================] - 6s 1ms/sample - loss: 0.1839 - accuracy: 0.9296 - val_loss: 0.1645 - val_accuracy: 0.9404\n","Epoch 6/15\n","4520/4520 [==============================] - 6s 1ms/sample - loss: 0.1204 - accuracy: 0.9573 - val_loss: 0.1256 - val_accuracy: 0.9583\n","Epoch 7/15\n","4520/4520 [==============================] - 6s 1ms/sample - loss: 0.0840 - accuracy: 0.9721 - val_loss: 0.1192 - val_accuracy: 0.9563\n","Epoch 8/15\n","4520/4520 [==============================] - 6s 1ms/sample - loss: 0.0763 - accuracy: 0.9737 - val_loss: 0.1163 - val_accuracy: 0.9543\n","Epoch 9/15\n","4520/4520 [==============================] - 6s 1ms/sample - loss: 0.0723 - accuracy: 0.9757 - val_loss: 0.0954 - val_accuracy: 0.9563\n","Epoch 10/15\n","4520/4520 [==============================] - 6s 1ms/sample - loss: 0.0592 - accuracy: 0.9803 - val_loss: 0.1140 - val_accuracy: 0.9642\n","Epoch 11/15\n","4520/4520 [==============================] - 6s 1ms/sample - loss: 0.0485 - accuracy: 0.9845 - val_loss: 0.0940 - val_accuracy: 0.9642\n","Epoch 12/15\n","4520/4520 [==============================] - 6s 1ms/sample - loss: 0.0420 - accuracy: 0.9867 - val_loss: 0.1082 - val_accuracy: 0.9642\n","Epoch 13/15\n","4520/4520 [==============================] - 6s 1ms/sample - loss: 0.0390 - accuracy: 0.9861 - val_loss: 0.1032 - val_accuracy: 0.9662\n","Epoch 14/15\n","4520/4520 [==============================] - 6s 1ms/sample - loss: 0.0335 - accuracy: 0.9907 - val_loss: 0.1009 - val_accuracy: 0.9602\n","Epoch 15/15\n","4520/4520 [==============================] - 7s 1ms/sample - loss: 0.0304 - accuracy: 0.9909 - val_loss: 0.0931 - val_accuracy: 0.9622\n","Weights from best epoch have been loaded into model.\n","                   precision    recall  f1-score   support\n","\n","hateful_offensive       0.93      1.00      0.96       242\n","           slight       1.00      0.93      0.96       261\n","\n","         accuracy                           0.96       503\n","        macro avg       0.96      0.96      0.96       503\n","     weighted avg       0.96      0.96      0.96       503\n","\n","model /content/drive/My Drive/Weight_file/hateful_offensive_slight.h5\n","Model Saved\n","/content/drive/My Drive/data_eqd/V-off-H-off-memotion_eq_onlyoffensive.csv\n","model very_offensive_hateful_offensive\n","detected encoding: utf-8 (if wrong, set manually)\n","language: en\n","Word Counts: 4732\n","Nrows: 4407\n","4407 train sequences\n","train sequence lengths:\n","\tmean : 8\n","\t95percentile : 18\n","\t99percentile : 27\n","x_train shape: (4407,100)\n","y_train shape: (4407, 2)\n","490 test sequences\n","test sequence lengths:\n","\tmean : 8\n","\t95percentile : 19\n","\t99percentile : 27\n","x_test shape: (490,100)\n","y_test shape: (490, 2)\n","Is Multi-Label? False\n","compiling word ID features...\n","maxlen is 100\n","done.\n","\n","\n","begin training using triangular learning rate policy with max lr of 0.001...\n","Train on 4407 samples, validate on 490 samples\n","Epoch 1/15\n","4407/4407 [==============================] - 7s 2ms/sample - loss: 0.8206 - accuracy: 0.5677 - val_loss: 0.6843 - val_accuracy: 0.5102\n","Epoch 2/15\n","4407/4407 [==============================] - 7s 1ms/sample - loss: 0.5707 - accuracy: 0.6978 - val_loss: 0.6391 - val_accuracy: 0.5755\n","Epoch 3/15\n","4407/4407 [==============================] - 6s 1ms/sample - loss: 0.3856 - accuracy: 0.8221 - val_loss: 0.4667 - val_accuracy: 0.8898\n","Epoch 4/15\n","4407/4407 [==============================] - 6s 1ms/sample - loss: 0.2437 - accuracy: 0.9047 - val_loss: 0.2207 - val_accuracy: 0.9571\n","Epoch 5/15\n","4407/4407 [==============================] - 6s 1ms/sample - loss: 0.1557 - accuracy: 0.9487 - val_loss: 0.0985 - val_accuracy: 0.9673\n","Epoch 6/15\n","4407/4407 [==============================] - 6s 1ms/sample - loss: 0.1050 - accuracy: 0.9641 - val_loss: 0.0693 - val_accuracy: 0.9776\n","Epoch 7/15\n","4407/4407 [==============================] - 7s 1ms/sample - loss: 0.0765 - accuracy: 0.9753 - val_loss: 0.0636 - val_accuracy: 0.9796\n","Epoch 8/15\n","4407/4407 [==============================] - 6s 1ms/sample - loss: 0.0603 - accuracy: 0.9794 - val_loss: 0.0569 - val_accuracy: 0.9796\n","Epoch 9/15\n","4407/4407 [==============================] - 6s 1ms/sample - loss: 0.0441 - accuracy: 0.9857 - val_loss: 0.0495 - val_accuracy: 0.9796\n","Epoch 10/15\n","4407/4407 [==============================] - 6s 1ms/sample - loss: 0.0343 - accuracy: 0.9902 - val_loss: 0.0594 - val_accuracy: 0.9796\n","Epoch 11/15\n","4407/4407 [==============================] - 6s 1ms/sample - loss: 0.0358 - accuracy: 0.9871 - val_loss: 0.0525 - val_accuracy: 0.9776\n","Epoch 12/15\n","4407/4407 [==============================] - 6s 1ms/sample - loss: 0.0330 - accuracy: 0.9900 - val_loss: 0.0551 - val_accuracy: 0.9837\n","Epoch 13/15\n","4407/4407 [==============================] - 6s 1ms/sample - loss: 0.0356 - accuracy: 0.9864 - val_loss: 0.0510 - val_accuracy: 0.9816\n","Epoch 14/15\n","4384/4407 [============================>.] - ETA: 0s - loss: 0.0275 - accuracy: 0.9927\n","Epoch 00014: Reducing Max LR on Plateau: new max lr will be 0.0005 (if not early_stopping).\n","4407/4407 [==============================] - 6s 1ms/sample - loss: 0.0277 - accuracy: 0.9927 - val_loss: 0.0562 - val_accuracy: 0.9837\n","Epoch 15/15\n","4407/4407 [==============================] - 6s 1ms/sample - loss: 0.0268 - accuracy: 0.9905 - val_loss: 0.0567 - val_accuracy: 0.9837\n","Weights from best epoch have been loaded into model.\n","                   precision    recall  f1-score   support\n","\n","   very_offensive       1.00      0.97      0.98       240\n","hateful_offensive       0.97      1.00      0.98       250\n","\n","         accuracy                           0.98       490\n","        macro avg       0.98      0.98      0.98       490\n","     weighted avg       0.98      0.98      0.98       490\n","\n","model /content/drive/My Drive/Weight_file/very_offensive_hateful_offensive.h5\n","Model Saved\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"w88GuYYhDy0c","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PI5_EiTGGim5","colab_type":"code","outputId":"0396bb04-adc8-4784-f8b9-1fd4592accf3","executionInfo":{"status":"ok","timestamp":1583855407825,"user_tz":-330,"elapsed":215758,"user":{"displayName":"AMBUJE GUPTA","photoUrl":"","userId":"08374827863947652219"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import ktrain\n","from ktrain import text\n","\n","\n","\n","modelslist = [['funny','very_funny'],['very_offensive', 'slight']]\n","data_files = ['Fun-V-fun-memotion_eq_onlyfunny.csv','Slight-V-off-memotion_eq_onlyoffensive.csv']\n","\n","for columnclass, data_file in zip(modelslist, data_files):\n","\n","    \n","    columns=columnclass      \n","    DATA_PATH = '/content/drive/My Drive/data_eqd/' + data_file\n","    print(DATA_PATH)\n","\n","    NUM_WORDS = 90000\n","    MAXLEN = 100\n","\n","    h5name = '_'.join(columnclass)\n","    print('model', h5name)\n","\n","    (x_train, y_train), (x_test, y_test), preproc = text.texts_from_csv(DATA_PATH,\n","                          'cleaned_ocr',\n","                          label_columns = columns,\n","                          val_filepath=None, # if None, 10% of data will be used for validation\n","                          max_features=NUM_WORDS, maxlen=MAXLEN,\n","                        ngram_range=1,)\n","\n","\n","    model = text.text_classifier('logreg', (x_train, y_train), preproc=preproc)\n","\n","    learner = ktrain.get_learner(model, train_data=(x_train, y_train), val_data=(x_test, y_test))\n","    a='/content/drive/My Drive/Weight_file/'+h5name\n","    learner.autofit(0.001, 500,early_stopping=150, reduce_on_plateau=75)\n","\n","    a=learner.validate(val_data=(x_test, y_test), class_names=columns)\n","  \n","    predictor = ktrain.get_predictor(learner.model, preproc)\n","\n","   # score = 'xyz'########################\n","    h5name = \"/content/drive/My Drive/Weight_file/\"+h5name +  '.h5'\n","    print('model', h5name)\n","\n","    predictor.save(h5name)\n","    print(\"Model Saved\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/data_eqd/Fun-V-fun-memotion_eq_onlyfunny.csv\n","model funny_very_funny\n","detected encoding: utf-8 (if wrong, set manually)\n","language: en\n","Word Counts: 8686\n","Nrows: 4221\n","4221 train sequences\n","train sequence lengths:\n","\tmean : 8\n","\t95percentile : 17\n","\t99percentile : 25\n","x_train shape: (4221,100)\n","y_train shape: (4221, 2)\n","469 test sequences\n","test sequence lengths:\n","\tmean : 7\n","\t95percentile : 14\n","\t99percentile : 19\n","x_test shape: (469,100)\n","y_test shape: (469, 2)\n","Is Multi-Label? False\n","compiling word ID features...\n","maxlen is 100\n","done.\n","\n","\n","begin training using triangular learning rate policy with max lr of 0.001...\n","Train on 4221 samples, validate on 469 samples\n","Epoch 1/500\n","4221/4221 [==============================] - 1s 267us/sample - loss: 0.6934 - accuracy: 0.4975 - val_loss: 0.6925 - val_accuracy: 0.5203\n","Epoch 2/500\n","4221/4221 [==============================] - 1s 142us/sample - loss: 0.6771 - accuracy: 0.7107 - val_loss: 0.6909 - val_accuracy: 0.5288\n","Epoch 3/500\n","4221/4221 [==============================] - 1s 139us/sample - loss: 0.6603 - accuracy: 0.7766 - val_loss: 0.6900 - val_accuracy: 0.5480\n","Epoch 4/500\n","4221/4221 [==============================] - 1s 134us/sample - loss: 0.6446 - accuracy: 0.7991 - val_loss: 0.6902 - val_accuracy: 0.5309\n","Epoch 5/500\n","4221/4221 [==============================] - 1s 137us/sample - loss: 0.6295 - accuracy: 0.8197 - val_loss: 0.6894 - val_accuracy: 0.5330\n","Epoch 6/500\n","4221/4221 [==============================] - 1s 142us/sample - loss: 0.6152 - accuracy: 0.8323 - val_loss: 0.6892 - val_accuracy: 0.5458\n","Epoch 7/500\n","4221/4221 [==============================] - 1s 141us/sample - loss: 0.6015 - accuracy: 0.8387 - val_loss: 0.6897 - val_accuracy: 0.5501\n","Epoch 8/500\n","4221/4221 [==============================] - 1s 138us/sample - loss: 0.5888 - accuracy: 0.8443 - val_loss: 0.6898 - val_accuracy: 0.5352\n","Epoch 9/500\n","4221/4221 [==============================] - 1s 138us/sample - loss: 0.5767 - accuracy: 0.8493 - val_loss: 0.6901 - val_accuracy: 0.5373\n","Epoch 10/500\n","4221/4221 [==============================] - 1s 132us/sample - loss: 0.5649 - accuracy: 0.8564 - val_loss: 0.6908 - val_accuracy: 0.5437\n","Epoch 11/500\n","4221/4221 [==============================] - 1s 147us/sample - loss: 0.5539 - accuracy: 0.8631 - val_loss: 0.6919 - val_accuracy: 0.5373\n","Epoch 12/500\n","4221/4221 [==============================] - 1s 136us/sample - loss: 0.5433 - accuracy: 0.8659 - val_loss: 0.6925 - val_accuracy: 0.5501\n","Epoch 13/500\n","4221/4221 [==============================] - 1s 139us/sample - loss: 0.5330 - accuracy: 0.8699 - val_loss: 0.6936 - val_accuracy: 0.5501\n","Epoch 14/500\n","4221/4221 [==============================] - 1s 137us/sample - loss: 0.5233 - accuracy: 0.8733 - val_loss: 0.6944 - val_accuracy: 0.5437\n","Epoch 15/500\n","4221/4221 [==============================] - 1s 139us/sample - loss: 0.5140 - accuracy: 0.8766 - val_loss: 0.6950 - val_accuracy: 0.5373\n","Epoch 16/500\n","4221/4221 [==============================] - 1s 136us/sample - loss: 0.5050 - accuracy: 0.8796 - val_loss: 0.6967 - val_accuracy: 0.5373\n","Epoch 17/500\n","4221/4221 [==============================] - 1s 138us/sample - loss: 0.4963 - accuracy: 0.8796 - val_loss: 0.6982 - val_accuracy: 0.5437\n","Epoch 18/500\n","4221/4221 [==============================] - 1s 139us/sample - loss: 0.4879 - accuracy: 0.8842 - val_loss: 0.6997 - val_accuracy: 0.5437\n","Epoch 19/500\n","4221/4221 [==============================] - 1s 139us/sample - loss: 0.4799 - accuracy: 0.8896 - val_loss: 0.7016 - val_accuracy: 0.5480\n","Epoch 20/500\n","4221/4221 [==============================] - 1s 137us/sample - loss: 0.4720 - accuracy: 0.8913 - val_loss: 0.7030 - val_accuracy: 0.5501\n","Epoch 21/500\n","4221/4221 [==============================] - 1s 140us/sample - loss: 0.4645 - accuracy: 0.8932 - val_loss: 0.7042 - val_accuracy: 0.5458\n","Epoch 22/500\n","4221/4221 [==============================] - 1s 137us/sample - loss: 0.4571 - accuracy: 0.8943 - val_loss: 0.7067 - val_accuracy: 0.5501\n","Epoch 23/500\n","4221/4221 [==============================] - 1s 134us/sample - loss: 0.4500 - accuracy: 0.8960 - val_loss: 0.7080 - val_accuracy: 0.5458\n","Epoch 24/500\n","4221/4221 [==============================] - 1s 138us/sample - loss: 0.4431 - accuracy: 0.8986 - val_loss: 0.7098 - val_accuracy: 0.5416\n","Epoch 25/500\n","4221/4221 [==============================] - 1s 135us/sample - loss: 0.4365 - accuracy: 0.8991 - val_loss: 0.7111 - val_accuracy: 0.5437\n","Epoch 26/500\n","4221/4221 [==============================] - 1s 144us/sample - loss: 0.4300 - accuracy: 0.9024 - val_loss: 0.7122 - val_accuracy: 0.5458\n","Epoch 27/500\n","4221/4221 [==============================] - 1s 137us/sample - loss: 0.4237 - accuracy: 0.9031 - val_loss: 0.7147 - val_accuracy: 0.5416\n","Epoch 28/500\n","4221/4221 [==============================] - 1s 139us/sample - loss: 0.4175 - accuracy: 0.9071 - val_loss: 0.7172 - val_accuracy: 0.5394\n","Epoch 29/500\n","4221/4221 [==============================] - 1s 138us/sample - loss: 0.4116 - accuracy: 0.9069 - val_loss: 0.7189 - val_accuracy: 0.5373\n","Epoch 30/500\n","4221/4221 [==============================] - 1s 139us/sample - loss: 0.4058 - accuracy: 0.9083 - val_loss: 0.7209 - val_accuracy: 0.5394\n","Epoch 31/500\n","4221/4221 [==============================] - 1s 137us/sample - loss: 0.4001 - accuracy: 0.9100 - val_loss: 0.7233 - val_accuracy: 0.5437\n","Epoch 32/500\n","4221/4221 [==============================] - 1s 134us/sample - loss: 0.3946 - accuracy: 0.9109 - val_loss: 0.7245 - val_accuracy: 0.5416\n","Epoch 33/500\n","4221/4221 [==============================] - 1s 138us/sample - loss: 0.3891 - accuracy: 0.9131 - val_loss: 0.7264 - val_accuracy: 0.5437\n","Epoch 34/500\n","4221/4221 [==============================] - 1s 139us/sample - loss: 0.3839 - accuracy: 0.9149 - val_loss: 0.7284 - val_accuracy: 0.5373\n","Epoch 35/500\n","4221/4221 [==============================] - 1s 139us/sample - loss: 0.3788 - accuracy: 0.9157 - val_loss: 0.7308 - val_accuracy: 0.5352\n","Epoch 36/500\n","4221/4221 [==============================] - 1s 140us/sample - loss: 0.3737 - accuracy: 0.9180 - val_loss: 0.7324 - val_accuracy: 0.5394\n","Epoch 37/500\n","4221/4221 [==============================] - 1s 137us/sample - loss: 0.3688 - accuracy: 0.9197 - val_loss: 0.7341 - val_accuracy: 0.5394\n","Epoch 38/500\n","4221/4221 [==============================] - 1s 137us/sample - loss: 0.3641 - accuracy: 0.9209 - val_loss: 0.7355 - val_accuracy: 0.5352\n","Epoch 39/500\n","4221/4221 [==============================] - 1s 140us/sample - loss: 0.3594 - accuracy: 0.9228 - val_loss: 0.7382 - val_accuracy: 0.5288\n","Epoch 40/500\n","4221/4221 [==============================] - 1s 143us/sample - loss: 0.3547 - accuracy: 0.9244 - val_loss: 0.7403 - val_accuracy: 0.5309\n","Epoch 41/500\n","4221/4221 [==============================] - 1s 136us/sample - loss: 0.3503 - accuracy: 0.9282 - val_loss: 0.7419 - val_accuracy: 0.5267\n","Epoch 42/500\n","4221/4221 [==============================] - 1s 139us/sample - loss: 0.3459 - accuracy: 0.9277 - val_loss: 0.7447 - val_accuracy: 0.5245\n","Epoch 43/500\n","4221/4221 [==============================] - 1s 140us/sample - loss: 0.3416 - accuracy: 0.9282 - val_loss: 0.7476 - val_accuracy: 0.5181\n","Epoch 44/500\n","4221/4221 [==============================] - 1s 136us/sample - loss: 0.3374 - accuracy: 0.9294 - val_loss: 0.7490 - val_accuracy: 0.5267\n","Epoch 45/500\n","4221/4221 [==============================] - 1s 136us/sample - loss: 0.3333 - accuracy: 0.9299 - val_loss: 0.7519 - val_accuracy: 0.5224\n","Epoch 46/500\n","4221/4221 [==============================] - 1s 141us/sample - loss: 0.3292 - accuracy: 0.9325 - val_loss: 0.7542 - val_accuracy: 0.5160\n","Epoch 47/500\n","4221/4221 [==============================] - 1s 139us/sample - loss: 0.3252 - accuracy: 0.9320 - val_loss: 0.7574 - val_accuracy: 0.5160\n","Epoch 48/500\n","4221/4221 [==============================] - 1s 135us/sample - loss: 0.3215 - accuracy: 0.9327 - val_loss: 0.7603 - val_accuracy: 0.5181\n","Epoch 49/500\n","4221/4221 [==============================] - 1s 134us/sample - loss: 0.3175 - accuracy: 0.9339 - val_loss: 0.7634 - val_accuracy: 0.5181\n","Epoch 50/500\n","4221/4221 [==============================] - 1s 138us/sample - loss: 0.3139 - accuracy: 0.9341 - val_loss: 0.7658 - val_accuracy: 0.5203\n","Epoch 51/500\n","4221/4221 [==============================] - 1s 138us/sample - loss: 0.3101 - accuracy: 0.9351 - val_loss: 0.7679 - val_accuracy: 0.5160\n","Epoch 52/500\n","4221/4221 [==============================] - 1s 138us/sample - loss: 0.3066 - accuracy: 0.9363 - val_loss: 0.7707 - val_accuracy: 0.5160\n","Epoch 53/500\n","4221/4221 [==============================] - 1s 137us/sample - loss: 0.3030 - accuracy: 0.9360 - val_loss: 0.7734 - val_accuracy: 0.5160\n","Epoch 54/500\n","4221/4221 [==============================] - 1s 137us/sample - loss: 0.2996 - accuracy: 0.9365 - val_loss: 0.7756 - val_accuracy: 0.5139\n","Epoch 55/500\n","4221/4221 [==============================] - 1s 136us/sample - loss: 0.2962 - accuracy: 0.9370 - val_loss: 0.7783 - val_accuracy: 0.5139\n","Epoch 56/500\n","4221/4221 [==============================] - 1s 141us/sample - loss: 0.2929 - accuracy: 0.9391 - val_loss: 0.7805 - val_accuracy: 0.5181\n","Epoch 57/500\n","4221/4221 [==============================] - 1s 138us/sample - loss: 0.2896 - accuracy: 0.9396 - val_loss: 0.7833 - val_accuracy: 0.5181\n","Epoch 58/500\n","4221/4221 [==============================] - 1s 138us/sample - loss: 0.2865 - accuracy: 0.9408 - val_loss: 0.7861 - val_accuracy: 0.5160\n","Epoch 59/500\n","4221/4221 [==============================] - 1s 140us/sample - loss: 0.2832 - accuracy: 0.9408 - val_loss: 0.7885 - val_accuracy: 0.5139\n","Epoch 60/500\n","4221/4221 [==============================] - 1s 135us/sample - loss: 0.2802 - accuracy: 0.9420 - val_loss: 0.7905 - val_accuracy: 0.5117\n","Epoch 61/500\n","4221/4221 [==============================] - 1s 137us/sample - loss: 0.2771 - accuracy: 0.9427 - val_loss: 0.7930 - val_accuracy: 0.5096\n","Epoch 62/500\n","4221/4221 [==============================] - 1s 144us/sample - loss: 0.2741 - accuracy: 0.9439 - val_loss: 0.7956 - val_accuracy: 0.5053\n","Epoch 63/500\n","4221/4221 [==============================] - 1s 135us/sample - loss: 0.2712 - accuracy: 0.9439 - val_loss: 0.7985 - val_accuracy: 0.5053\n","Epoch 64/500\n","4221/4221 [==============================] - 1s 140us/sample - loss: 0.2683 - accuracy: 0.9443 - val_loss: 0.8011 - val_accuracy: 0.5053\n","Epoch 65/500\n","4221/4221 [==============================] - 1s 134us/sample - loss: 0.2655 - accuracy: 0.9457 - val_loss: 0.8038 - val_accuracy: 0.5075\n","Epoch 66/500\n","4221/4221 [==============================] - 1s 140us/sample - loss: 0.2626 - accuracy: 0.9467 - val_loss: 0.8066 - val_accuracy: 0.5032\n","Epoch 67/500\n","4221/4221 [==============================] - 1s 137us/sample - loss: 0.2599 - accuracy: 0.9476 - val_loss: 0.8098 - val_accuracy: 0.5032\n","Epoch 68/500\n","4221/4221 [==============================] - 1s 139us/sample - loss: 0.2573 - accuracy: 0.9469 - val_loss: 0.8122 - val_accuracy: 0.5096\n","Epoch 69/500\n","4221/4221 [==============================] - 1s 137us/sample - loss: 0.2546 - accuracy: 0.9476 - val_loss: 0.8148 - val_accuracy: 0.5075\n","Epoch 70/500\n","4221/4221 [==============================] - 1s 140us/sample - loss: 0.2520 - accuracy: 0.9472 - val_loss: 0.8181 - val_accuracy: 0.5075\n","Epoch 71/500\n","4221/4221 [==============================] - 1s 139us/sample - loss: 0.2495 - accuracy: 0.9476 - val_loss: 0.8210 - val_accuracy: 0.5096\n","Epoch 72/500\n","4221/4221 [==============================] - 1s 136us/sample - loss: 0.2470 - accuracy: 0.9481 - val_loss: 0.8235 - val_accuracy: 0.5075\n","Epoch 73/500\n","4221/4221 [==============================] - 1s 137us/sample - loss: 0.2445 - accuracy: 0.9488 - val_loss: 0.8262 - val_accuracy: 0.5032\n","Epoch 74/500\n","4221/4221 [==============================] - 1s 139us/sample - loss: 0.2421 - accuracy: 0.9495 - val_loss: 0.8294 - val_accuracy: 0.5053\n","Epoch 75/500\n","4221/4221 [==============================] - 1s 143us/sample - loss: 0.2397 - accuracy: 0.9491 - val_loss: 0.8327 - val_accuracy: 0.5053\n","Epoch 76/500\n","4221/4221 [==============================] - 1s 139us/sample - loss: 0.2374 - accuracy: 0.9505 - val_loss: 0.8358 - val_accuracy: 0.5032\n","Epoch 77/500\n","4221/4221 [==============================] - 1s 142us/sample - loss: 0.2350 - accuracy: 0.9514 - val_loss: 0.8388 - val_accuracy: 0.5053\n","Epoch 78/500\n","4221/4221 [==============================] - 1s 142us/sample - loss: 0.2328 - accuracy: 0.9514 - val_loss: 0.8421 - val_accuracy: 0.5053\n","Epoch 79/500\n","4221/4221 [==============================] - 1s 141us/sample - loss: 0.2306 - accuracy: 0.9510 - val_loss: 0.8453 - val_accuracy: 0.5053\n","Epoch 80/500\n","4221/4221 [==============================] - 1s 152us/sample - loss: 0.2284 - accuracy: 0.9517 - val_loss: 0.8478 - val_accuracy: 0.5032\n","Epoch 81/500\n","3872/4221 [==========================>...] - ETA: 0s - loss: 0.2250 - accuracy: 0.9530\n","Epoch 00081: Reducing Max LR on Plateau: new max lr will be 0.0005 (if not early_stopping).\n","4221/4221 [==============================] - 1s 148us/sample - loss: 0.2262 - accuracy: 0.9526 - val_loss: 0.8508 - val_accuracy: 0.5053\n","Epoch 82/500\n","4221/4221 [==============================] - 1s 145us/sample - loss: 0.2235 - accuracy: 0.9536 - val_loss: 0.8521 - val_accuracy: 0.5053\n","Epoch 83/500\n","4221/4221 [==============================] - 1s 149us/sample - loss: 0.2224 - accuracy: 0.9531 - val_loss: 0.8540 - val_accuracy: 0.5053\n","Epoch 84/500\n","4221/4221 [==============================] - 1s 150us/sample - loss: 0.2213 - accuracy: 0.9538 - val_loss: 0.8557 - val_accuracy: 0.5053\n","Epoch 85/500\n","4221/4221 [==============================] - 1s 151us/sample - loss: 0.2202 - accuracy: 0.9543 - val_loss: 0.8571 - val_accuracy: 0.5032\n","Epoch 86/500\n","4221/4221 [==============================] - 1s 148us/sample - loss: 0.2191 - accuracy: 0.9545 - val_loss: 0.8591 - val_accuracy: 0.5032\n","Epoch 87/500\n","4221/4221 [==============================] - 1s 146us/sample - loss: 0.2180 - accuracy: 0.9548 - val_loss: 0.8608 - val_accuracy: 0.5032\n","Epoch 88/500\n","4221/4221 [==============================] - 1s 150us/sample - loss: 0.2169 - accuracy: 0.9550 - val_loss: 0.8625 - val_accuracy: 0.5011\n","Epoch 89/500\n","4221/4221 [==============================] - 1s 149us/sample - loss: 0.2158 - accuracy: 0.9552 - val_loss: 0.8640 - val_accuracy: 0.5053\n","Epoch 90/500\n","4221/4221 [==============================] - 1s 146us/sample - loss: 0.2147 - accuracy: 0.9552 - val_loss: 0.8654 - val_accuracy: 0.5032\n","Epoch 91/500\n","4221/4221 [==============================] - 1s 148us/sample - loss: 0.2136 - accuracy: 0.9555 - val_loss: 0.8669 - val_accuracy: 0.5032\n","Epoch 92/500\n","4221/4221 [==============================] - 1s 144us/sample - loss: 0.2126 - accuracy: 0.9555 - val_loss: 0.8686 - val_accuracy: 0.5032\n","Epoch 93/500\n","4221/4221 [==============================] - 1s 150us/sample - loss: 0.2115 - accuracy: 0.9557 - val_loss: 0.8702 - val_accuracy: 0.5032\n","Epoch 94/500\n","4221/4221 [==============================] - 1s 148us/sample - loss: 0.2105 - accuracy: 0.9555 - val_loss: 0.8719 - val_accuracy: 0.5053\n","Epoch 95/500\n","4221/4221 [==============================] - 1s 143us/sample - loss: 0.2094 - accuracy: 0.9555 - val_loss: 0.8738 - val_accuracy: 0.5053\n","Epoch 96/500\n","4221/4221 [==============================] - 1s 138us/sample - loss: 0.2083 - accuracy: 0.9562 - val_loss: 0.8752 - val_accuracy: 0.5032\n","Epoch 97/500\n","4221/4221 [==============================] - 1s 137us/sample - loss: 0.2073 - accuracy: 0.9569 - val_loss: 0.8768 - val_accuracy: 0.5053\n","Epoch 98/500\n","4221/4221 [==============================] - 1s 140us/sample - loss: 0.2063 - accuracy: 0.9569 - val_loss: 0.8782 - val_accuracy: 0.5053\n","Epoch 99/500\n","4221/4221 [==============================] - 1s 136us/sample - loss: 0.2053 - accuracy: 0.9574 - val_loss: 0.8804 - val_accuracy: 0.5075\n","Epoch 100/500\n","4221/4221 [==============================] - 1s 138us/sample - loss: 0.2043 - accuracy: 0.9576 - val_loss: 0.8821 - val_accuracy: 0.5032\n","Epoch 101/500\n","4221/4221 [==============================] - 1s 139us/sample - loss: 0.2032 - accuracy: 0.9571 - val_loss: 0.8837 - val_accuracy: 0.5075\n","Epoch 102/500\n","4221/4221 [==============================] - 1s 139us/sample - loss: 0.2022 - accuracy: 0.9583 - val_loss: 0.8851 - val_accuracy: 0.5075\n","Epoch 103/500\n","4221/4221 [==============================] - 1s 138us/sample - loss: 0.2012 - accuracy: 0.9578 - val_loss: 0.8871 - val_accuracy: 0.5075\n","Epoch 104/500\n","4221/4221 [==============================] - 1s 138us/sample - loss: 0.2003 - accuracy: 0.9588 - val_loss: 0.8889 - val_accuracy: 0.5075\n","Epoch 105/500\n","4221/4221 [==============================] - 1s 138us/sample - loss: 0.1993 - accuracy: 0.9588 - val_loss: 0.8905 - val_accuracy: 0.5075\n","Epoch 106/500\n","4221/4221 [==============================] - 1s 138us/sample - loss: 0.1983 - accuracy: 0.9590 - val_loss: 0.8925 - val_accuracy: 0.5096\n","Epoch 107/500\n","4221/4221 [==============================] - 1s 140us/sample - loss: 0.1974 - accuracy: 0.9593 - val_loss: 0.8942 - val_accuracy: 0.5053\n","Epoch 108/500\n","4221/4221 [==============================] - 1s 138us/sample - loss: 0.1964 - accuracy: 0.9595 - val_loss: 0.8961 - val_accuracy: 0.5096\n","Epoch 109/500\n","4221/4221 [==============================] - 1s 135us/sample - loss: 0.1955 - accuracy: 0.9590 - val_loss: 0.8977 - val_accuracy: 0.5096\n","Epoch 110/500\n","4221/4221 [==============================] - 1s 138us/sample - loss: 0.1945 - accuracy: 0.9593 - val_loss: 0.8991 - val_accuracy: 0.5075\n","Epoch 111/500\n","4221/4221 [==============================] - 1s 141us/sample - loss: 0.1936 - accuracy: 0.9593 - val_loss: 0.9010 - val_accuracy: 0.5075\n","Epoch 112/500\n","4221/4221 [==============================] - 1s 141us/sample - loss: 0.1927 - accuracy: 0.9597 - val_loss: 0.9030 - val_accuracy: 0.5075\n","Epoch 113/500\n","4221/4221 [==============================] - 1s 135us/sample - loss: 0.1918 - accuracy: 0.9595 - val_loss: 0.9050 - val_accuracy: 0.5075\n","Epoch 114/500\n","4221/4221 [==============================] - 1s 136us/sample - loss: 0.1909 - accuracy: 0.9595 - val_loss: 0.9065 - val_accuracy: 0.5117\n","Epoch 115/500\n","4221/4221 [==============================] - 1s 138us/sample - loss: 0.1899 - accuracy: 0.9607 - val_loss: 0.9086 - val_accuracy: 0.5075\n","Epoch 116/500\n","4221/4221 [==============================] - 1s 135us/sample - loss: 0.1891 - accuracy: 0.9609 - val_loss: 0.9102 - val_accuracy: 0.5096\n","Epoch 117/500\n","4221/4221 [==============================] - 1s 137us/sample - loss: 0.1882 - accuracy: 0.9607 - val_loss: 0.9120 - val_accuracy: 0.5117\n","Epoch 118/500\n","4221/4221 [==============================] - 1s 138us/sample - loss: 0.1873 - accuracy: 0.9607 - val_loss: 0.9140 - val_accuracy: 0.5117\n","Epoch 119/500\n","4221/4221 [==============================] - 1s 137us/sample - loss: 0.1864 - accuracy: 0.9614 - val_loss: 0.9159 - val_accuracy: 0.5117\n","Epoch 120/500\n","4221/4221 [==============================] - 1s 141us/sample - loss: 0.1855 - accuracy: 0.9614 - val_loss: 0.9181 - val_accuracy: 0.5096\n","Epoch 121/500\n","4221/4221 [==============================] - 1s 138us/sample - loss: 0.1847 - accuracy: 0.9616 - val_loss: 0.9196 - val_accuracy: 0.5096\n","Epoch 122/500\n","4221/4221 [==============================] - 1s 137us/sample - loss: 0.1839 - accuracy: 0.9619 - val_loss: 0.9215 - val_accuracy: 0.5096\n","Epoch 123/500\n","4221/4221 [==============================] - 1s 135us/sample - loss: 0.1830 - accuracy: 0.9619 - val_loss: 0.9234 - val_accuracy: 0.5075\n","Epoch 124/500\n","4221/4221 [==============================] - 1s 142us/sample - loss: 0.1822 - accuracy: 0.9614 - val_loss: 0.9254 - val_accuracy: 0.5075\n","Epoch 125/500\n","4221/4221 [==============================] - 1s 136us/sample - loss: 0.1814 - accuracy: 0.9616 - val_loss: 0.9269 - val_accuracy: 0.5053\n","Epoch 126/500\n","4221/4221 [==============================] - 1s 137us/sample - loss: 0.1805 - accuracy: 0.9621 - val_loss: 0.9285 - val_accuracy: 0.5053\n","Epoch 127/500\n","4221/4221 [==============================] - 1s 138us/sample - loss: 0.1797 - accuracy: 0.9616 - val_loss: 0.9306 - val_accuracy: 0.5032\n","Epoch 128/500\n","4221/4221 [==============================] - 1s 138us/sample - loss: 0.1789 - accuracy: 0.9619 - val_loss: 0.9321 - val_accuracy: 0.5032\n","Epoch 129/500\n","4221/4221 [==============================] - 1s 140us/sample - loss: 0.1781 - accuracy: 0.9621 - val_loss: 0.9342 - val_accuracy: 0.5011\n","Epoch 130/500\n","4221/4221 [==============================] - 1s 138us/sample - loss: 0.1774 - accuracy: 0.9619 - val_loss: 0.9359 - val_accuracy: 0.5032\n","Epoch 131/500\n","4221/4221 [==============================] - 1s 138us/sample - loss: 0.1765 - accuracy: 0.9614 - val_loss: 0.9380 - val_accuracy: 0.5096\n","Epoch 132/500\n","4221/4221 [==============================] - 1s 136us/sample - loss: 0.1757 - accuracy: 0.9621 - val_loss: 0.9398 - val_accuracy: 0.5053\n","Epoch 133/500\n","4221/4221 [==============================] - 1s 139us/sample - loss: 0.1750 - accuracy: 0.9623 - val_loss: 0.9420 - val_accuracy: 0.5032\n","Epoch 134/500\n","4221/4221 [==============================] - 1s 135us/sample - loss: 0.1742 - accuracy: 0.9623 - val_loss: 0.9439 - val_accuracy: 0.5032\n","Epoch 135/500\n","4221/4221 [==============================] - 1s 137us/sample - loss: 0.1734 - accuracy: 0.9633 - val_loss: 0.9458 - val_accuracy: 0.5053\n","Epoch 136/500\n","4221/4221 [==============================] - 1s 136us/sample - loss: 0.1727 - accuracy: 0.9633 - val_loss: 0.9478 - val_accuracy: 0.5053\n","Epoch 137/500\n","4221/4221 [==============================] - 1s 136us/sample - loss: 0.1719 - accuracy: 0.9635 - val_loss: 0.9494 - val_accuracy: 0.5053\n","Epoch 138/500\n","4221/4221 [==============================] - 1s 139us/sample - loss: 0.1711 - accuracy: 0.9640 - val_loss: 0.9511 - val_accuracy: 0.5053\n","Epoch 139/500\n","4221/4221 [==============================] - 1s 137us/sample - loss: 0.1704 - accuracy: 0.9640 - val_loss: 0.9526 - val_accuracy: 0.5075\n","Epoch 140/500\n","4221/4221 [==============================] - 1s 136us/sample - loss: 0.1696 - accuracy: 0.9642 - val_loss: 0.9545 - val_accuracy: 0.5032\n","Epoch 141/500\n","4221/4221 [==============================] - 1s 140us/sample - loss: 0.1689 - accuracy: 0.9645 - val_loss: 0.9566 - val_accuracy: 0.5053\n","Epoch 142/500\n","4221/4221 [==============================] - 1s 142us/sample - loss: 0.1682 - accuracy: 0.9647 - val_loss: 0.9587 - val_accuracy: 0.5011\n","Epoch 143/500\n","4221/4221 [==============================] - 1s 138us/sample - loss: 0.1675 - accuracy: 0.9642 - val_loss: 0.9606 - val_accuracy: 0.5032\n","Epoch 144/500\n","4221/4221 [==============================] - 1s 137us/sample - loss: 0.1668 - accuracy: 0.9647 - val_loss: 0.9626 - val_accuracy: 0.5032\n","Epoch 145/500\n","4221/4221 [==============================] - 1s 137us/sample - loss: 0.1660 - accuracy: 0.9649 - val_loss: 0.9643 - val_accuracy: 0.5032\n","Epoch 146/500\n","4221/4221 [==============================] - 1s 135us/sample - loss: 0.1653 - accuracy: 0.9649 - val_loss: 0.9660 - val_accuracy: 0.5032\n","Epoch 147/500\n","4221/4221 [==============================] - 1s 138us/sample - loss: 0.1646 - accuracy: 0.9649 - val_loss: 0.9679 - val_accuracy: 0.5053\n","Epoch 148/500\n","4221/4221 [==============================] - 1s 138us/sample - loss: 0.1639 - accuracy: 0.9647 - val_loss: 0.9696 - val_accuracy: 0.5053\n","Epoch 149/500\n","4221/4221 [==============================] - 1s 136us/sample - loss: 0.1633 - accuracy: 0.9649 - val_loss: 0.9717 - val_accuracy: 0.5032\n","Epoch 150/500\n","4221/4221 [==============================] - 1s 146us/sample - loss: 0.1626 - accuracy: 0.9654 - val_loss: 0.9739 - val_accuracy: 0.5075\n","Epoch 151/500\n","4221/4221 [==============================] - 1s 149us/sample - loss: 0.1619 - accuracy: 0.9652 - val_loss: 0.9760 - val_accuracy: 0.5075\n","Epoch 152/500\n","4221/4221 [==============================] - 1s 151us/sample - loss: 0.1612 - accuracy: 0.9649 - val_loss: 0.9777 - val_accuracy: 0.5075\n","Epoch 153/500\n","4221/4221 [==============================] - 1s 146us/sample - loss: 0.1605 - accuracy: 0.9659 - val_loss: 0.9798 - val_accuracy: 0.5075\n","Epoch 154/500\n","4221/4221 [==============================] - 1s 149us/sample - loss: 0.1599 - accuracy: 0.9656 - val_loss: 0.9819 - val_accuracy: 0.5053\n","Epoch 155/500\n","4221/4221 [==============================] - 1s 154us/sample - loss: 0.1592 - accuracy: 0.9654 - val_loss: 0.9839 - val_accuracy: 0.5117\n","Epoch 156/500\n","4096/4221 [============================>.] - ETA: 0s - loss: 0.1580 - accuracy: 0.9656\n","Epoch 00156: Reducing Max LR on Plateau: new max lr will be 0.00025 (if not early_stopping).\n","Restoring model weights from the end of the best epoch.\n","4221/4221 [==============================] - 1s 153us/sample - loss: 0.1585 - accuracy: 0.9654 - val_loss: 0.9857 - val_accuracy: 0.5117\n","Epoch 00156: early stopping\n","Weights from best epoch have been loaded into model.\n","              precision    recall  f1-score   support\n","\n","       funny       0.58      0.62      0.60       258\n","  very_funny       0.49      0.45      0.47       211\n","\n","    accuracy                           0.55       469\n","   macro avg       0.54      0.54      0.54       469\n","weighted avg       0.54      0.55      0.54       469\n","\n","model /content/drive/My Drive/Weight_file/funny_very_funny.h5\n","Model Saved\n","/content/drive/My Drive/data_eqd/Slight-V-off-memotion_eq_onlyoffensive.csv\n","model very_offensive_slight\n","detected encoding: utf-8 (if wrong, set manually)\n","language: en\n","Word Counts: 8077\n","Nrows: 4552\n","4552 train sequences\n","train sequence lengths:\n","\tmean : 8\n","\t95percentile : 17\n","\t99percentile : 25\n","x_train shape: (4552,100)\n","y_train shape: (4552, 2)\n","506 test sequences\n","test sequence lengths:\n","\tmean : 7\n","\t95percentile : 15\n","\t99percentile : 20\n","x_test shape: (506,100)\n","y_test shape: (506, 2)\n","Is Multi-Label? False\n","compiling word ID features...\n","maxlen is 100\n","done.\n","\n","\n","begin training using triangular learning rate policy with max lr of 0.001...\n","Train on 4552 samples, validate on 506 samples\n","Epoch 1/500\n","4552/4552 [==============================] - 1s 260us/sample - loss: 0.6898 - accuracy: 0.5729 - val_loss: 0.6875 - val_accuracy: 0.5830\n","Epoch 2/500\n","4552/4552 [==============================] - 1s 146us/sample - loss: 0.6679 - accuracy: 0.7744 - val_loss: 0.6814 - val_accuracy: 0.6304\n","Epoch 3/500\n","4552/4552 [==============================] - 1s 140us/sample - loss: 0.6460 - accuracy: 0.8100 - val_loss: 0.6746 - val_accuracy: 0.6364\n","Epoch 4/500\n","4552/4552 [==============================] - 1s 141us/sample - loss: 0.6256 - accuracy: 0.8282 - val_loss: 0.6690 - val_accuracy: 0.6364\n","Epoch 5/500\n","4552/4552 [==============================] - 1s 137us/sample - loss: 0.6068 - accuracy: 0.8449 - val_loss: 0.6642 - val_accuracy: 0.6423\n","Epoch 6/500\n","4552/4552 [==============================] - 1s 142us/sample - loss: 0.5893 - accuracy: 0.8533 - val_loss: 0.6599 - val_accuracy: 0.6482\n","Epoch 7/500\n","4552/4552 [==============================] - 1s 137us/sample - loss: 0.5727 - accuracy: 0.8625 - val_loss: 0.6565 - val_accuracy: 0.6403\n","Epoch 8/500\n","4552/4552 [==============================] - 1s 138us/sample - loss: 0.5572 - accuracy: 0.8686 - val_loss: 0.6535 - val_accuracy: 0.6462\n","Epoch 9/500\n","4552/4552 [==============================] - 1s 138us/sample - loss: 0.5430 - accuracy: 0.8730 - val_loss: 0.6507 - val_accuracy: 0.6403\n","Epoch 10/500\n","4552/4552 [==============================] - 1s 141us/sample - loss: 0.5294 - accuracy: 0.8768 - val_loss: 0.6483 - val_accuracy: 0.6443\n","Epoch 11/500\n","4552/4552 [==============================] - 1s 140us/sample - loss: 0.5165 - accuracy: 0.8809 - val_loss: 0.6451 - val_accuracy: 0.6285\n","Epoch 12/500\n","4552/4552 [==============================] - 1s 140us/sample - loss: 0.5044 - accuracy: 0.8853 - val_loss: 0.6432 - val_accuracy: 0.6383\n","Epoch 13/500\n","4552/4552 [==============================] - 1s 135us/sample - loss: 0.4929 - accuracy: 0.8877 - val_loss: 0.6402 - val_accuracy: 0.6364\n","Epoch 14/500\n","4552/4552 [==============================] - 1s 147us/sample - loss: 0.4819 - accuracy: 0.8928 - val_loss: 0.6386 - val_accuracy: 0.6344\n","Epoch 15/500\n","4552/4552 [==============================] - 1s 139us/sample - loss: 0.4714 - accuracy: 0.8935 - val_loss: 0.6368 - val_accuracy: 0.6383\n","Epoch 16/500\n","4552/4552 [==============================] - 1s 142us/sample - loss: 0.4613 - accuracy: 0.8963 - val_loss: 0.6353 - val_accuracy: 0.6403\n","Epoch 17/500\n","4552/4552 [==============================] - 1s 142us/sample - loss: 0.4517 - accuracy: 0.8985 - val_loss: 0.6334 - val_accuracy: 0.6423\n","Epoch 18/500\n","4552/4552 [==============================] - 1s 142us/sample - loss: 0.4426 - accuracy: 0.9022 - val_loss: 0.6320 - val_accuracy: 0.6383\n","Epoch 19/500\n","4552/4552 [==============================] - 1s 142us/sample - loss: 0.4337 - accuracy: 0.9027 - val_loss: 0.6305 - val_accuracy: 0.6462\n","Epoch 20/500\n","4552/4552 [==============================] - 1s 141us/sample - loss: 0.4252 - accuracy: 0.9049 - val_loss: 0.6297 - val_accuracy: 0.6403\n","Epoch 21/500\n","4552/4552 [==============================] - 1s 139us/sample - loss: 0.4170 - accuracy: 0.9073 - val_loss: 0.6292 - val_accuracy: 0.6344\n","Epoch 22/500\n","4552/4552 [==============================] - 1s 140us/sample - loss: 0.4090 - accuracy: 0.9101 - val_loss: 0.6284 - val_accuracy: 0.6403\n","Epoch 23/500\n","4552/4552 [==============================] - 1s 140us/sample - loss: 0.4014 - accuracy: 0.9115 - val_loss: 0.6282 - val_accuracy: 0.6383\n","Epoch 24/500\n","4552/4552 [==============================] - 1s 138us/sample - loss: 0.3941 - accuracy: 0.9139 - val_loss: 0.6266 - val_accuracy: 0.6403\n","Epoch 25/500\n","4552/4552 [==============================] - 1s 138us/sample - loss: 0.3869 - accuracy: 0.9159 - val_loss: 0.6257 - val_accuracy: 0.6403\n","Epoch 26/500\n","4552/4552 [==============================] - 1s 139us/sample - loss: 0.3801 - accuracy: 0.9161 - val_loss: 0.6254 - val_accuracy: 0.6423\n","Epoch 27/500\n","4552/4552 [==============================] - 1s 142us/sample - loss: 0.3735 - accuracy: 0.9183 - val_loss: 0.6253 - val_accuracy: 0.6423\n","Epoch 28/500\n","4552/4552 [==============================] - 1s 141us/sample - loss: 0.3670 - accuracy: 0.9216 - val_loss: 0.6239 - val_accuracy: 0.6443\n","Epoch 29/500\n","4552/4552 [==============================] - 1s 137us/sample - loss: 0.3608 - accuracy: 0.9229 - val_loss: 0.6232 - val_accuracy: 0.6522\n","Epoch 30/500\n","4552/4552 [==============================] - 1s 140us/sample - loss: 0.3547 - accuracy: 0.9249 - val_loss: 0.6230 - val_accuracy: 0.6502\n","Epoch 31/500\n","4552/4552 [==============================] - 1s 143us/sample - loss: 0.3489 - accuracy: 0.9264 - val_loss: 0.6221 - val_accuracy: 0.6522\n","Epoch 32/500\n","4552/4552 [==============================] - 1s 144us/sample - loss: 0.3431 - accuracy: 0.9264 - val_loss: 0.6216 - val_accuracy: 0.6502\n","Epoch 33/500\n","4552/4552 [==============================] - 1s 139us/sample - loss: 0.3376 - accuracy: 0.9293 - val_loss: 0.6220 - val_accuracy: 0.6542\n","Epoch 34/500\n","4552/4552 [==============================] - 1s 138us/sample - loss: 0.3323 - accuracy: 0.9293 - val_loss: 0.6221 - val_accuracy: 0.6482\n","Epoch 35/500\n","4552/4552 [==============================] - 1s 141us/sample - loss: 0.3270 - accuracy: 0.9304 - val_loss: 0.6227 - val_accuracy: 0.6601\n","Epoch 36/500\n","4552/4552 [==============================] - 1s 141us/sample - loss: 0.3219 - accuracy: 0.9317 - val_loss: 0.6232 - val_accuracy: 0.6621\n","Epoch 37/500\n","4552/4552 [==============================] - 1s 139us/sample - loss: 0.3170 - accuracy: 0.9348 - val_loss: 0.6226 - val_accuracy: 0.6561\n","Epoch 38/500\n","4552/4552 [==============================] - 1s 137us/sample - loss: 0.3121 - accuracy: 0.9354 - val_loss: 0.6227 - val_accuracy: 0.6601\n","Epoch 39/500\n","4552/4552 [==============================] - 1s 139us/sample - loss: 0.3073 - accuracy: 0.9356 - val_loss: 0.6226 - val_accuracy: 0.6621\n","Epoch 40/500\n","4552/4552 [==============================] - 1s 139us/sample - loss: 0.3028 - accuracy: 0.9383 - val_loss: 0.6230 - val_accuracy: 0.6640\n","Epoch 41/500\n","4552/4552 [==============================] - 1s 141us/sample - loss: 0.2982 - accuracy: 0.9398 - val_loss: 0.6230 - val_accuracy: 0.6640\n","Epoch 42/500\n","4552/4552 [==============================] - 1s 139us/sample - loss: 0.2939 - accuracy: 0.9396 - val_loss: 0.6229 - val_accuracy: 0.6601\n","Epoch 43/500\n","4552/4552 [==============================] - 1s 141us/sample - loss: 0.2897 - accuracy: 0.9407 - val_loss: 0.6238 - val_accuracy: 0.6640\n","Epoch 44/500\n","4552/4552 [==============================] - 1s 140us/sample - loss: 0.2855 - accuracy: 0.9411 - val_loss: 0.6244 - val_accuracy: 0.6621\n","Epoch 45/500\n","4552/4552 [==============================] - 1s 142us/sample - loss: 0.2814 - accuracy: 0.9433 - val_loss: 0.6248 - val_accuracy: 0.6621\n","Epoch 46/500\n","4552/4552 [==============================] - 1s 140us/sample - loss: 0.2775 - accuracy: 0.9431 - val_loss: 0.6252 - val_accuracy: 0.6680\n","Epoch 47/500\n","4552/4552 [==============================] - 1s 138us/sample - loss: 0.2736 - accuracy: 0.9429 - val_loss: 0.6258 - val_accuracy: 0.6680\n","Epoch 48/500\n","4552/4552 [==============================] - 1s 142us/sample - loss: 0.2698 - accuracy: 0.9446 - val_loss: 0.6266 - val_accuracy: 0.6660\n","Epoch 49/500\n","4552/4552 [==============================] - 1s 143us/sample - loss: 0.2660 - accuracy: 0.9455 - val_loss: 0.6267 - val_accuracy: 0.6680\n","Epoch 50/500\n","4552/4552 [==============================] - 1s 137us/sample - loss: 0.2625 - accuracy: 0.9462 - val_loss: 0.6271 - val_accuracy: 0.6700\n","Epoch 51/500\n","4552/4552 [==============================] - 1s 139us/sample - loss: 0.2589 - accuracy: 0.9468 - val_loss: 0.6269 - val_accuracy: 0.6680\n","Epoch 52/500\n","4552/4552 [==============================] - 1s 139us/sample - loss: 0.2554 - accuracy: 0.9477 - val_loss: 0.6271 - val_accuracy: 0.6700\n","Epoch 53/500\n","4552/4552 [==============================] - 1s 138us/sample - loss: 0.2520 - accuracy: 0.9479 - val_loss: 0.6275 - val_accuracy: 0.6719\n","Epoch 54/500\n","4552/4552 [==============================] - 1s 140us/sample - loss: 0.2487 - accuracy: 0.9484 - val_loss: 0.6285 - val_accuracy: 0.6798\n","Epoch 55/500\n","4552/4552 [==============================] - 1s 137us/sample - loss: 0.2455 - accuracy: 0.9497 - val_loss: 0.6297 - val_accuracy: 0.6838\n","Epoch 56/500\n","4552/4552 [==============================] - 1s 140us/sample - loss: 0.2423 - accuracy: 0.9523 - val_loss: 0.6312 - val_accuracy: 0.6818\n","Epoch 57/500\n","4552/4552 [==============================] - 1s 136us/sample - loss: 0.2391 - accuracy: 0.9532 - val_loss: 0.6323 - val_accuracy: 0.6838\n","Epoch 58/500\n","4552/4552 [==============================] - 1s 139us/sample - loss: 0.2361 - accuracy: 0.9541 - val_loss: 0.6330 - val_accuracy: 0.6937\n","Epoch 59/500\n","4552/4552 [==============================] - 1s 139us/sample - loss: 0.2331 - accuracy: 0.9550 - val_loss: 0.6338 - val_accuracy: 0.6897\n","Epoch 60/500\n","4552/4552 [==============================] - 1s 139us/sample - loss: 0.2302 - accuracy: 0.9550 - val_loss: 0.6347 - val_accuracy: 0.6937\n","Epoch 61/500\n","4552/4552 [==============================] - 1s 138us/sample - loss: 0.2273 - accuracy: 0.9543 - val_loss: 0.6356 - val_accuracy: 0.6917\n","Epoch 62/500\n","4552/4552 [==============================] - 1s 139us/sample - loss: 0.2245 - accuracy: 0.9545 - val_loss: 0.6364 - val_accuracy: 0.6897\n","Epoch 63/500\n","4552/4552 [==============================] - 1s 140us/sample - loss: 0.2218 - accuracy: 0.9556 - val_loss: 0.6371 - val_accuracy: 0.6838\n","Epoch 64/500\n","4552/4552 [==============================] - 1s 140us/sample - loss: 0.2190 - accuracy: 0.9574 - val_loss: 0.6379 - val_accuracy: 0.6858\n","Epoch 65/500\n","4552/4552 [==============================] - 1s 141us/sample - loss: 0.2164 - accuracy: 0.9578 - val_loss: 0.6395 - val_accuracy: 0.6877\n","Epoch 66/500\n","4552/4552 [==============================] - 1s 138us/sample - loss: 0.2138 - accuracy: 0.9578 - val_loss: 0.6405 - val_accuracy: 0.6858\n","Epoch 67/500\n","4552/4552 [==============================] - 1s 145us/sample - loss: 0.2113 - accuracy: 0.9591 - val_loss: 0.6418 - val_accuracy: 0.6838\n","Epoch 68/500\n","4552/4552 [==============================] - 1s 141us/sample - loss: 0.2088 - accuracy: 0.9591 - val_loss: 0.6427 - val_accuracy: 0.6858\n","Epoch 69/500\n","4552/4552 [==============================] - 1s 138us/sample - loss: 0.2064 - accuracy: 0.9605 - val_loss: 0.6441 - val_accuracy: 0.6877\n","Epoch 70/500\n","4552/4552 [==============================] - 1s 139us/sample - loss: 0.2039 - accuracy: 0.9609 - val_loss: 0.6460 - val_accuracy: 0.6917\n","Epoch 71/500\n","4552/4552 [==============================] - 1s 142us/sample - loss: 0.2015 - accuracy: 0.9618 - val_loss: 0.6469 - val_accuracy: 0.6937\n","Epoch 72/500\n","4552/4552 [==============================] - 1s 141us/sample - loss: 0.1992 - accuracy: 0.9618 - val_loss: 0.6479 - val_accuracy: 0.6917\n","Epoch 73/500\n","4552/4552 [==============================] - 1s 138us/sample - loss: 0.1970 - accuracy: 0.9618 - val_loss: 0.6499 - val_accuracy: 0.6917\n","Epoch 74/500\n","4552/4552 [==============================] - 1s 142us/sample - loss: 0.1948 - accuracy: 0.9624 - val_loss: 0.6515 - val_accuracy: 0.6937\n","Epoch 75/500\n","4552/4552 [==============================] - 1s 142us/sample - loss: 0.1926 - accuracy: 0.9631 - val_loss: 0.6535 - val_accuracy: 0.6917\n","Epoch 76/500\n","4552/4552 [==============================] - 1s 140us/sample - loss: 0.1905 - accuracy: 0.9629 - val_loss: 0.6548 - val_accuracy: 0.6917\n","Epoch 77/500\n","4552/4552 [==============================] - 1s 138us/sample - loss: 0.1884 - accuracy: 0.9646 - val_loss: 0.6568 - val_accuracy: 0.6957\n","Epoch 78/500\n","4552/4552 [==============================] - 1s 138us/sample - loss: 0.1863 - accuracy: 0.9640 - val_loss: 0.6584 - val_accuracy: 0.6976\n","Epoch 79/500\n","4552/4552 [==============================] - 1s 137us/sample - loss: 0.1844 - accuracy: 0.9649 - val_loss: 0.6593 - val_accuracy: 0.6957\n","Epoch 80/500\n","4552/4552 [==============================] - 1s 140us/sample - loss: 0.1824 - accuracy: 0.9659 - val_loss: 0.6603 - val_accuracy: 0.6937\n","Epoch 81/500\n","4552/4552 [==============================] - 1s 139us/sample - loss: 0.1804 - accuracy: 0.9666 - val_loss: 0.6614 - val_accuracy: 0.6976\n","Epoch 82/500\n","4552/4552 [==============================] - 1s 134us/sample - loss: 0.1785 - accuracy: 0.9670 - val_loss: 0.6629 - val_accuracy: 0.6976\n","Epoch 83/500\n","4552/4552 [==============================] - 1s 142us/sample - loss: 0.1765 - accuracy: 0.9670 - val_loss: 0.6648 - val_accuracy: 0.6957\n","Epoch 84/500\n","4552/4552 [==============================] - 1s 138us/sample - loss: 0.1747 - accuracy: 0.9673 - val_loss: 0.6663 - val_accuracy: 0.6937\n","Epoch 85/500\n","4552/4552 [==============================] - 1s 140us/sample - loss: 0.1728 - accuracy: 0.9677 - val_loss: 0.6683 - val_accuracy: 0.6976\n","Epoch 86/500\n","4552/4552 [==============================] - 1s 139us/sample - loss: 0.1711 - accuracy: 0.9679 - val_loss: 0.6703 - val_accuracy: 0.6937\n","Epoch 87/500\n","4552/4552 [==============================] - 1s 136us/sample - loss: 0.1693 - accuracy: 0.9690 - val_loss: 0.6716 - val_accuracy: 0.6996\n","Epoch 88/500\n","4552/4552 [==============================] - 1s 140us/sample - loss: 0.1675 - accuracy: 0.9692 - val_loss: 0.6730 - val_accuracy: 0.6996\n","Epoch 89/500\n","4552/4552 [==============================] - 1s 136us/sample - loss: 0.1658 - accuracy: 0.9692 - val_loss: 0.6751 - val_accuracy: 0.7016\n","Epoch 90/500\n","4552/4552 [==============================] - 1s 135us/sample - loss: 0.1641 - accuracy: 0.9688 - val_loss: 0.6772 - val_accuracy: 0.6996\n","Epoch 91/500\n","4552/4552 [==============================] - 1s 140us/sample - loss: 0.1625 - accuracy: 0.9695 - val_loss: 0.6791 - val_accuracy: 0.6996\n","Epoch 92/500\n","4552/4552 [==============================] - 1s 138us/sample - loss: 0.1609 - accuracy: 0.9690 - val_loss: 0.6812 - val_accuracy: 0.7016\n","Epoch 93/500\n","4552/4552 [==============================] - 1s 139us/sample - loss: 0.1593 - accuracy: 0.9697 - val_loss: 0.6832 - val_accuracy: 0.7016\n","Epoch 94/500\n","4552/4552 [==============================] - 1s 140us/sample - loss: 0.1577 - accuracy: 0.9710 - val_loss: 0.6847 - val_accuracy: 0.7016\n","Epoch 95/500\n","4552/4552 [==============================] - 1s 137us/sample - loss: 0.1561 - accuracy: 0.9712 - val_loss: 0.6867 - val_accuracy: 0.6976\n","Epoch 96/500\n","4552/4552 [==============================] - 1s 139us/sample - loss: 0.1547 - accuracy: 0.9717 - val_loss: 0.6892 - val_accuracy: 0.6976\n","Epoch 97/500\n","4552/4552 [==============================] - 1s 141us/sample - loss: 0.1531 - accuracy: 0.9714 - val_loss: 0.6912 - val_accuracy: 0.6996\n","Epoch 98/500\n","4552/4552 [==============================] - 1s 135us/sample - loss: 0.1517 - accuracy: 0.9719 - val_loss: 0.6938 - val_accuracy: 0.6976\n","Epoch 99/500\n","4552/4552 [==============================] - 1s 144us/sample - loss: 0.1502 - accuracy: 0.9717 - val_loss: 0.6954 - val_accuracy: 0.6996\n","Epoch 100/500\n","4552/4552 [==============================] - 1s 141us/sample - loss: 0.1488 - accuracy: 0.9730 - val_loss: 0.6973 - val_accuracy: 0.6996\n","Epoch 101/500\n","4552/4552 [==============================] - 1s 138us/sample - loss: 0.1474 - accuracy: 0.9730 - val_loss: 0.6989 - val_accuracy: 0.6996\n","Epoch 102/500\n","4552/4552 [==============================] - 1s 140us/sample - loss: 0.1460 - accuracy: 0.9728 - val_loss: 0.7014 - val_accuracy: 0.7016\n","Epoch 103/500\n","4552/4552 [==============================] - 1s 136us/sample - loss: 0.1446 - accuracy: 0.9721 - val_loss: 0.7035 - val_accuracy: 0.7016\n","Epoch 104/500\n","4552/4552 [==============================] - 1s 140us/sample - loss: 0.1433 - accuracy: 0.9732 - val_loss: 0.7062 - val_accuracy: 0.7036\n","Epoch 105/500\n","4552/4552 [==============================] - 1s 139us/sample - loss: 0.1419 - accuracy: 0.9739 - val_loss: 0.7080 - val_accuracy: 0.7036\n","Epoch 106/500\n","4552/4552 [==============================] - 1s 140us/sample - loss: 0.1407 - accuracy: 0.9743 - val_loss: 0.7097 - val_accuracy: 0.7036\n","Epoch 107/500\n","4352/4552 [===========================>..] - ETA: 0s - loss: 0.1388 - accuracy: 0.9745\n","Epoch 00107: Reducing Max LR on Plateau: new max lr will be 0.0005 (if not early_stopping).\n","4552/4552 [==============================] - 1s 142us/sample - loss: 0.1393 - accuracy: 0.9739 - val_loss: 0.7106 - val_accuracy: 0.7036\n","Epoch 108/500\n","4552/4552 [==============================] - 1s 140us/sample - loss: 0.1377 - accuracy: 0.9747 - val_loss: 0.7119 - val_accuracy: 0.7036\n","Epoch 109/500\n","4552/4552 [==============================] - 1s 140us/sample - loss: 0.1371 - accuracy: 0.9754 - val_loss: 0.7131 - val_accuracy: 0.7036\n","Epoch 110/500\n","4552/4552 [==============================] - 1s 141us/sample - loss: 0.1364 - accuracy: 0.9750 - val_loss: 0.7141 - val_accuracy: 0.7036\n","Epoch 111/500\n","4552/4552 [==============================] - 1s 143us/sample - loss: 0.1357 - accuracy: 0.9750 - val_loss: 0.7155 - val_accuracy: 0.7036\n","Epoch 112/500\n","4552/4552 [==============================] - 1s 144us/sample - loss: 0.1351 - accuracy: 0.9750 - val_loss: 0.7166 - val_accuracy: 0.7055\n","Epoch 113/500\n","4552/4552 [==============================] - 1s 138us/sample - loss: 0.1344 - accuracy: 0.9754 - val_loss: 0.7175 - val_accuracy: 0.7055\n","Epoch 114/500\n","4552/4552 [==============================] - 1s 144us/sample - loss: 0.1338 - accuracy: 0.9752 - val_loss: 0.7187 - val_accuracy: 0.7055\n","Epoch 115/500\n","4552/4552 [==============================] - 1s 143us/sample - loss: 0.1331 - accuracy: 0.9756 - val_loss: 0.7198 - val_accuracy: 0.7055\n","Epoch 116/500\n","4552/4552 [==============================] - 1s 137us/sample - loss: 0.1325 - accuracy: 0.9754 - val_loss: 0.7213 - val_accuracy: 0.7095\n","Epoch 117/500\n","4552/4552 [==============================] - 1s 138us/sample - loss: 0.1318 - accuracy: 0.9754 - val_loss: 0.7229 - val_accuracy: 0.7075\n","Epoch 118/500\n","4552/4552 [==============================] - 1s 142us/sample - loss: 0.1312 - accuracy: 0.9758 - val_loss: 0.7237 - val_accuracy: 0.7095\n","Epoch 119/500\n","4552/4552 [==============================] - 1s 137us/sample - loss: 0.1305 - accuracy: 0.9761 - val_loss: 0.7252 - val_accuracy: 0.7115\n","Epoch 120/500\n","4552/4552 [==============================] - 1s 138us/sample - loss: 0.1299 - accuracy: 0.9758 - val_loss: 0.7268 - val_accuracy: 0.7115\n","Epoch 121/500\n","4552/4552 [==============================] - 1s 141us/sample - loss: 0.1292 - accuracy: 0.9758 - val_loss: 0.7281 - val_accuracy: 0.7115\n","Epoch 122/500\n","4552/4552 [==============================] - 1s 139us/sample - loss: 0.1286 - accuracy: 0.9758 - val_loss: 0.7296 - val_accuracy: 0.7134\n","Epoch 123/500\n","4552/4552 [==============================] - 1s 141us/sample - loss: 0.1280 - accuracy: 0.9763 - val_loss: 0.7310 - val_accuracy: 0.7134\n","Epoch 124/500\n","4552/4552 [==============================] - 1s 142us/sample - loss: 0.1274 - accuracy: 0.9761 - val_loss: 0.7324 - val_accuracy: 0.7134\n","Epoch 125/500\n","4552/4552 [==============================] - 1s 140us/sample - loss: 0.1268 - accuracy: 0.9765 - val_loss: 0.7334 - val_accuracy: 0.7134\n","Epoch 126/500\n","4552/4552 [==============================] - 1s 139us/sample - loss: 0.1262 - accuracy: 0.9765 - val_loss: 0.7352 - val_accuracy: 0.7134\n","Epoch 127/500\n","4552/4552 [==============================] - 1s 140us/sample - loss: 0.1256 - accuracy: 0.9765 - val_loss: 0.7363 - val_accuracy: 0.7134\n","Epoch 128/500\n","4552/4552 [==============================] - 1s 139us/sample - loss: 0.1250 - accuracy: 0.9769 - val_loss: 0.7378 - val_accuracy: 0.7134\n","Epoch 129/500\n","4552/4552 [==============================] - 1s 138us/sample - loss: 0.1244 - accuracy: 0.9772 - val_loss: 0.7389 - val_accuracy: 0.7134\n","Epoch 130/500\n","4552/4552 [==============================] - 1s 138us/sample - loss: 0.1237 - accuracy: 0.9767 - val_loss: 0.7401 - val_accuracy: 0.7134\n","Epoch 131/500\n","4552/4552 [==============================] - 1s 142us/sample - loss: 0.1232 - accuracy: 0.9769 - val_loss: 0.7411 - val_accuracy: 0.7134\n","Epoch 132/500\n","4552/4552 [==============================] - 1s 142us/sample - loss: 0.1226 - accuracy: 0.9774 - val_loss: 0.7423 - val_accuracy: 0.7134\n","Epoch 133/500\n","4552/4552 [==============================] - 1s 139us/sample - loss: 0.1220 - accuracy: 0.9774 - val_loss: 0.7437 - val_accuracy: 0.7134\n","Epoch 134/500\n","4552/4552 [==============================] - 1s 142us/sample - loss: 0.1214 - accuracy: 0.9774 - val_loss: 0.7452 - val_accuracy: 0.7134\n","Epoch 135/500\n","4552/4552 [==============================] - 1s 136us/sample - loss: 0.1209 - accuracy: 0.9774 - val_loss: 0.7467 - val_accuracy: 0.7134\n","Epoch 136/500\n","4552/4552 [==============================] - 1s 138us/sample - loss: 0.1203 - accuracy: 0.9774 - val_loss: 0.7482 - val_accuracy: 0.7134\n","Epoch 137/500\n","4552/4552 [==============================] - 1s 139us/sample - loss: 0.1197 - accuracy: 0.9774 - val_loss: 0.7496 - val_accuracy: 0.7134\n","Epoch 138/500\n","4552/4552 [==============================] - 1s 136us/sample - loss: 0.1192 - accuracy: 0.9774 - val_loss: 0.7514 - val_accuracy: 0.7134\n","Epoch 139/500\n","4552/4552 [==============================] - 1s 140us/sample - loss: 0.1186 - accuracy: 0.9776 - val_loss: 0.7526 - val_accuracy: 0.7154\n","Epoch 140/500\n","4552/4552 [==============================] - 1s 141us/sample - loss: 0.1181 - accuracy: 0.9774 - val_loss: 0.7540 - val_accuracy: 0.7154\n","Epoch 141/500\n","4552/4552 [==============================] - 1s 138us/sample - loss: 0.1175 - accuracy: 0.9780 - val_loss: 0.7553 - val_accuracy: 0.7154\n","Epoch 142/500\n","4552/4552 [==============================] - 1s 140us/sample - loss: 0.1170 - accuracy: 0.9778 - val_loss: 0.7569 - val_accuracy: 0.7154\n","Epoch 143/500\n","4552/4552 [==============================] - 1s 137us/sample - loss: 0.1164 - accuracy: 0.9778 - val_loss: 0.7582 - val_accuracy: 0.7154\n","Epoch 144/500\n","4552/4552 [==============================] - 1s 138us/sample - loss: 0.1159 - accuracy: 0.9783 - val_loss: 0.7596 - val_accuracy: 0.7154\n","Epoch 145/500\n","4552/4552 [==============================] - 1s 141us/sample - loss: 0.1153 - accuracy: 0.9778 - val_loss: 0.7612 - val_accuracy: 0.7154\n","Epoch 146/500\n","4552/4552 [==============================] - 1s 137us/sample - loss: 0.1148 - accuracy: 0.9780 - val_loss: 0.7628 - val_accuracy: 0.7174\n","Epoch 147/500\n","4552/4552 [==============================] - 1s 142us/sample - loss: 0.1143 - accuracy: 0.9787 - val_loss: 0.7640 - val_accuracy: 0.7174\n","Epoch 148/500\n","4552/4552 [==============================] - 1s 138us/sample - loss: 0.1138 - accuracy: 0.9789 - val_loss: 0.7656 - val_accuracy: 0.7194\n","Epoch 149/500\n","4552/4552 [==============================] - 1s 140us/sample - loss: 0.1133 - accuracy: 0.9787 - val_loss: 0.7672 - val_accuracy: 0.7213\n","Epoch 150/500\n","4552/4552 [==============================] - 1s 142us/sample - loss: 0.1128 - accuracy: 0.9787 - val_loss: 0.7689 - val_accuracy: 0.7213\n","Epoch 151/500\n","4552/4552 [==============================] - 1s 140us/sample - loss: 0.1122 - accuracy: 0.9791 - val_loss: 0.7707 - val_accuracy: 0.7194\n","Epoch 152/500\n","4552/4552 [==============================] - 1s 142us/sample - loss: 0.1117 - accuracy: 0.9789 - val_loss: 0.7722 - val_accuracy: 0.7213\n","Epoch 153/500\n","4552/4552 [==============================] - 1s 139us/sample - loss: 0.1112 - accuracy: 0.9787 - val_loss: 0.7733 - val_accuracy: 0.7213\n","Epoch 154/500\n","4552/4552 [==============================] - 1s 137us/sample - loss: 0.1107 - accuracy: 0.9789 - val_loss: 0.7749 - val_accuracy: 0.7213\n","Epoch 155/500\n","4552/4552 [==============================] - 1s 138us/sample - loss: 0.1103 - accuracy: 0.9787 - val_loss: 0.7762 - val_accuracy: 0.7253\n","Epoch 156/500\n","4552/4552 [==============================] - 1s 138us/sample - loss: 0.1097 - accuracy: 0.9789 - val_loss: 0.7778 - val_accuracy: 0.7233\n","Epoch 157/500\n","4552/4552 [==============================] - 1s 143us/sample - loss: 0.1093 - accuracy: 0.9793 - val_loss: 0.7796 - val_accuracy: 0.7253\n","Epoch 158/500\n","4552/4552 [==============================] - 1s 136us/sample - loss: 0.1088 - accuracy: 0.9791 - val_loss: 0.7810 - val_accuracy: 0.7253\n","Epoch 159/500\n","4552/4552 [==============================] - 1s 138us/sample - loss: 0.1083 - accuracy: 0.9796 - val_loss: 0.7825 - val_accuracy: 0.7253\n","Epoch 160/500\n","4552/4552 [==============================] - 1s 136us/sample - loss: 0.1078 - accuracy: 0.9800 - val_loss: 0.7841 - val_accuracy: 0.7253\n","Epoch 161/500\n","4552/4552 [==============================] - 1s 137us/sample - loss: 0.1074 - accuracy: 0.9800 - val_loss: 0.7855 - val_accuracy: 0.7253\n","Epoch 162/500\n","4552/4552 [==============================] - 1s 136us/sample - loss: 0.1069 - accuracy: 0.9800 - val_loss: 0.7872 - val_accuracy: 0.7253\n","Epoch 163/500\n","4552/4552 [==============================] - 1s 143us/sample - loss: 0.1064 - accuracy: 0.9802 - val_loss: 0.7888 - val_accuracy: 0.7233\n","Epoch 164/500\n","4552/4552 [==============================] - 1s 137us/sample - loss: 0.1059 - accuracy: 0.9802 - val_loss: 0.7902 - val_accuracy: 0.7233\n","Epoch 165/500\n","4552/4552 [==============================] - 1s 135us/sample - loss: 0.1055 - accuracy: 0.9804 - val_loss: 0.7915 - val_accuracy: 0.7233\n","Epoch 166/500\n","4552/4552 [==============================] - 1s 141us/sample - loss: 0.1050 - accuracy: 0.9804 - val_loss: 0.7932 - val_accuracy: 0.7233\n","Epoch 167/500\n","4552/4552 [==============================] - 1s 134us/sample - loss: 0.1046 - accuracy: 0.9804 - val_loss: 0.7947 - val_accuracy: 0.7213\n","Epoch 168/500\n","4552/4552 [==============================] - 1s 140us/sample - loss: 0.1041 - accuracy: 0.9807 - val_loss: 0.7963 - val_accuracy: 0.7213\n","Epoch 169/500\n","4552/4552 [==============================] - 1s 137us/sample - loss: 0.1037 - accuracy: 0.9809 - val_loss: 0.7975 - val_accuracy: 0.7213\n","Epoch 170/500\n","4552/4552 [==============================] - 1s 138us/sample - loss: 0.1032 - accuracy: 0.9809 - val_loss: 0.7991 - val_accuracy: 0.7213\n","Epoch 171/500\n","4552/4552 [==============================] - 1s 140us/sample - loss: 0.1028 - accuracy: 0.9807 - val_loss: 0.8010 - val_accuracy: 0.7213\n","Epoch 172/500\n","4552/4552 [==============================] - 1s 138us/sample - loss: 0.1024 - accuracy: 0.9811 - val_loss: 0.8023 - val_accuracy: 0.7213\n","Epoch 173/500\n","4552/4552 [==============================] - 1s 136us/sample - loss: 0.1019 - accuracy: 0.9807 - val_loss: 0.8039 - val_accuracy: 0.7213\n","Epoch 174/500\n","4552/4552 [==============================] - 1s 136us/sample - loss: 0.1015 - accuracy: 0.9813 - val_loss: 0.8056 - val_accuracy: 0.7233\n","Epoch 175/500\n","4552/4552 [==============================] - 1s 140us/sample - loss: 0.1011 - accuracy: 0.9811 - val_loss: 0.8073 - val_accuracy: 0.7213\n","Epoch 176/500\n","4552/4552 [==============================] - 1s 138us/sample - loss: 0.1006 - accuracy: 0.9811 - val_loss: 0.8088 - val_accuracy: 0.7233\n","Epoch 177/500\n","4552/4552 [==============================] - 1s 138us/sample - loss: 0.1002 - accuracy: 0.9811 - val_loss: 0.8105 - val_accuracy: 0.7253\n","Epoch 178/500\n","4552/4552 [==============================] - 1s 136us/sample - loss: 0.0998 - accuracy: 0.9813 - val_loss: 0.8119 - val_accuracy: 0.7253\n","Epoch 179/500\n","4552/4552 [==============================] - 1s 140us/sample - loss: 0.0994 - accuracy: 0.9813 - val_loss: 0.8135 - val_accuracy: 0.7253\n","Epoch 180/500\n","4552/4552 [==============================] - 1s 140us/sample - loss: 0.0990 - accuracy: 0.9815 - val_loss: 0.8148 - val_accuracy: 0.7233\n","Epoch 181/500\n","4552/4552 [==============================] - 1s 138us/sample - loss: 0.0986 - accuracy: 0.9820 - val_loss: 0.8163 - val_accuracy: 0.7233\n","Epoch 182/500\n","4416/4552 [============================>.] - ETA: 0s - loss: 0.0980 - accuracy: 0.9817\n","Epoch 00182: Reducing Max LR on Plateau: new max lr will be 0.00025 (if not early_stopping).\n","Restoring model weights from the end of the best epoch.\n","4552/4552 [==============================] - 1s 140us/sample - loss: 0.0981 - accuracy: 0.9815 - val_loss: 0.8175 - val_accuracy: 0.7233\n","Epoch 00182: early stopping\n","Weights from best epoch have been loaded into model.\n","                precision    recall  f1-score   support\n","\n","very_offensive       0.64      0.69      0.67       255\n","        slight       0.66      0.61      0.63       251\n","\n","      accuracy                           0.65       506\n","     macro avg       0.65      0.65      0.65       506\n","  weighted avg       0.65      0.65      0.65       506\n","\n","model /content/drive/My Drive/Weight_file/very_offensive_slight.h5\n","Model Saved\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vqLsGhwSGtcz","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9MdvY6GzI6Ic","colab_type":"code","outputId":"672483b5-488b-45a7-d97f-d4e6d68558ec","executionInfo":{"status":"error","timestamp":1583859550090,"user_tz":-330,"elapsed":3699,"user":{"displayName":"AMBUJE GUPTA","photoUrl":"","userId":"08374827863947652219"}},"colab":{"base_uri":"https://localhost:8080/","height":925}},"source":["import ktrain\n","from ktrain import text\n","\n","\n","\n","modelslist = [['not_motivational','motivational'],['not_offensive','offensive']]\n","data_files = ['memotion_eq_motivation.csv','memotion_eq_offensive.csv']\n","\n","for columnclass, data_file in zip(modelslist, data_files):\n","\n","    \n","    columns=columnclass      \n","    DATA_PATH = '/content/drive/My Drive/data_eqd/' + data_file\n","    print(DATA_PATH)\n","\n","    NUM_WORDS = 90000\n","    MAXLEN = 100\n","\n","    h5name = '_'.join(columnclass)\n","    print('model', h5name)\n","\n","    (x_train, y_train), (x_test, y_test), preproc = text.texts_from_csv(DATA_PATH,\n","                          'cleaned_ocr',\n","                          label_columns = columns,\n","                          val_filepath=None, # if None, 10% of data will be used for validation\n","                          max_features=NUM_WORDS, maxlen=MAXLEN,\n","                        ngram_range=1,)\n","\n","\n","    model = text.text_classifier('logreg', (x_train, y_train), preproc=preproc)\n","\n","    learner = ktrain.get_learner(model, train_data=(x_train, y_train), val_data=(x_test, y_test),batch_size=64)\n","    a='/content/drive/My Drive/Weight_file/'+h5name\n","    learner.autofit(0.001, 20,early_stopping=10,reduce_on_plateau=5)\n","\n","    a=learner.validate(val_data=(x_test, y_test), class_names=columns)\n","  \n","    predictor = ktrain.get_predictor(learner.model, preproc)\n","\n","   # score = 'xyz'########################\n","    h5name = \"/content/drive/My Drive/Weight_file/\"+h5name +  '.h5'\n","    print('model', h5name)\n","\n","    predictor.save(h5name)\n","    print(\"Model Saved\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/data_eqd/memotion_eq_motivation.csv\n","model not_motivational_motivational\n","detected encoding: utf-8 (if wrong, set manually)\n","language: en\n","Word Counts: 11456\n","Nrows: 7704\n","7704 train sequences\n","train sequence lengths:\n","\tmean : 8\n","\t95percentile : 18\n","\t99percentile : 26\n","x_train shape: (7704,100)\n","y_train shape: (7704, 2)\n","857 test sequences\n","test sequence lengths:\n","\tmean : 7\n","\t95percentile : 15\n","\t99percentile : 21\n","x_test shape: (857,100)\n","y_test shape: (857, 2)\n","Is Multi-Label? False\n","compiling word ID features...\n","maxlen is 100\n","done.\n","\n","\n","begin training using triangular learning rate policy with max lr of 0.001...\n","Train on 7704 samples, validate on 857 samples\n","Epoch 1/100\n","1408/7704 [====>.........................] - ETA: 8s - loss: 0.9499 - accuracy: 0.4948"],"name":"stdout"},{"output_type":"error","ename":"Exception","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[0;34m(self, epoch, mode)\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m       \u001b[0;32myield\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    486\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-61-bfde8fa231f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mlearner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mktrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_learner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/My Drive/Weight_file/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mh5name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautofit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreduce_on_plateau\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ktrain/core.py\u001b[0m in \u001b[0;36mautofit\u001b[0;34m(self, lr, epochs, early_stopping, reduce_on_plateau, reduce_factor, cycle_momentum, monitor, checkpoint_folder, verbose, class_weight, callbacks)\u001b[0m\n\u001b[1;32m    858\u001b[0m         hist = self.fit(lr, epochs, early_stopping=early_stopping,\n\u001b[1;32m    859\u001b[0m                         \u001b[0mcheckpoint_folder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_folder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m                         verbose=verbose, class_weight=class_weight, callbacks=kcallbacks)\n\u001b[0m\u001b[1;32m    861\u001b[0m         \u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m         \u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'iterations'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'iterations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ktrain/core.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, lr, n_cycles, cycle_len, cycle_mult, lr_decay, checkpoint_folder, early_stopping, verbose, class_weight, callbacks)\u001b[0m\n\u001b[1;32m    985\u001b[0m                                   \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m                                   \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 987\u001b[0;31m                                   callbacks=kcallbacks)\n\u001b[0m\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msgdr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msgdr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    370\u001b[0m                       total_epochs=1)\n\u001b[1;32m    371\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[0;32m--> 372\u001b[0;31m                                  prefix='val_')\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[0;34m(self, epoch, mode)\u001b[0m\n\u001b[1;32m    683\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0;31m# Epochs only apply to `fit`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m       \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ktrain/lroptimize/triangular.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mcurrent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cannot monitor %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonitor_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mException\u001b[0m: cannot monitor val_loss"]}]},{"cell_type":"code","metadata":{"id":"M-Dg0AoiY32C","colab_type":"code","outputId":"b45d7a71-0599-4d67-eecd-71531653f5bb","executionInfo":{"status":"ok","timestamp":1583860130547,"user_tz":-330,"elapsed":120565,"user":{"displayName":"AMBUJE GUPTA","photoUrl":"","userId":"08374827863947652219"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import ktrain\n","from ktrain import text\n","\n","\n","\n","modelslist = [['not_offensive','offensive']]\n","data_files = ['memotion_eq_offensive.csv']\n","\n","for columnclass, data_file in zip(modelslist, data_files):\n","\n","    \n","    columns=columnclass      \n","    DATA_PATH = '/content/drive/My Drive/data_eqd/' + data_file\n","    print(DATA_PATH)\n","\n","    NUM_WORDS = 90000\n","    MAXLEN = 100\n","\n","    h5name = '_'.join(columnclass)\n","    print('model', h5name)\n","\n","    (x_train, y_train), (x_test, y_test), preproc = text.texts_from_csv(DATA_PATH,\n","                          'cleaned_ocr',\n","                          label_columns = columns,\n","                          val_filepath=None, # if None, 10% of data will be used for validation\n","                          max_features=NUM_WORDS, maxlen=MAXLEN,\n","                        ngram_range=1,)\n","\n","\n","    model = text.text_classifier('fasttext', (x_train, y_train), preproc=preproc)\n","\n","    learner = ktrain.get_learner(model, train_data=(x_train, y_train), val_data=(x_test, y_test),batch_size=64)\n","    a='/content/drive/My Drive/Weight_file/'+h5name\n","    learner.autofit(0.01, 20,early_stopping=15,reduce_on_plateau=5)\n","\n","    a=learner.validate(val_data=(x_test, y_test), class_names=columns)\n","  \n","    predictor = ktrain.get_predictor(learner.model, preproc)\n","\n","   # score = 'xyz'########################\n","    h5name = \"/content/drive/My Drive/Weight_file/\"+h5name +  '.h5'\n","    print('model', h5name)\n","\n","    predictor.save(h5name)\n","    print(\"Model Saved\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/data_eqd/memotion_eq_offensive.csv\n","model not_offensive_offensive\n","detected encoding: utf-8 (if wrong, set manually)\n","language: en\n","Word Counts: 11396\n","Nrows: 7704\n","7704 train sequences\n","train sequence lengths:\n","\tmean : 8\n","\t95percentile : 17\n","\t99percentile : 26\n","x_train shape: (7704,100)\n","y_train shape: (7704, 2)\n","857 test sequences\n","test sequence lengths:\n","\tmean : 8\n","\t95percentile : 17\n","\t99percentile : 22\n","x_test shape: (857,100)\n","y_test shape: (857, 2)\n","Is Multi-Label? False\n","compiling word ID features...\n","maxlen is 100\n","done.\n","\n","\n","begin training using triangular learning rate policy with max lr of 0.01...\n","Train on 7704 samples, validate on 857 samples\n","Epoch 1/20\n","7704/7704 [==============================] - 7s 881us/sample - loss: 0.7737 - accuracy: 0.5123 - val_loss: 0.6948 - val_accuracy: 0.4761\n","Epoch 2/20\n","7704/7704 [==============================] - 6s 752us/sample - loss: 0.6769 - accuracy: 0.5826 - val_loss: 0.6900 - val_accuracy: 0.5111\n","Epoch 3/20\n","7704/7704 [==============================] - 6s 767us/sample - loss: 0.5988 - accuracy: 0.6759 - val_loss: 0.6617 - val_accuracy: 0.6161\n","Epoch 4/20\n","7704/7704 [==============================] - 6s 751us/sample - loss: 0.4983 - accuracy: 0.7592 - val_loss: 0.6472 - val_accuracy: 0.6499\n","Epoch 5/20\n","7704/7704 [==============================] - 6s 756us/sample - loss: 0.4174 - accuracy: 0.8100 - val_loss: 0.6675 - val_accuracy: 0.6604\n","Epoch 6/20\n","7704/7704 [==============================] - 6s 752us/sample - loss: 0.3447 - accuracy: 0.8512 - val_loss: 0.7011 - val_accuracy: 0.7013\n","Epoch 7/20\n","7704/7704 [==============================] - 6s 742us/sample - loss: 0.2983 - accuracy: 0.8732 - val_loss: 0.7366 - val_accuracy: 0.7013\n","Epoch 8/20\n","7704/7704 [==============================] - 6s 760us/sample - loss: 0.2586 - accuracy: 0.8921 - val_loss: 0.7702 - val_accuracy: 0.7095\n","Epoch 9/20\n","7616/7704 [============================>.] - ETA: 0s - loss: 0.2231 - accuracy: 0.9132\n","Epoch 00009: Reducing Max LR on Plateau: new max lr will be 0.005 (if not early_stopping).\n","7704/7704 [==============================] - 6s 779us/sample - loss: 0.2227 - accuracy: 0.9132 - val_loss: 0.7846 - val_accuracy: 0.7083\n","Epoch 10/20\n","7704/7704 [==============================] - 6s 769us/sample - loss: 0.1867 - accuracy: 0.9264 - val_loss: 0.8899 - val_accuracy: 0.7141\n","Epoch 11/20\n","7704/7704 [==============================] - 6s 782us/sample - loss: 0.1529 - accuracy: 0.9399 - val_loss: 0.9809 - val_accuracy: 0.7235\n","Epoch 12/20\n","7704/7704 [==============================] - 6s 787us/sample - loss: 0.1340 - accuracy: 0.9459 - val_loss: 1.0406 - val_accuracy: 0.7060\n","Epoch 13/20\n","7704/7704 [==============================] - 6s 784us/sample - loss: 0.1251 - accuracy: 0.9540 - val_loss: 1.0244 - val_accuracy: 0.7188\n","Epoch 14/20\n","7680/7704 [============================>.] - ETA: 0s - loss: 0.1095 - accuracy: 0.9591\n","Epoch 00014: Reducing Max LR on Plateau: new max lr will be 0.0025 (if not early_stopping).\n","7704/7704 [==============================] - 6s 784us/sample - loss: 0.1097 - accuracy: 0.9590 - val_loss: 1.1071 - val_accuracy: 0.7328\n","Epoch 15/20\n","7704/7704 [==============================] - 6s 781us/sample - loss: 0.0962 - accuracy: 0.9642 - val_loss: 1.1282 - val_accuracy: 0.7340\n","Epoch 16/20\n","7704/7704 [==============================] - 6s 778us/sample - loss: 0.0904 - accuracy: 0.9656 - val_loss: 1.1895 - val_accuracy: 0.7375\n","Epoch 17/20\n","7704/7704 [==============================] - 6s 763us/sample - loss: 0.0772 - accuracy: 0.9725 - val_loss: 1.2188 - val_accuracy: 0.7223\n","Epoch 18/20\n","7704/7704 [==============================] - 6s 769us/sample - loss: 0.0804 - accuracy: 0.9718 - val_loss: 1.1897 - val_accuracy: 0.7293\n","Epoch 19/20\n","7680/7704 [============================>.] - ETA: 0s - loss: 0.0759 - accuracy: 0.9710\n","Epoch 00019: Reducing Max LR on Plateau: new max lr will be 0.00125 (if not early_stopping).\n","Restoring model weights from the end of the best epoch.\n","7704/7704 [==============================] - 6s 774us/sample - loss: 0.0758 - accuracy: 0.9709 - val_loss: 1.2503 - val_accuracy: 0.7211\n","Epoch 00019: early stopping\n","Weights from best epoch have been loaded into model.\n","               precision    recall  f1-score   support\n","\n","not_offensive       0.67      0.65      0.66       449\n","    offensive       0.63      0.65      0.64       408\n","\n","     accuracy                           0.65       857\n","    macro avg       0.65      0.65      0.65       857\n"," weighted avg       0.65      0.65      0.65       857\n","\n","model /content/drive/My Drive/Weight_file/not_offensive_offensive.h5\n","Model Saved\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GeixhmGBI56V","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lfZLxlNoI5qb","colab_type":"code","outputId":"bdc26519-be9e-430f-ff9e-47aeafcef8e0","executionInfo":{"status":"error","timestamp":1583857103890,"user_tz":-330,"elapsed":3051,"user":{"displayName":"AMBUJE GUPTA","photoUrl":"","userId":"08374827863947652219"}},"colab":{"base_uri":"https://localhost:8080/","height":925}},"source":["import ktrain\n","from ktrain import text\n","\n","\n","\n","modelslist = [['not_funny','fun'],['not_sarcastic','sarcastic']]\n","data_files = ['memotion_eq_humour.csv','memotion_eq_sarcastic.csv']\n","\n","for columnclass, data_file in zip(modelslist, data_files):\n","\n","    \n","    columns=columnclass      \n","    DATA_PATH = '/content/drive/My Drive/data_eqd/' + data_file\n","    print(DATA_PATH)\n","\n","    NUM_WORDS = 90000\n","    MAXLEN = 100\n","\n","    h5name = '_'.join(columnclass)\n","    print('model', h5name)\n","\n","    (x_train, y_train), (x_test, y_test), preproc = text.texts_from_csv(DATA_PATH,\n","                          'cleaned_ocr',\n","                          label_columns = columns,\n","                          val_filepath=None, # if None, 10% of data will be used for validation\n","                          max_features=NUM_WORDS, maxlen=MAXLEN,\n","                        ngram_range=1,)\n","\n","\n","    model = text.text_classifier('fasttext', (x_train, y_train), preproc=preproc)\n","\n","    learner = ktrain.get_learner(model, train_data=(x_train, y_train), val_data=(x_test, y_test))\n","    a='/content/drive/My Drive/Weight_file/'+h5name\n","    learner.autofit(0.001, 15,early_stopping=8 ,reduce_on_plateau=4)\n","\n","    a=learner.validate(val_data=(x_test, y_test), class_names=columns)\n","  \n","    predictor = ktrain.get_predictor(learner.model, preproc)\n","\n","   # score = 'xyz'########################\n","    h5name = \"/content/drive/My Drive/Weight_file/\"+h5name +  '.h5'\n","    print('model', h5name)\n","\n","    predictor.save(h5name)\n","    print(\"Model Saved\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/data_eqd/memotion_eq_humour.csv\n","model not_funny_fun\n","detected encoding: utf-8 (if wrong, set manually)\n","language: en\n","Word Counts: 11546\n","Nrows: 9264\n","9264 train sequences\n","train sequence lengths:\n","\tmean : 8\n","\t95percentile : 18\n","\t99percentile : 26\n","x_train shape: (9264,100)\n","y_train shape: (9264, 2)\n","1030 test sequences\n","test sequence lengths:\n","\tmean : 8\n","\t95percentile : 17\n","\t99percentile : 25\n","x_test shape: (1030,100)\n","y_test shape: (1030, 2)\n","Is Multi-Label? False\n","compiling word ID features...\n","maxlen is 100\n","done.\n","\n","\n","begin training using triangular learning rate policy with max lr of 0.001...\n","Train on 9264 samples, validate on 1030 samples\n","Epoch 1/15\n","  32/9264 [..............................] - ETA: 2:05"],"name":"stdout"},{"output_type":"error","ename":"Exception","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[0;34m(self, epoch, mode)\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m       \u001b[0;32myield\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1822\u001b[0;31m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1823\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgraph_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2150\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2151\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2040\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2041\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2042\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mdistributed_function\u001b[0;34m(input_iterator)\u001b[0m\n\u001b[1;32m     72\u001b[0m     outputs = strategy.experimental_run_v2(\n\u001b[0;32m---> 73\u001b[0;31m         per_replica_function, args=(model, x, y, sample_weights))\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;31m# Out of PerReplica outputs reduce or pick values to return.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mexperimental_run_v2\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    759\u001b[0m                                 convert_by_default=False)\n\u001b[0;32m--> 760\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extended\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mcall_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1786\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1787\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   2131\u001b[0m         replica_id_in_sync_group=constant_op.constant(0, dtypes.int32)):\n\u001b[0;32m-> 2132\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       output_loss_metrics=model._output_loss_metrics)\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, inputs, targets, sample_weights, output_loss_metrics)\u001b[0m\n\u001b[1;32m    310\u001b[0m           \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m           output_loss_metrics=output_loss_metrics))\n\u001b[0m\u001b[1;32m    312\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36m_process_single_batch\u001b[0;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[1;32m    251\u001b[0m               \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m               training=training))\n\u001b[0m\u001b[1;32m    253\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36m_model_loss\u001b[0;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m   \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m   \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m                   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    707\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m         convert_kwargs_to_constants=base_layer_utils.call_context().saving)\n\u001b[0m\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask, convert_kwargs_to_constants)\u001b[0m\n\u001b[1;32m    859\u001b[0m           \u001b[0;31m# Compute outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m           \u001b[0moutput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputed_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m                   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/layers/core.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m    165\u001b[0m                                  \u001b[0mdropped_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                                  lambda: array_ops.identity(inputs))\n\u001b[0m\u001b[1;32m    167\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msmart_cond\u001b[0;34m(pred, true_fn, false_fn, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m   return smart_module.smart_cond(\n\u001b[0;32m---> 59\u001b[0;31m       pred, true_fn=true_fn, false_fn=false_fn, name=name)\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/smart_cond.py\u001b[0m in \u001b[0;36msmart_cond\u001b[0;34m(pred, true_fn, false_fn, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpred_value\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtrue_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/layers/core.py\u001b[0m in \u001b[0;36mdropped_inputs\u001b[0;34m()\u001b[0m\n\u001b[1;32m    161\u001b[0m           \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m           rate=self.rate)\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    506\u001b[0m                 instructions)\n\u001b[0;32m--> 507\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(x, keep_prob, noise_shape, seed, name, rate)\u001b[0m\n\u001b[1;32m   4228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4229\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mdropout_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoise_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mdropout_v2\u001b[0;34m(x, rate, noise_shape, seed, name)\u001b[0m\n\u001b[1;32m   4307\u001b[0m     random_tensor = random_ops.random_uniform(\n\u001b[0;32m-> 4308\u001b[0;31m         noise_shape, seed=seed, dtype=x.dtype)\n\u001b[0m\u001b[1;32m   4309\u001b[0m     \u001b[0mkeep_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/random_ops.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(shape, minval, maxval, dtype, seed, name)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0mminval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"min\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m     \u001b[0mmaxval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"max\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m     \u001b[0mseed1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, preferred_dtype, dtype_hint)\u001b[0m\n\u001b[1;32m   1183\u001b[0m       \"dtype_hint\", dtype_hint, \"preferred_dtype\", preferred_dtype)\n\u001b[0;32m-> 1184\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_tensor_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1241\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_hint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1242\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m   1243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx, accept_composite_tensors)\u001b[0m\n\u001b[1;32m   1295\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1296\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    226\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 227\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    264\u001b[0m           \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m           allow_broadcast=allow_broadcast))\n\u001b[0m\u001b[1;32m    266\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-51-e2bdfbbb1926>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mlearner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mktrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_learner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/My Drive/Weight_file/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mh5name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautofit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mreduce_on_plateau\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ktrain/core.py\u001b[0m in \u001b[0;36mautofit\u001b[0;34m(self, lr, epochs, early_stopping, reduce_on_plateau, reduce_factor, cycle_momentum, monitor, checkpoint_folder, verbose, class_weight, callbacks)\u001b[0m\n\u001b[1;32m    858\u001b[0m         hist = self.fit(lr, epochs, early_stopping=early_stopping,\n\u001b[1;32m    859\u001b[0m                         \u001b[0mcheckpoint_folder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_folder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m                         verbose=verbose, class_weight=class_weight, callbacks=kcallbacks)\n\u001b[0m\u001b[1;32m    861\u001b[0m         \u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m         \u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'iterations'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'iterations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ktrain/core.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, lr, n_cycles, cycle_len, cycle_mult, lr_decay, checkpoint_folder, early_stopping, verbose, class_weight, callbacks)\u001b[0m\n\u001b[1;32m    985\u001b[0m                                   \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m                                   \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 987\u001b[0;31m                                   callbacks=kcallbacks)\n\u001b[0m\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msgdr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msgdr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    370\u001b[0m                       total_epochs=1)\n\u001b[1;32m    371\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[0;32m--> 372\u001b[0;31m                                  prefix='val_')\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[0;34m(self, epoch, mode)\u001b[0m\n\u001b[1;32m    683\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0;31m# Epochs only apply to `fit`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m       \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ktrain/lroptimize/triangular.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mcurrent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cannot monitor %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonitor_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mException\u001b[0m: cannot monitor val_loss"]}]},{"cell_type":"code","metadata":{"id":"oA6R2oYwNspF","colab_type":"code","outputId":"c22dc675-3d2b-4156-a06b-6fb9caed237c","executionInfo":{"status":"ok","timestamp":1583858326169,"user_tz":-330,"elapsed":296314,"user":{"displayName":"AMBUJE GUPTA","photoUrl":"","userId":"08374827863947652219"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import ktrain\n","from ktrain import text\n","\n","\n","\n","modelslist = [['positive','neutral'],['positive','negative'],['negative','neutral']]\n","data_files = ['Pos-Neu-memotion_eq_sentiment.csv','Pos-Neg-memotion_eq_sentiment.csv','Neu-Neg-memotion_eq_sentiment.csv']\n","\n","for columnclass, data_file in zip(modelslist, data_files):\n","\n","    \n","    columns=columnclass      \n","    DATA_PATH = '/content/drive/My Drive/data_eqd/' + data_file\n","    print(DATA_PATH)\n","\n","    NUM_WORDS = 90000\n","    MAXLEN = 100\n","\n","    h5name = '_'.join(columnclass)\n","    print('model', h5name)\n","\n","    (x_train, y_train), (x_test, y_test), preproc = text.texts_from_csv(DATA_PATH,\n","                          'cleaned_ocr',\n","                          label_columns = columns,\n","                          val_filepath=None, # if None, 10% of data will be used for validation\n","                          max_features=NUM_WORDS, maxlen=MAXLEN,\n","                        ngram_range=1,)\n","\n","\n","    model = text.text_classifier('fasttext', (x_train, y_train), preproc=preproc)\n","\n","    learner = ktrain.get_learner(model, train_data=(x_train, y_train), val_data=(x_test, y_test),batch_size=64)\n","    a='/content/drive/My Drive/Weight_file/'+h5name\n","    learner.autofit(0.001, 15,early_stopping=8,reduce_on_plateau=4)\n","\n","    a=learner.validate(val_data=(x_test, y_test), class_names=columns)\n","  \n","    predictor = ktrain.get_predictor(learner.model, preproc)\n","\n","   # score = 'xyz'########################\n","    h5name = \"/content/drive/My Drive/Weight_file/\"+h5name +  '.h5'\n","    print('model', h5name)\n","\n","    predictor.save(h5name)\n","    print(\"Model Saved\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/data_eqd/Pos-Neu-memotion_eq_sentiment.csv\n","model positive_neutral\n","detected encoding: utf-8 (if wrong, set manually)\n","language: en\n","Word Counts: 10840\n","Nrows: 7705\n","7705 train sequences\n","train sequence lengths:\n","\tmean : 8\n","\t95percentile : 17\n","\t99percentile : 25\n","x_train shape: (7705,100)\n","y_train shape: (7705, 2)\n","857 test sequences\n","test sequence lengths:\n","\tmean : 7\n","\t95percentile : 16\n","\t99percentile : 24\n","x_test shape: (857,100)\n","y_test shape: (857, 2)\n","Is Multi-Label? False\n","compiling word ID features...\n","maxlen is 100\n","done.\n","\n","\n","begin training using triangular learning rate policy with max lr of 0.001...\n","Train on 7705 samples, validate on 857 samples\n","Epoch 1/15\n","7705/7705 [==============================] - 7s 931us/sample - loss: 0.8988 - accuracy: 0.5006 - val_loss: 0.6951 - val_accuracy: 0.5111\n","Epoch 2/15\n","7705/7705 [==============================] - 6s 827us/sample - loss: 0.7548 - accuracy: 0.5422 - val_loss: 0.6928 - val_accuracy: 0.5111\n","Epoch 3/15\n","7705/7705 [==============================] - 6s 820us/sample - loss: 0.6944 - accuracy: 0.5849 - val_loss: 0.6878 - val_accuracy: 0.5099\n","Epoch 4/15\n","7705/7705 [==============================] - 6s 788us/sample - loss: 0.6392 - accuracy: 0.6349 - val_loss: 0.6709 - val_accuracy: 0.5706\n","Epoch 5/15\n","7705/7705 [==============================] - 6s 804us/sample - loss: 0.5777 - accuracy: 0.6954 - val_loss: 0.6398 - val_accuracy: 0.6324\n","Epoch 6/15\n","7705/7705 [==============================] - 6s 814us/sample - loss: 0.5089 - accuracy: 0.7520 - val_loss: 0.6041 - val_accuracy: 0.6604\n","Epoch 7/15\n","7705/7705 [==============================] - 6s 784us/sample - loss: 0.4388 - accuracy: 0.7979 - val_loss: 0.5780 - val_accuracy: 0.6943\n","Epoch 8/15\n","7705/7705 [==============================] - 6s 795us/sample - loss: 0.3653 - accuracy: 0.8369 - val_loss: 0.5640 - val_accuracy: 0.7270\n","Epoch 9/15\n","7705/7705 [==============================] - 6s 794us/sample - loss: 0.3130 - accuracy: 0.8658 - val_loss: 0.5495 - val_accuracy: 0.7433\n","Epoch 10/15\n","7705/7705 [==============================] - 6s 824us/sample - loss: 0.2619 - accuracy: 0.8933 - val_loss: 0.5492 - val_accuracy: 0.7608\n","Epoch 11/15\n","7705/7705 [==============================] - 6s 795us/sample - loss: 0.2262 - accuracy: 0.9098 - val_loss: 0.5347 - val_accuracy: 0.7666\n","Epoch 12/15\n","7705/7705 [==============================] - 6s 793us/sample - loss: 0.2041 - accuracy: 0.9165 - val_loss: 0.5570 - val_accuracy: 0.7736\n","Epoch 13/15\n","7705/7705 [==============================] - 6s 807us/sample - loss: 0.1788 - accuracy: 0.9294 - val_loss: 0.5696 - val_accuracy: 0.7876\n","Epoch 14/15\n","7705/7705 [==============================] - 6s 813us/sample - loss: 0.1614 - accuracy: 0.9385 - val_loss: 0.5929 - val_accuracy: 0.7736\n","Epoch 15/15\n","7680/7705 [============================>.] - ETA: 0s - loss: 0.1445 - accuracy: 0.9449\n","Epoch 00015: Reducing Max LR on Plateau: new max lr will be 0.0005 (if not early_stopping).\n","7705/7705 [==============================] - 6s 801us/sample - loss: 0.1442 - accuracy: 0.9451 - val_loss: 0.5982 - val_accuracy: 0.7760\n","Weights from best epoch have been loaded into model.\n","              precision    recall  f1-score   support\n","\n","    positive       0.89      0.62      0.73       419\n","     neutral       0.72      0.93      0.81       438\n","\n","    accuracy                           0.78       857\n","   macro avg       0.80      0.77      0.77       857\n","weighted avg       0.80      0.78      0.77       857\n","\n","model /content/drive/My Drive/Weight_file/positive_neutral.h5\n","Model Saved\n","/content/drive/My Drive/data_eqd/Pos-Neg-memotion_eq_sentiment.csv\n","model positive_negative\n","detected encoding: utf-8 (if wrong, set manually)\n","language: en\n","Word Counts: 9096\n","Nrows: 7719\n","7719 train sequences\n","train sequence lengths:\n","\tmean : 8\n","\t95percentile : 18\n","\t99percentile : 26\n","x_train shape: (7719,100)\n","y_train shape: (7719, 2)\n","858 test sequences\n","test sequence lengths:\n","\tmean : 8\n","\t95percentile : 17\n","\t99percentile : 26\n","x_test shape: (858,100)\n","y_test shape: (858, 2)\n","Is Multi-Label? False\n","compiling word ID features...\n","maxlen is 100\n","done.\n","\n","\n","begin training using triangular learning rate policy with max lr of 0.001...\n","Train on 7719 samples, validate on 858 samples\n","Epoch 1/15\n","7719/7719 [==============================] - 7s 938us/sample - loss: 0.8997 - accuracy: 0.5278 - val_loss: 0.6903 - val_accuracy: 0.5128\n","Epoch 2/15\n","7719/7719 [==============================] - 6s 832us/sample - loss: 0.7377 - accuracy: 0.5810 - val_loss: 0.6816 - val_accuracy: 0.5326\n","Epoch 3/15\n","7719/7719 [==============================] - 6s 815us/sample - loss: 0.6104 - accuracy: 0.6741 - val_loss: 0.6356 - val_accuracy: 0.6876\n","Epoch 4/15\n","7719/7719 [==============================] - 6s 814us/sample - loss: 0.4837 - accuracy: 0.7673 - val_loss: 0.5044 - val_accuracy: 0.8636\n","Epoch 5/15\n","7719/7719 [==============================] - 6s 813us/sample - loss: 0.3620 - accuracy: 0.8430 - val_loss: 0.3263 - val_accuracy: 0.9207\n","Epoch 6/15\n","7719/7719 [==============================] - 6s 811us/sample - loss: 0.2757 - accuracy: 0.8881 - val_loss: 0.2070 - val_accuracy: 0.9371\n","Epoch 7/15\n","7719/7719 [==============================] - 6s 815us/sample - loss: 0.1952 - accuracy: 0.9278 - val_loss: 0.1537 - val_accuracy: 0.9487\n","Epoch 8/15\n","7719/7719 [==============================] - 6s 807us/sample - loss: 0.1583 - accuracy: 0.9399 - val_loss: 0.1489 - val_accuracy: 0.9406\n","Epoch 9/15\n","7719/7719 [==============================] - 6s 813us/sample - loss: 0.1272 - accuracy: 0.9492 - val_loss: 0.1304 - val_accuracy: 0.9499\n","Epoch 10/15\n","7719/7719 [==============================] - 6s 809us/sample - loss: 0.1001 - accuracy: 0.9636 - val_loss: 0.1187 - val_accuracy: 0.9557\n","Epoch 11/15\n","7719/7719 [==============================] - 6s 813us/sample - loss: 0.0887 - accuracy: 0.9702 - val_loss: 0.1147 - val_accuracy: 0.9592\n","Epoch 12/15\n","7719/7719 [==============================] - 6s 804us/sample - loss: 0.0783 - accuracy: 0.9729 - val_loss: 0.1230 - val_accuracy: 0.9569\n","Epoch 13/15\n","7719/7719 [==============================] - 6s 810us/sample - loss: 0.0781 - accuracy: 0.9749 - val_loss: 0.1415 - val_accuracy: 0.9476\n","Epoch 14/15\n","7719/7719 [==============================] - 6s 799us/sample - loss: 0.0562 - accuracy: 0.9803 - val_loss: 0.1338 - val_accuracy: 0.9510\n","Epoch 15/15\n","7680/7719 [============================>.] - ETA: 0s - loss: 0.0494 - accuracy: 0.9827\n","Epoch 00015: Reducing Max LR on Plateau: new max lr will be 0.0005 (if not early_stopping).\n","7719/7719 [==============================] - 6s 801us/sample - loss: 0.0494 - accuracy: 0.9828 - val_loss: 0.1165 - val_accuracy: 0.9580\n","Weights from best epoch have been loaded into model.\n","              precision    recall  f1-score   support\n","\n","    positive       1.00      0.92      0.96       418\n","    negative       0.93      1.00      0.96       440\n","\n","    accuracy                           0.96       858\n","   macro avg       0.96      0.96      0.96       858\n","weighted avg       0.96      0.96      0.96       858\n","\n","model /content/drive/My Drive/Weight_file/positive_negative.h5\n","Model Saved\n","/content/drive/My Drive/data_eqd/Neu-Neg-memotion_eq_sentiment.csv\n","model negative_neutral\n","detected encoding: utf-8 (if wrong, set manually)\n","language: en\n","Word Counts: 6855\n","Nrows: 7937\n","7937 train sequences\n","train sequence lengths:\n","\tmean : 8\n","\t95percentile : 18\n","\t99percentile : 26\n","x_train shape: (7937,100)\n","y_train shape: (7937, 2)\n","882 test sequences\n","test sequence lengths:\n","\tmean : 8\n","\t95percentile : 18\n","\t99percentile : 28\n","x_test shape: (882,100)\n","y_test shape: (882, 2)\n","Is Multi-Label? False\n","compiling word ID features...\n","maxlen is 100\n","done.\n","\n","\n","begin training using triangular learning rate policy with max lr of 0.001...\n","Train on 7937 samples, validate on 882 samples\n","Epoch 1/15\n","7937/7937 [==============================] - 8s 965us/sample - loss: 0.8410 - accuracy: 0.5400 - val_loss: 0.6876 - val_accuracy: 0.5317\n","Epoch 2/15\n","7937/7937 [==============================] - 6s 818us/sample - loss: 0.6590 - accuracy: 0.6366 - val_loss: 0.6681 - val_accuracy: 0.7698\n","Epoch 3/15\n","7937/7937 [==============================] - 6s 811us/sample - loss: 0.5230 - accuracy: 0.7329 - val_loss: 0.5797 - val_accuracy: 0.9240\n","Epoch 4/15\n","7937/7937 [==============================] - 7s 823us/sample - loss: 0.3648 - accuracy: 0.8420 - val_loss: 0.3648 - val_accuracy: 0.9626\n","Epoch 5/15\n","7937/7937 [==============================] - 7s 823us/sample - loss: 0.2401 - accuracy: 0.9051 - val_loss: 0.1645 - val_accuracy: 0.9705\n","Epoch 6/15\n","7937/7937 [==============================] - 6s 819us/sample - loss: 0.1632 - accuracy: 0.9373 - val_loss: 0.0800 - val_accuracy: 0.9898\n","Epoch 7/15\n","7937/7937 [==============================] - 6s 809us/sample - loss: 0.1220 - accuracy: 0.9541 - val_loss: 0.0540 - val_accuracy: 0.9875\n","Epoch 8/15\n","7937/7937 [==============================] - 7s 824us/sample - loss: 0.0910 - accuracy: 0.9676 - val_loss: 0.0381 - val_accuracy: 0.9875\n","Epoch 9/15\n","7937/7937 [==============================] - 7s 819us/sample - loss: 0.0727 - accuracy: 0.9735 - val_loss: 0.0377 - val_accuracy: 0.9875\n","Epoch 10/15\n","7937/7937 [==============================] - 6s 805us/sample - loss: 0.0671 - accuracy: 0.9773 - val_loss: 0.0342 - val_accuracy: 0.9875\n","Epoch 11/15\n","7937/7937 [==============================] - 6s 799us/sample - loss: 0.0593 - accuracy: 0.9793 - val_loss: 0.0320 - val_accuracy: 0.9864\n","Epoch 12/15\n","7937/7937 [==============================] - 6s 818us/sample - loss: 0.0457 - accuracy: 0.9844 - val_loss: 0.0237 - val_accuracy: 0.9887\n","Epoch 13/15\n","7937/7937 [==============================] - 6s 789us/sample - loss: 0.0447 - accuracy: 0.9853 - val_loss: 0.0166 - val_accuracy: 0.9977\n","Epoch 14/15\n","7937/7937 [==============================] - 6s 804us/sample - loss: 0.0392 - accuracy: 0.9878 - val_loss: 0.0215 - val_accuracy: 0.9932\n","Epoch 15/15\n","7937/7937 [==============================] - 6s 814us/sample - loss: 0.0385 - accuracy: 0.9875 - val_loss: 0.0198 - val_accuracy: 0.9932\n","Weights from best epoch have been loaded into model.\n","              precision    recall  f1-score   support\n","\n","    negative       0.99      1.00      0.99       449\n","     neutral       1.00      0.99      0.99       433\n","\n","    accuracy                           0.99       882\n","   macro avg       0.99      0.99      0.99       882\n","weighted avg       0.99      0.99      0.99       882\n","\n","model /content/drive/My Drive/Weight_file/negative_neutral.h5\n","Model Saved\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hmhIcZBUS5R5","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9CWDLQS9UJtN","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"broMOq-OUJwC","colab_type":"code","outputId":"ef74b6f1-133c-4cda-c27a-f75296ece784","executionInfo":{"status":"ok","timestamp":1583859000761,"user_tz":-330,"elapsed":533205,"user":{"displayName":"AMBUJE GUPTA","photoUrl":"","userId":"08374827863947652219"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import ktrain\n","from ktrain import text\n","\n","\n","\n","modelslist = [['positive','neutral']]\n","data_files = ['Pos-Neu-memotion_eq_sentiment.csv']\n","for columnclass, data_file in zip(modelslist, data_files):\n","\n","    \n","    columns=columnclass      \n","    DATA_PATH = '/content/drive/My Drive/data_eqd/' + data_file\n","    print(DATA_PATH)\n","\n","    NUM_WORDS = 90000\n","    MAXLEN = 100\n","\n","    h5name = '_'.join(columnclass)\n","    print('model', h5name)\n","\n","    (x_train, y_train), (x_test, y_test), preproc = text.texts_from_csv(DATA_PATH,\n","                          'cleaned_ocr',\n","                          label_columns = columns,\n","                          val_filepath=None, # if None, 10% of data will be used for validation\n","                          max_features=NUM_WORDS, maxlen=MAXLEN,\n","                        ngram_range=1,)\n","\n","\n","    model = text.text_classifier('fasttext', (x_train, y_train), preproc=preproc)\n","\n","    learner = ktrain.get_learner(model, train_data=(x_train, y_train), val_data=(x_test, y_test),batch_size=64)\n","    a='/content/drive/My Drive/Weight_file/'+h5name\n","    learner.autofit(0.001, 150,early_stopping=80,reduce_on_plateau=40)\n","\n","    a=learner.validate(val_data=(x_test, y_test), class_names=columns)\n","  \n","    predictor = ktrain.get_predictor(learner.model, preproc)\n","\n","   # score = 'xyz'########################\n","    h5name = \"/content/drive/My Drive/Weight_file/\"+h5name +  '.h5'\n","    print('model', h5name)\n","\n","    predictor.save(h5name)\n","    print(\"Model Saved\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/data_eqd/Pos-Neu-memotion_eq_sentiment.csv\n","model positive_neutral\n","detected encoding: utf-8 (if wrong, set manually)\n","language: en\n","Word Counts: 10932\n","Nrows: 7705\n","7705 train sequences\n","train sequence lengths:\n","\tmean : 8\n","\t95percentile : 17\n","\t99percentile : 26\n","x_train shape: (7705,100)\n","y_train shape: (7705, 2)\n","857 test sequences\n","test sequence lengths:\n","\tmean : 7\n","\t95percentile : 16\n","\t99percentile : 23\n","x_test shape: (857,100)\n","y_test shape: (857, 2)\n","Is Multi-Label? False\n","compiling word ID features...\n","maxlen is 100\n","done.\n","\n","\n","begin training using triangular learning rate policy with max lr of 0.001...\n","Train on 7705 samples, validate on 857 samples\n","Epoch 1/150\n","7705/7705 [==============================] - 7s 886us/sample - loss: 0.8725 - accuracy: 0.5053 - val_loss: 0.6939 - val_accuracy: 0.4702\n","Epoch 2/150\n","7705/7705 [==============================] - 6s 772us/sample - loss: 0.7506 - accuracy: 0.5468 - val_loss: 0.6926 - val_accuracy: 0.5006\n","Epoch 3/150\n","7705/7705 [==============================] - 6s 754us/sample - loss: 0.6999 - accuracy: 0.5759 - val_loss: 0.6865 - val_accuracy: 0.5974\n","Epoch 4/150\n","7705/7705 [==============================] - 6s 766us/sample - loss: 0.6503 - accuracy: 0.6250 - val_loss: 0.6672 - val_accuracy: 0.6534\n","Epoch 5/150\n","7705/7705 [==============================] - 6s 756us/sample - loss: 0.6033 - accuracy: 0.6719 - val_loss: 0.6273 - val_accuracy: 0.6896\n","Epoch 6/150\n","7705/7705 [==============================] - 6s 754us/sample - loss: 0.5388 - accuracy: 0.7232 - val_loss: 0.5803 - val_accuracy: 0.7200\n","Epoch 7/150\n","7705/7705 [==============================] - 6s 764us/sample - loss: 0.4632 - accuracy: 0.7761 - val_loss: 0.5339 - val_accuracy: 0.7316\n","Epoch 8/150\n","7705/7705 [==============================] - 6s 748us/sample - loss: 0.3966 - accuracy: 0.8188 - val_loss: 0.5068 - val_accuracy: 0.7515\n","Epoch 9/150\n","7705/7705 [==============================] - 6s 763us/sample - loss: 0.3265 - accuracy: 0.8626 - val_loss: 0.4801 - val_accuracy: 0.7795\n","Epoch 10/150\n","7705/7705 [==============================] - 6s 764us/sample - loss: 0.2837 - accuracy: 0.8780 - val_loss: 0.4642 - val_accuracy: 0.8016\n","Epoch 11/150\n","7705/7705 [==============================] - 6s 752us/sample - loss: 0.2358 - accuracy: 0.9049 - val_loss: 0.4462 - val_accuracy: 0.8063\n","Epoch 12/150\n","7705/7705 [==============================] - 6s 755us/sample - loss: 0.2009 - accuracy: 0.9197 - val_loss: 0.4602 - val_accuracy: 0.8063\n","Epoch 13/150\n","7705/7705 [==============================] - 6s 763us/sample - loss: 0.1895 - accuracy: 0.9291 - val_loss: 0.4274 - val_accuracy: 0.8215\n","Epoch 14/150\n","7705/7705 [==============================] - 6s 733us/sample - loss: 0.1594 - accuracy: 0.9386 - val_loss: 0.4504 - val_accuracy: 0.8203\n","Epoch 15/150\n","7705/7705 [==============================] - 6s 739us/sample - loss: 0.1437 - accuracy: 0.9433 - val_loss: 0.5063 - val_accuracy: 0.8168\n","Epoch 16/150\n","7705/7705 [==============================] - 6s 735us/sample - loss: 0.1309 - accuracy: 0.9517 - val_loss: 0.4963 - val_accuracy: 0.8226\n","Epoch 17/150\n","7705/7705 [==============================] - 6s 728us/sample - loss: 0.1147 - accuracy: 0.9576 - val_loss: 0.5306 - val_accuracy: 0.8051\n","Epoch 18/150\n","7705/7705 [==============================] - 6s 742us/sample - loss: 0.1194 - accuracy: 0.9554 - val_loss: 0.5185 - val_accuracy: 0.8191\n","Epoch 19/150\n","7705/7705 [==============================] - 6s 746us/sample - loss: 0.1101 - accuracy: 0.9590 - val_loss: 0.5094 - val_accuracy: 0.8285\n","Epoch 20/150\n","7705/7705 [==============================] - 6s 740us/sample - loss: 0.1017 - accuracy: 0.9635 - val_loss: 0.5478 - val_accuracy: 0.8261\n","Epoch 21/150\n","7705/7705 [==============================] - 6s 736us/sample - loss: 0.0963 - accuracy: 0.9644 - val_loss: 0.5223 - val_accuracy: 0.8355\n","Epoch 22/150\n","7705/7705 [==============================] - 6s 737us/sample - loss: 0.0911 - accuracy: 0.9677 - val_loss: 0.5703 - val_accuracy: 0.8168\n","Epoch 23/150\n","7705/7705 [==============================] - 6s 739us/sample - loss: 0.0829 - accuracy: 0.9722 - val_loss: 0.5593 - val_accuracy: 0.8203\n","Epoch 24/150\n","7705/7705 [==============================] - 6s 739us/sample - loss: 0.0790 - accuracy: 0.9703 - val_loss: 0.5765 - val_accuracy: 0.8191\n","Epoch 25/150\n","7705/7705 [==============================] - 6s 741us/sample - loss: 0.0816 - accuracy: 0.9682 - val_loss: 0.6300 - val_accuracy: 0.8156\n","Epoch 26/150\n","7705/7705 [==============================] - 6s 743us/sample - loss: 0.0705 - accuracy: 0.9751 - val_loss: 0.6353 - val_accuracy: 0.8273\n","Epoch 27/150\n","7705/7705 [==============================] - 6s 734us/sample - loss: 0.0765 - accuracy: 0.9729 - val_loss: 0.6217 - val_accuracy: 0.8331\n","Epoch 28/150\n","7705/7705 [==============================] - 6s 735us/sample - loss: 0.0726 - accuracy: 0.9743 - val_loss: 0.6388 - val_accuracy: 0.8343\n","Epoch 29/150\n","7705/7705 [==============================] - 6s 730us/sample - loss: 0.0671 - accuracy: 0.9766 - val_loss: 0.6450 - val_accuracy: 0.8296\n","Epoch 30/150\n","7705/7705 [==============================] - 6s 745us/sample - loss: 0.0642 - accuracy: 0.9756 - val_loss: 0.6302 - val_accuracy: 0.8308\n","Epoch 31/150\n","7705/7705 [==============================] - 6s 752us/sample - loss: 0.0634 - accuracy: 0.9765 - val_loss: 0.6373 - val_accuracy: 0.8226\n","Epoch 32/150\n","7705/7705 [==============================] - 6s 746us/sample - loss: 0.0588 - accuracy: 0.9803 - val_loss: 0.6642 - val_accuracy: 0.8156\n","Epoch 33/150\n","7705/7705 [==============================] - 6s 739us/sample - loss: 0.0704 - accuracy: 0.9748 - val_loss: 0.6291 - val_accuracy: 0.8331\n","Epoch 34/150\n","7705/7705 [==============================] - 6s 755us/sample - loss: 0.0583 - accuracy: 0.9794 - val_loss: 0.6470 - val_accuracy: 0.8285\n","Epoch 35/150\n","7705/7705 [==============================] - 6s 745us/sample - loss: 0.0534 - accuracy: 0.9803 - val_loss: 0.6591 - val_accuracy: 0.8285\n","Epoch 36/150\n","7705/7705 [==============================] - 6s 737us/sample - loss: 0.0519 - accuracy: 0.9814 - val_loss: 0.6748 - val_accuracy: 0.8320\n","Epoch 37/150\n","7705/7705 [==============================] - 6s 731us/sample - loss: 0.0516 - accuracy: 0.9794 - val_loss: 0.6756 - val_accuracy: 0.8320\n","Epoch 38/150\n","7705/7705 [==============================] - 6s 731us/sample - loss: 0.0560 - accuracy: 0.9814 - val_loss: 0.6559 - val_accuracy: 0.8331\n","Epoch 39/150\n","7705/7705 [==============================] - 6s 724us/sample - loss: 0.0460 - accuracy: 0.9836 - val_loss: 0.6641 - val_accuracy: 0.8401\n","Epoch 40/150\n","7705/7705 [==============================] - 6s 742us/sample - loss: 0.0451 - accuracy: 0.9825 - val_loss: 0.6866 - val_accuracy: 0.8378\n","Epoch 41/150\n","7705/7705 [==============================] - 6s 740us/sample - loss: 0.0540 - accuracy: 0.9791 - val_loss: 0.7148 - val_accuracy: 0.8320\n","Epoch 42/150\n","7705/7705 [==============================] - 6s 742us/sample - loss: 0.0520 - accuracy: 0.9821 - val_loss: 0.7246 - val_accuracy: 0.8215\n","Epoch 43/150\n","7705/7705 [==============================] - 6s 740us/sample - loss: 0.0507 - accuracy: 0.9823 - val_loss: 0.6921 - val_accuracy: 0.8296\n","Epoch 44/150\n","7705/7705 [==============================] - 6s 728us/sample - loss: 0.0417 - accuracy: 0.9835 - val_loss: 0.7054 - val_accuracy: 0.8261\n","Epoch 45/150\n","7705/7705 [==============================] - 6s 747us/sample - loss: 0.0442 - accuracy: 0.9836 - val_loss: 0.6961 - val_accuracy: 0.8296\n","Epoch 46/150\n","7705/7705 [==============================] - 6s 734us/sample - loss: 0.0364 - accuracy: 0.9868 - val_loss: 0.7628 - val_accuracy: 0.8308\n","Epoch 47/150\n","7705/7705 [==============================] - 6s 731us/sample - loss: 0.0398 - accuracy: 0.9842 - val_loss: 0.7571 - val_accuracy: 0.8238\n","Epoch 48/150\n","7705/7705 [==============================] - 6s 725us/sample - loss: 0.0393 - accuracy: 0.9860 - val_loss: 0.7259 - val_accuracy: 0.8250\n","Epoch 49/150\n","7705/7705 [==============================] - 6s 753us/sample - loss: 0.0519 - accuracy: 0.9820 - val_loss: 0.7490 - val_accuracy: 0.8215\n","Epoch 50/150\n","7705/7705 [==============================] - 6s 743us/sample - loss: 0.0441 - accuracy: 0.9829 - val_loss: 0.7263 - val_accuracy: 0.8355\n","Epoch 51/150\n","7705/7705 [==============================] - 6s 738us/sample - loss: 0.0432 - accuracy: 0.9827 - val_loss: 0.7664 - val_accuracy: 0.8320\n","Epoch 52/150\n","7705/7705 [==============================] - 6s 744us/sample - loss: 0.0411 - accuracy: 0.9843 - val_loss: 0.8342 - val_accuracy: 0.8203\n","Epoch 53/150\n","7680/7705 [============================>.] - ETA: 0s - loss: 0.0416 - accuracy: 0.9835\n","Epoch 00053: Reducing Max LR on Plateau: new max lr will be 0.0005 (if not early_stopping).\n","7705/7705 [==============================] - 6s 742us/sample - loss: 0.0416 - accuracy: 0.9835 - val_loss: 0.8299 - val_accuracy: 0.8250\n","Epoch 54/150\n","7705/7705 [==============================] - 6s 745us/sample - loss: 0.0404 - accuracy: 0.9844 - val_loss: 0.8296 - val_accuracy: 0.8285\n","Epoch 55/150\n","7705/7705 [==============================] - 6s 745us/sample - loss: 0.0353 - accuracy: 0.9848 - val_loss: 0.8359 - val_accuracy: 0.8285\n","Epoch 56/150\n","7705/7705 [==============================] - 6s 739us/sample - loss: 0.0350 - accuracy: 0.9852 - val_loss: 0.8249 - val_accuracy: 0.8308\n","Epoch 57/150\n","7705/7705 [==============================] - 6s 746us/sample - loss: 0.0305 - accuracy: 0.9874 - val_loss: 0.8048 - val_accuracy: 0.8378\n","Epoch 58/150\n","7705/7705 [==============================] - 6s 743us/sample - loss: 0.0296 - accuracy: 0.9874 - val_loss: 0.8217 - val_accuracy: 0.8366\n","Epoch 59/150\n","7705/7705 [==============================] - 6s 725us/sample - loss: 0.0323 - accuracy: 0.9878 - val_loss: 0.8321 - val_accuracy: 0.8366\n","Epoch 60/150\n","7705/7705 [==============================] - 6s 741us/sample - loss: 0.0314 - accuracy: 0.9873 - val_loss: 0.8476 - val_accuracy: 0.8343\n","Epoch 61/150\n","7705/7705 [==============================] - 6s 752us/sample - loss: 0.0286 - accuracy: 0.9888 - val_loss: 0.8721 - val_accuracy: 0.8331\n","Epoch 62/150\n","7705/7705 [==============================] - 6s 724us/sample - loss: 0.0302 - accuracy: 0.9877 - val_loss: 0.8781 - val_accuracy: 0.8285\n","Epoch 63/150\n","7705/7705 [==============================] - 6s 723us/sample - loss: 0.0310 - accuracy: 0.9874 - val_loss: 0.8635 - val_accuracy: 0.8331\n","Epoch 64/150\n","7705/7705 [==============================] - 6s 723us/sample - loss: 0.0251 - accuracy: 0.9899 - val_loss: 0.8525 - val_accuracy: 0.8343\n","Epoch 65/150\n","7705/7705 [==============================] - 6s 726us/sample - loss: 0.0233 - accuracy: 0.9894 - val_loss: 0.8665 - val_accuracy: 0.8308\n","Epoch 66/150\n","7705/7705 [==============================] - 6s 725us/sample - loss: 0.0284 - accuracy: 0.9877 - val_loss: 0.8902 - val_accuracy: 0.8320\n","Epoch 67/150\n","7705/7705 [==============================] - 6s 721us/sample - loss: 0.0249 - accuracy: 0.9900 - val_loss: 0.8940 - val_accuracy: 0.8296\n","Epoch 68/150\n","7705/7705 [==============================] - 6s 721us/sample - loss: 0.0224 - accuracy: 0.9909 - val_loss: 0.9022 - val_accuracy: 0.8285\n","Epoch 69/150\n","7705/7705 [==============================] - 6s 729us/sample - loss: 0.0247 - accuracy: 0.9895 - val_loss: 0.9152 - val_accuracy: 0.8238\n","Epoch 70/150\n","7705/7705 [==============================] - 6s 721us/sample - loss: 0.0260 - accuracy: 0.9901 - val_loss: 0.9173 - val_accuracy: 0.8273\n","Epoch 71/150\n","7705/7705 [==============================] - 6s 733us/sample - loss: 0.0296 - accuracy: 0.9891 - val_loss: 0.8955 - val_accuracy: 0.8285\n","Epoch 72/150\n","7705/7705 [==============================] - 6s 737us/sample - loss: 0.0254 - accuracy: 0.9890 - val_loss: 0.9094 - val_accuracy: 0.8250\n","Epoch 73/150\n","7705/7705 [==============================] - 6s 731us/sample - loss: 0.0236 - accuracy: 0.9901 - val_loss: 0.9397 - val_accuracy: 0.8250\n","Epoch 74/150\n","7705/7705 [==============================] - 6s 730us/sample - loss: 0.0257 - accuracy: 0.9897 - val_loss: 0.9056 - val_accuracy: 0.8285\n","Epoch 75/150\n","7705/7705 [==============================] - 6s 724us/sample - loss: 0.0254 - accuracy: 0.9907 - val_loss: 0.9398 - val_accuracy: 0.8273\n","Epoch 76/150\n","7705/7705 [==============================] - 6s 727us/sample - loss: 0.0231 - accuracy: 0.9897 - val_loss: 0.9279 - val_accuracy: 0.8273\n","Epoch 77/150\n","7705/7705 [==============================] - 6s 727us/sample - loss: 0.0232 - accuracy: 0.9900 - val_loss: 0.9282 - val_accuracy: 0.8261\n","Epoch 78/150\n","7705/7705 [==============================] - 6s 719us/sample - loss: 0.0233 - accuracy: 0.9909 - val_loss: 0.9629 - val_accuracy: 0.8203\n","Epoch 79/150\n","7705/7705 [==============================] - 6s 726us/sample - loss: 0.0264 - accuracy: 0.9896 - val_loss: 0.9648 - val_accuracy: 0.8261\n","Epoch 80/150\n","7705/7705 [==============================] - 6s 724us/sample - loss: 0.0237 - accuracy: 0.9894 - val_loss: 0.9618 - val_accuracy: 0.8261\n","Epoch 81/150\n","7705/7705 [==============================] - 6s 727us/sample - loss: 0.0223 - accuracy: 0.9917 - val_loss: 0.9704 - val_accuracy: 0.8273\n","Epoch 82/150\n","7705/7705 [==============================] - 6s 726us/sample - loss: 0.0232 - accuracy: 0.9895 - val_loss: 0.9928 - val_accuracy: 0.8285\n","Epoch 83/150\n","7705/7705 [==============================] - 6s 729us/sample - loss: 0.0239 - accuracy: 0.9900 - val_loss: 0.9779 - val_accuracy: 0.8261\n","Epoch 84/150\n","7705/7705 [==============================] - 6s 726us/sample - loss: 0.0236 - accuracy: 0.9887 - val_loss: 0.9964 - val_accuracy: 0.8250\n","Epoch 85/150\n","7705/7705 [==============================] - 6s 742us/sample - loss: 0.0216 - accuracy: 0.9909 - val_loss: 1.0174 - val_accuracy: 0.8331\n","Epoch 86/150\n","7705/7705 [==============================] - 6s 733us/sample - loss: 0.0217 - accuracy: 0.9905 - val_loss: 0.9926 - val_accuracy: 0.8343\n","Epoch 87/150\n","7705/7705 [==============================] - 6s 717us/sample - loss: 0.0207 - accuracy: 0.9901 - val_loss: 0.9991 - val_accuracy: 0.8343\n","Epoch 88/150\n","7705/7705 [==============================] - 6s 727us/sample - loss: 0.0214 - accuracy: 0.9904 - val_loss: 1.0302 - val_accuracy: 0.8296\n","Epoch 89/150\n","7705/7705 [==============================] - 6s 740us/sample - loss: 0.0267 - accuracy: 0.9894 - val_loss: 1.0454 - val_accuracy: 0.8308\n","Epoch 90/150\n","7705/7705 [==============================] - 6s 730us/sample - loss: 0.0231 - accuracy: 0.9909 - val_loss: 1.0014 - val_accuracy: 0.8320\n","Epoch 91/150\n","7705/7705 [==============================] - 6s 718us/sample - loss: 0.0193 - accuracy: 0.9918 - val_loss: 1.0147 - val_accuracy: 0.8331\n","Epoch 92/150\n","7705/7705 [==============================] - 6s 722us/sample - loss: 0.0206 - accuracy: 0.9903 - val_loss: 1.0493 - val_accuracy: 0.8320\n","Epoch 93/150\n","7616/7705 [============================>.] - ETA: 0s - loss: 0.0208 - accuracy: 0.9913\n","Epoch 00093: Reducing Max LR on Plateau: new max lr will be 0.00025 (if not early_stopping).\n","Restoring model weights from the end of the best epoch.\n","7705/7705 [==============================] - 6s 724us/sample - loss: 0.0208 - accuracy: 0.9913 - val_loss: 1.0284 - val_accuracy: 0.8331\n","Epoch 00093: early stopping\n","Weights from best epoch have been loaded into model.\n","              precision    recall  f1-score   support\n","\n","    positive       0.90      0.70      0.78       397\n","     neutral       0.78      0.93      0.85       460\n","\n","    accuracy                           0.82       857\n","   macro avg       0.84      0.81      0.82       857\n","weighted avg       0.83      0.82      0.82       857\n","\n","model /content/drive/My Drive/Weight_file/positive_neutral.h5\n","Model Saved\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MzoQo_4TUJq5","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"paUhBmYwbByw","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"B-Ignk2EbB11","colab_type":"code","outputId":"5f10b51b-9a00-44d2-cbdd-46c010a202cc","executionInfo":{"status":"error","timestamp":1583861820988,"user_tz":-330,"elapsed":839788,"user":{"displayName":"AMBUJE GUPTA","photoUrl":"","userId":"08374827863947652219"}},"colab":{"base_uri":"https://localhost:8080/","height":723}},"source":["import ktrain\n","from ktrain import text\n","\n","\n","\n","modelslist = [['very_offensive', 'slight']]\n","data_files = ['Slight-V-off-memotion_eq_onlyoffensive.csv']\n","for columnclass, data_file in zip(modelslist, data_files):\n","\n","    \n","    columns=columnclass      \n","    DATA_PATH = '/content/drive/My Drive/data_eqd/' + data_file\n","    print(DATA_PATH)\n","\n","    NUM_WORDS = 90000\n","    MAXLEN = 100\n","\n","    h5name = '_'.join(columnclass)\n","    print('model', h5name)\n","\n","    (x_train, y_train), (x_test, y_test), preproc = text.texts_from_csv(DATA_PATH,\n","                          'cleaned_ocr',preprocess_mode='bert',\n","                          label_columns = columns,\n","                          val_filepath=None, # if None, 10% of data will be used for validation\n","                          max_features=NUM_WORDS, maxlen=MAXLEN,\n","                        ngram_range=1,)\n","\n","\n","    model = text.text_classifier('bert', (x_train, y_train), preproc=preproc)\n","\n","    learner = ktrain.get_learner(model, train_data=(x_train, y_train), val_data=(x_test, y_test))\n","    a='/content/drive/My Drive/Weight_file/'+h5name\n","    learner.autofit(0.01, 10,early_stopping=8, reduce_on_plateau=5)\n","\n","    a=learner.validate(val_data=(x_test, y_test), class_names=columns)\n","  \n","    predictor = ktrain.get_predictor(learner.model, preproc)\n","\n","   # score = 'xyz'########################\n","    h5name = \"/content/drive/My Drive/Weight_file/\"+h5name +  '.h5'\n","    print('model', h5name)\n","\n","    predictor.save(h5name)\n","    print(\"Model Saved\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/data_eqd/Slight-V-off-memotion_eq_onlyoffensive.csv\n","model very_offensive_slight\n","detected encoding: utf-8 (if wrong, set manually)\n","preprocessing train...\n","language: en\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["done."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["preprocessing test...\n","language: en\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["done."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Is Multi-Label? False\n","maxlen is 100\n","done.\n","\n","\n","begin training using triangular learning rate policy with max lr of 0.01...\n","Train on 4552 samples, validate on 506 samples\n","Epoch 1/10\n","1280/4552 [=======>......................] - ETA: 35:23 - loss: 1.0999 - accuracy: 0.5096"],"name":"stdout"},{"output_type":"error","ename":"Exception","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[0;34m(self, epoch, mode)\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m       \u001b[0;32myield\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    486\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-76-7fb2bf76f275>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mlearner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mktrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_learner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/My Drive/Weight_file/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mh5name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautofit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_on_plateau\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ktrain/core.py\u001b[0m in \u001b[0;36mautofit\u001b[0;34m(self, lr, epochs, early_stopping, reduce_on_plateau, reduce_factor, cycle_momentum, monitor, checkpoint_folder, verbose, class_weight, callbacks)\u001b[0m\n\u001b[1;32m    858\u001b[0m         hist = self.fit(lr, epochs, early_stopping=early_stopping,\n\u001b[1;32m    859\u001b[0m                         \u001b[0mcheckpoint_folder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_folder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m                         verbose=verbose, class_weight=class_weight, callbacks=kcallbacks)\n\u001b[0m\u001b[1;32m    861\u001b[0m         \u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m         \u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'iterations'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'iterations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ktrain/core.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, lr, n_cycles, cycle_len, cycle_mult, lr_decay, checkpoint_folder, early_stopping, verbose, class_weight, callbacks)\u001b[0m\n\u001b[1;32m    985\u001b[0m                                   \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m                                   \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 987\u001b[0;31m                                   callbacks=kcallbacks)\n\u001b[0m\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msgdr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msgdr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    370\u001b[0m                       total_epochs=1)\n\u001b[1;32m    371\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[0;32m--> 372\u001b[0;31m                                  prefix='val_')\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[0;34m(self, epoch, mode)\u001b[0m\n\u001b[1;32m    683\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0;31m# Epochs only apply to `fit`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m       \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ktrain/lroptimize/triangular.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mcurrent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cannot monitor %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonitor_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mException\u001b[0m: cannot monitor val_loss"]}]},{"cell_type":"code","metadata":{"id":"6cifag4FbBv7","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}